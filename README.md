# AWS-Notes-Repo-Full-Prof

AWS Well-Architected Framework July 2020 This paper has been archived. The latest version is available at: https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html This whitepaper describes the AWS Well-Architected Framework. It provides guidance to help cus tomers apply best practices in the design, delivery, and maintenance of AWS environments. We address general design principles as well as specific best practices and guidance in five conceptual areas that we define as the pillars of the Well-Architected Framework. Archived AWS Well-Architected Framework Notices Customers are responsible for making their own independent assessment of the in formation in this document. This document: (a) is for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Copyright © 2020 Amazon Web Services, Inc. or its affiliates Archived Archived AWS Well-Architected Framework Introduction.................................................................................................................................1 Definitions........................................................................................................................... 2 On Architecture..................................................................................................................3 General Design Principles................................................................................................ 5 The Five Pillars of the Framework.........................................................................................6 Operational Excellence....................................................................................................6 Security.............................................................................................................................. 15 Reliability...........................................................................................................................22 Performance Efficiency...................................................................................................28 Cost Optimization........................................................................................................... 36 The Review Process.................................................................................................................43 Conclusion.................................................................................................................................45 Contributors..............................................................................................................................46 Further Reading....................................................................................................................... 47 Document Revisions................................................................................................................48 Appendix: Questions and Best Practices.............................................................................49 Operational Excellence................................................................................................. 49 Security.............................................................................................................................. 60 Reliability...........................................................................................................................69 Performance Efficiency...................................................................................................80 Cost Optimization........................................................................................................... 88 iii AWS Well-Architected Framework Introduction The AWS Well-Architected Framework helps you understand the pros and cons of de cisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, effi cient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. The process for reviewing an architecture is a constructive conversation about archi tectural decisions, and is not an audit mechanism. We believe that having well-archi tected systems greatly increases the likelihood of business success. AWS Solutions Architects have years of experience architecting solutions across a wide variety of business verticals and use cases. We have helped design and review thou sands of customers’ architectures on AWS. From this experience, we have identified best practices and core strategies for architecting systems in the cloud. The AWS Well-Architected Framework documents a set of foundational questions that allow you to understand if a specific architecture aligns well with cloud best practices. The framework provides a consistent approach to evaluating systems against the qualities you expect from modern cloud-based systems, and the remedi ation that would be required to achieve those qualities. As AWS continues to evolve, and we continue to learn more from working with our customers, we will continue to refine the definition of well-architected. This framework is intended for those in technology roles, such as chief technology of ficers (CTOs), architects, developers, and operations team members. It describes AWS best practices and strategies to use when designing and operating a cloud workload, and provides links to further implementation details and architectural patterns. For more information, see the AWS Well-Architected homepage. AWS also provides a service for reviewing your workloads at no charge. The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consis tent process for you to review and measure your architecture using the AWS Well-Ar chitected Framework. The AWS WA Tool provides recommendations for making your workloads more reliable, secure, efficient, and cost-effective. To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with a repository of code and documentation to give you hands-on ex perience implementing best practices. We also have teamed up with select AWS Part ner Network (APN) Partners, who are members of the AWS Well-Architected Partner program. These APN Partners have deep AWS knowledge, and can help you review and improve your workloads. Archived 1 AWS Well-Architected Framework Definitions Every day, experts at AWS assist customers in architecting systems to take advantage of best practices in the cloud. We work with you on making architectural trade-offs as your designs evolve. As you deploy these systems into live environments, we learn how well these systems perform and the consequences of those trade-offs. Based on what we have learned, we have created the AWS Well-Architected Frame work, which provides a consistent set of best practices for customers and partners to evaluate architectures, and provides a set of questions you can use to evaluate how well an architecture is aligned to AWS best practices. The AWS Well-Architected Framework is based on five pillars — operational excel lence, security, reliability, performance efficiency, and cost optimization. Table 1. The pillars of the AWS Well-Architected Framework Name Description Operational Excellence The ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and proce dures to deliver business value. Security The security pillar encompasses the ability to protect data, systems, and assets to take advantage of cloud technologies to improve your security. Reliability The reliability pillar encompasses the ability of a work load to perform its intended function correctly and con sistently when it’s expected to. This includes the ability to operate and test the workload through its total life cycle. This paper provides in-depth, best practice guid ance for implementing reliable workloads on AWS. Performance Efficiency The ability to use computing resources efficiently to meet system requirements, and to maintain that effi ciency as demand changes and technologies evolve. Cost Optimization The ability to run systems to deliver business value at the lowest price point. In the AWS Well-Architected Framework, we use these terms: Archived • A component is the code, configuration, and AWS Resources that together deliver against a requirement. A component is often the unit of technical ownership, and is decoupled from other components. 2 AWS Well-Architected Framework • The term workload is used to identify a set of components that together deliver business value. A workload is usually the level of detail that business and technolo gy leaders communicate about. • We think about architecture as being how components work together in a work load. How components communicate and interact is often the focus of architecture diagrams. • Milestones mark key changes in your architecture as it evolves throughout the product lifecycle (design, testing, go live, and in production). • Within an organization the technology portfolio is the collection of workloads that are required for the business to operate. When architecting workloads, you make trade-offs between pillars based on your business context. These business decisions can drive your engineering priorities. You might optimize to reduce cost at the expense of reliability in development environ ments, or, for mission-critical solutions, you might optimize reliability with increased costs. In ecommerce solutions, performance can affect revenue and customer propen sity to buy. Security and operational excellence are generally not traded-off against the other pillars. On Architecture In on-premises environments, customers often have a central team for technology ar chitecture that acts as an overlay to other product or feature teams to ensure they are following best practice. Technology architecture teams typically include a set of roles such as: Technical Architect (infrastructure), Solutions Architect (software), Data Ar chitect, Networking Architect, and Security Architect. Often these teams use TOGAF or the Zachman Framework as part of an enterprise architecture capability. At AWS, we prefer to distribute capabilities into teams rather than having a central ized team with that capability. There are risks when you choose to distribute decision making authority, for example, ensure that teams are meeting internal standards. We mitigate these risks in two ways. First, we have practices 1 that focus on enabling each team to have that capability, and we put in place experts who ensure that teams raise the bar on the standards they need to meet. Second, we implement mechanisms 2 that carry out automated checks to ensure standards are being met. This distributed approach is supported by the Amazon leadership principles, and establishes a culture Archived 1Ways of doing things, process, standards, and accepted norms. 2 “Good intentions never work, you need good mechanisms to make anything happen” Jeff Bezos. This means replacing humans best efforts with mechanisms (often automated) that check for compliance with rules or process. 3 AWS Well-Architected Framework across all roles that works back 3 from the customer. Customer-obsessed teams build products in response to a customer need. For architecture, this means that we expect every team to have the capability to cre ate architectures and to follow best practices. To help new teams gain these capa bilities or existing teams to raise their bar, we enable access to a virtual communi ty of principal engineers who can review their designs and help them understand what AWS best practices are. The principal engineering community works to make best practices visible and accessible. One way they do this, for example, is through lunchtime talks that focus on applying best practices to real examples. These talks are recorded and can be used as part of onboarding materials for new team members. AWS best practices emerge from our experience running thousands of systems at in ternet scale. We prefer to use data to define best practice, but we also use subject matter experts, like principal engineers, to set them. As principal engineers see new best practices emerge, they work as a community to ensure that teams follow them. In time, these best practices are formalized into our internal review processes, as well as into mechanisms that enforce compliance. The Well-Architected Framework is the customer-facing implementation of our internal review process, where we have cod ified our principal engineering thinking across field roles, like Solutions Architecture and internal engineering teams. The Well-Architected Framework is a scalable mecha nism that lets you take advantage of these learnings. By following the approach of a principal engineering community with distributed ownership of architecture, we believe that a Well-Architected enterprise architecture can emerge that is driven by customer need. Technology leaders (such as a CTOs or development managers), carrying out Well-Architected reviews across all your work loads will allow you to better understand the risks in your technology portfolio. Using this approach, you can identify themes across teams that your organization could ad dress by mechanisms, training, or lunchtime talks where your principal engineers can share their thinking on specific areas with multiple teams. Archived 3Working backward is a fundamental part of our innovation process. We start with the customer and what they want, and let that define and guide our efforts. 4 AWS Well-Architected Framework General Design Principles The Well-Architected Framework identifies a set of general design principles to facili tate good design in the cloud: • Stop guessing your capacity needs: If you make a poor capacity decision when de ploying a workload, you might end up sitting on expensive idle resources or deal ing with the performance implications of limited capacity. With cloud computing, these problems can go away. You can use as much or as little capacity as you need, and scale up and down automatically. • Test systems at production scale: In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources. Because you only pay for the test environment when it's running, you can simulate your live environment for a fraction of the cost of testing on premises. • Automate to make architectural experimentation easier: Automation allows you to create and replicate your workloads at low cost and avoid the expense of manu al effort. You can track changes to your automation, audit the impact, and revert to previous parameters when necessary. • Allow for evolutionary architectures: Allow for evolutionary architectures. In a tra ditional environment, architectural decisions are often implemented as static, one time events, with a few major versions of a system during its lifetime. As a business and its context continue to evolve, these initial decisions might hinder the system's ability to deliver changing business requirements. In the cloud, the capability to au tomate and test on demand lowers the risk of impact from design changes. This al lows systems to evolve over time so that businesses can take advantage of innova tions as a standard practice. • Drive architectures using data: In the cloud, you can collect data on how your ar chitectural choices affect the behavior of your workload. This lets you make fact based decisions on how to improve your workload. Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improve ments over time. • Improve through game days: Test how your architecture and processes perform by regularly scheduling game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events. Archived 5 AWS Well-Architected Framework The Five Pillars of the Framework Creating a software system is a lot like constructing a building. If the foundation is not solid, structural problems can undermine the integrity and function of the build ing. When architecting technology solutions, if you neglect the five pillars of opera tional excellence, security, reliability, performance efficiency, and cost optimization, it can become challenging to build a system that delivers on your expectations and re quirements. Incorporating these pillars into your architecture will help you produce stable and efficient systems. This will allow you to focus on the other aspects of de sign, such as functional requirements. Operational Excellence The Operational Excellence pillar includes the ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value. The operational excellence pillar provides an overview of design principles, best prac tices, and questions. You can find prescriptive guidance on implementation in the Op erational Excellence Pillar whitepaper. Design Principles There are five design principles for operational excellence in the cloud: • Perform operations as code: In the cloud, you can apply the same engineering dis cipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure) as code and update it with code. You can implement your operations procedures as code and automate their execu tion by triggering them in response to events. By performing operations as code, you limit human error and enable consistent responses to events. • Make frequent, small, reversible changes: Design workloads to allow components to be updated regularly. Make changes in small increments that can be reversed if they fail (without affecting customers when possible). • Refine operations procedures frequently: As you use operations procedures, look for opportunities to improve them. As you evolve your workload, evolve your proce dures appropriately. Set up regular game days to review and validate that all proce dures are effective and that teams are familiar with them. Archived • Anticipate failure: Perform “pre-mortem” exercises to identify potential sources of failure so that they can be removed or mitigated. Test your failure scenarios and validate your understanding of their impact. Test your response procedures to en 6 AWS Well-Architected Framework sure that they are effective, and that teams are familiar with their execution. Set up regular game days to test workloads and team responses to simulated events. • Learn from all operational failures: Drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization. Definition There are four best practice areas for operational excellence in the cloud: • Organization • Prepare • Operate • Evolve Your organization’s leadership defines business objectives. Your organization must understand requirements and priorities and use these to organize and conduct work to support the achievement of business outcomes. Your workload must emit the in formation necessary to support it. Implementing services to enable integration, de ployment, and delivery of your workload will enable an increased flow of beneficial changes into production by automating repetitive processes. There may be risks inherent in the operation of your workload. You must understand those risks and make an informed decision to enter production. Your teams must be able to support your workload. Business and operational metrics derived from de sired business outcomes will enable you to understand the health of your workload, your operations activities, and respond to incidents. Your priorities will change as your business needs and business environment changes. Use these as a feedback loop to continually drive improvement for your organization and the operation of your work load. Best Practices Organization Archived Your teams need to have a shared understanding of your entire workload, their role in it, and shared business goals to set the priorities that will enable business success. Well-defined priorities will maximize the benefits of your efforts. Evaluate internal and external customer needs involving key stakeholders, including business, devel opment, and operations teams, to determine where to focus efforts. Evaluating cus tomer needs will ensure that you have a thorough understanding of the support that 7 AWS Well-Architected Framework is required to achieve business outcomes. Ensure that you are aware of guidelines or obligations defined by your organizational governance and external factors, such as regulatory compliance requirements and industry standards, that may mandate or emphasize specific focus. Validate that you have mechanisms to identify changes to internal governance and external compliance requirements. If no requirements are identified, ensure that you have applied due diligence to this determination. Review your priorities regularly so that they can be updated as needs change. Evaluate threats to the business (for example, business risk and liabilities, and infor mation security threats) and maintain this information in a risk registry. Evaluate the impact of risks, and tradeoffs between competing interests or alternative approaches. For example, accelerating speed to market for new features may be emphasized over cost optimization, or you may choose a relational database for non-relational data to simplify the effort to migrate a system without refactoring. Manage benefits and risks to make informed decisions when determining where to focus efforts. Some risks or choices may be acceptable for a time, it may be possible to mitigate associated risks, or it may become unacceptable to allow a risk to remain, in which case you will take action to address the risk. Your teams must understand their part in achieving business outcomes. Teams need to understand their roles in the success of other teams, the role of other teams in their success, and have shared goals. Understanding responsibility, ownership, how decisions are made, and who has authority to make decisions will help focus efforts and maximize the benefits from your teams. The needs of a team will be shaped by the customer they support, their organization, the makeup of the team, and the char acteristics of their workload. It's unreasonable to expect a single operating model to be able to support all teams and their workloads in your organization. Ensure that there are identified owners for each application, workload, platform, and infrastructure component, and that each process and procedure has an identified owner responsible for its definition, and owners responsible for their performance. Having understanding of the business value of each component, process, and pro cedure, of why those resources are in place or activities are performed, and why that ownership exists will inform the actions of your team members. Clearly define the re sponsibilities of team members so that they may act appropriately and have mech anisms to identify responsibility and ownership. Have mechanisms to request addi tions, changes, and exceptions so that you do not constrain innovation. Define agree ments between teams describing how they work together to support each other and your business outcomes. Archived Provide support for your team members so that they can be more effective in taking action and supporting your business outcomes. Engaged senior leadership should set expectations and measure success. They should be the sponsor, advocate, and driver for the adoption of best practices and evolution of the organization. Empower team members to take action when outcomes are at risk to minimize impact and encour age them to escalate to decision makers and stakeholders when they believe there 8 AWS Well-Architected Framework is a risk so that it can be address and incidents avoided. Provide timely, clear, and ac tionable communications of known risks and planned events so that team members can take timely and appropriate action. Encourage experimentation to accelerate learning and keeps team members interest ed and engaged. Teams must grow their skill sets to adopt new technologies, and to support changes in demand and responsibilities. Support and encourage this by pro viding dedicated structure time for learning. Ensure your team members have the re sources, both tools and team members, to be successful and scale to support your business outcomes. Leverage cross-organizational diversity to seek multiple unique perspectives. Use this perspective to increase innovation, challenge your assumptions, and reduce the risk of confirmation bias. Grow inclusion, diversity, and accessibility within your teams to gain beneficial perspectives. If there are external regulatory or compliance requirements that apply to your organi zation, you should use the resources provided by AWS Cloud Compliance to help ed ucate your teams so that they can determine the impact on your priorities. The Well Architected Framework emphasizes learning, measuring, and improving. It provides a consistent approach for you to evaluate architectures, and implement designs that will scale over time. AWS provides the AWS Well-Architected Tool to help you review your approach prior to development, the state of your workloads prior to production, and the state of your workloads in production. You can compare workloads to the lat est AWS architectural best practices, monitor their overall status, and gain insight in to potential risks. AWS Trusted Advisor is a tool that provides access to a core set of checks that recommend optimizations that may help shape your priorities. Business and Enterprise Support customers receive access to additional checks focusing on security, reliability, performance, and cost-optimization that can further help shape their priorities. AWS can help you educate your teams about AWS and its services to increase their understanding of how their choices can have an impact on your workload. You should use the resources provided by AWS Support (AWS Knowledge Center, AWS Discus sion Forums, and AWS Support Center) and AWS Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS questions. AWS also shares best practices and patterns that we have learned through the operation of AWS in The Amazon Builders' Library. A wide variety of oth er useful information is available through the AWS Blog and The Official AWS Pod cast. AWS Training and Certification provides some free training through self-paced digital courses on AWS fundamentals. You can also register for instructor-led training to further support the development of your teams’ AWS skills. Archived You should use tools or services that enable you to centrally govern your environ ments across accounts, such as AWS Organizations, to help manage your operating models. Services like AWS Control Tower expand this management capability by en abling you to define blueprints (supporting your operating models) for the setup of accounts, apply ongoing governance using AWS Organizations, and automate provi 9 AWS Well-Architected Framework sioning of new accounts. Managed Services providers such as AWS Managed Services, AWS Managed Services Partners, or Managed Services Providers in the AWS Partner Network, provide expertise implementing cloud environments, and support your se curity and compliance requirements and business goals. Adding Managed Services to your operating model can save you time and resources, and lets you keep your inter nal teams lean and focused on strategic outcomes that will differentiate your busi ness, rather than developing new skills and capabilities. The following questions focus on these considerations for operational excellence . (For a list of operational excellence questions and best practices, see the Appendix.). OPS 1:  How do you determine what your priorities are? Everyone needs to understand their part in enabling business success. Have shared goals in order to set priorities for resources. This will maximize the benefits of your efforts. OPS 2:  How do you structure your organization to support your business outcomes? Your teams must understand their part in achieving business outcomes. Teams need to un derstand their roles in the success of other teams, the role of other teams in their success, and have shared goals. Understanding responsibility, ownership, how decisions are made, and who has authority to make decisions will help focus efforts and maximize the benefits from your teams. OPS 3:  How does your organizational culture support your business outcomes? Provide support for your team members so that they can be more effective in taking action and supporting your business outcome. You might find that you want to emphasize a small subset of your priorities at some point in time. Use a balanced approach over the long term to ensure the development of needed capabilities and management of risk. Review your priorities regularly and update them as needs change. When responsibility and ownership are undefined or unknown, you are at risk of both not performing necessary action in a timely fashion and of redundant and potentially conflicting efforts emerging to address those needs. Organizational culture has a direct impact on team member job satisfaction and re tention. Enable the engagement and capabilities of your team members to enable the success of your business. Experimentation is required for innovation to happen and turn ideas into outcomes. Recognize that an undesired result is a successful experi ment that has identified a path that will not lead to success. Prepare To prepare for operational excellence, you have to understand your workloads and their expected behaviors. You will then be able design them to provide insight to their status and build the procedures to support them. Archived Design your workload so that it provides the information necessary for you to under stand its internal state (for example, metrics, logs, events, and traces) across all com 10 AWS Well-Architected Framework ponents in support of observability and investigating issues. Iterate to develop the telemetry necessary to monitor the health of your workload, identify when outcomes are at risk, and enable effective responses. When instrumenting your workload, cap ture a broad set of information to enable situational awareness (for example, changes in state, user activity, privilege access, utilization counters), knowing that you can use filters to select the most useful information over time. Adopt approaches that improve the flow of changes into production and that en able refactoring, fast feedback on quality, and bug fixing. These accelerate beneficial changes entering production, limit issues deployed, and enable rapid identification and remediation of issues introduced through deployment activities or discovered in your environments. Adopt approaches that provide fast feedback on quality and enable rapid recovery from changes that do not have desired outcomes. Using these practices mitigates the impact of issues introduced through the deployment of changes. Plan for unsuc cessful changes so that you are able to respond faster if necessary and test and val idate the changes you make. Be aware of planned activities in your environments so that you can manage the risk of changes impacting planed activities. Emphasize fre quent, small, reversible changes to limit the scope of change. This results in easier troubleshooting and faster remediation with the option to roll back a change. It also means you are able to get the benefit of valuable changes more frequently. Evaluate the operational readiness of your workload, processes, procedures, and per sonnel to understand the operational risks related to your workload. You should use a consistent process (including manual or automated checklists) to know when you are ready to go live with your workload or a change. This will also enable you to find any areas that you need to make plans to address. Have runbooks that document your routine activities and playbooks that guide your processes for issue resolution. Un derstand the benefits and risks to make informed decisions to allow changes to enter production. AWS enables you to view your entire workload (applications, infrastructure, policy, governance, and operations) as code. This means you can apply the same engineering discipline that you use for application code to every element of your stack and share these across teams or organizations to magnify the benefits of development efforts. Use operations as code in the cloud and the ability to safely experiment to develop your workload, your operations procedures, and practice failure. Using AWS CloudFor mation enables you to have consistent, templated, sandbox development, test, and production environments with increasing levels of operations control. Archived 11 AWS Well-Architected Framework The following questions focus on these considerations for operational excellence . OPS 4:  How do you design your workload so that you can understand its state? Design your workload so that it provides the information necessary across all components (for example, metrics, logs, and traces) for you to understand its internal state. This enables you to provide effective responses when appropriate. OPS 5:  How do you reduce defects, ease remediation, and improve flow into production? Adopt approaches that improve flow of changes into production, that enable refactoring, fast feedback on quality, and bug fixing. These accelerate beneficial changes entering pro duction, limit issues deployed, and enable rapid identification and remediation of issues in troduced through deployment activities. OPS 6:  How do you mitigate deployment risks? Adopt approaches that provide fast feedback on quality and enable rapid recovery from changes that do not have desired outcomes. Using these practices mitigates the impact of is sues introduced through the deployment of changes. OPS 7:  How do you know that you are ready to support a workload? Evaluate the operational readiness of your workload, processes and procedures, and person nel to understand the operational risks related to your workload. Invest in implementing operations activities as code to maximize the productivity of operations personnel, minimize error rates, and enable automated responses. Use “pre-mortems” to anticipate failure and create procedures where appropriate. Apply metadata using Resource Tags and AWS Resource Groups following a consistent tag ging strategy to enable identification of your resources. Tag your resources for orga nization, cost accounting, access controls, and targeting the execution of automated operations activities. Adopt deployment practices that take advantage of the elastic ity of the cloud to facilitate development activities, and pre-deployment of systems for faster implementations. When you make changes to the checklists you use to eval uate your workloads, plan what you will do with live systems that no longer comply. Operate Successful operation of a workload is measured by the achievement of business and customer outcomes. Define expected outcomes, determine how success will be mea sured, and identify metrics that will be used in those calculations to determine if your workload and operations are successful. Operational health includes both the health of the workload and the health and success of the operations activities performed in support of the workload (for example, deployment and incident response). Establish metrics baselines for improvement, investigation, and intervention, collect and an alyze your metrics, and then validate your understanding of operations success and how it changes over time. Use collected metrics to determine if you are satisfying cus tomer and business needs, and identify areas for improvement. Archived 12 AWS Well-Architected Framework Efficient and effective management of operational events is required to achieve op erational excellence. This applies to both planned and unplanned operational events. Use established runbooks for well-understood events, and use playbooks to aid in investigation and resolution of issues. Prioritize responses to events based on their business and customer impact. Ensure that if an alert is raised in response to an event, there is an associated process to be executed, with a specifically identified owner. Define in advance the personnel required to resolve an event and include es calation triggers to engage additional personnel, as it becomes necessary, based on urgency and impact. Identify and engage individuals with the authority to make a de cision on courses of action where there will be a business impact from an event re sponse not previously addressed. Communicate the operational status of workloads through dashboards and notifica tions that are tailored to the target audience (for example, customer, business, devel opers, operations) so that they may take appropriate action, so that their expectations are managed, and so that they are informed when normal operations resume. In AWS, you can generate dashboard views of your metrics collected from workloads and natively from AWS. You can leverage CloudWatch or third-party applications to aggregate and present business, workload, and operations level views of opera tions activities. AWS provides workload insights through logging capabilities including AWS X-Ray, CloudWatch, CloudTrail, and VPC Flow Logs enabling the identification of workload issues in support of root cause analysis and remediation. The following questions focus on these considerations for operational excellence . OPS 8:  How do you understand the health of your workload? Define, capture, and analyze workload metrics to gain visibility to workload events so that you can take appropriate action. OPS 9:  How do you understand the health of your operations? Define, capture, and analyze operations metrics to gain visibility to operations events so that you can take appropriate action. OPS 10:  How do you manage workload and operations events? Prepare and validate procedures for responding to events to minimize their disruption to your workload. All of the metrics you collect should be aligned to a business need and the outcomes they support. Develop scripted responses to well-understood events and automate their performance in response to recognizing the event. Evolve Archived You must learn, share, and continuously improve to sustain operational excellence. Dedicate work cycles to making continuous incremental improvements. Perform post 13 AWS Well-Architected Framework incident analysis of all customer impacting events. Identify the contributing factors and preventative action to limit or prevent recurrence. Communicate contributing factors with affected communities as appropriate. Regularly evaluate and prioritize opportunities for improvement (for example, feature requests, issue remediation, and compliance requirements), including both the workload and operations procedures. Include feedback loops within your procedures to rapidly identify areas for improve ment and capture learnings from the execution of operations. Share lessons learned across teams to share the benefits of those lessons. Analyze trends within lessons learned and perform cross-team retrospective analysis of op erations metrics to identify opportunities and methods for improvement. Implement changes intended to bring about improvement and evaluate the results to determine success. On AWS, you can export your log data to Amazon S3 or send logs directly to Amazon S3 for long-term storage. Using AWS Glue, you can discover and prepare your log da ta in Amazon S3 for analytics, and store associated metadata in the AWS Glue Data Catalog. Amazon Athena, through its native integration with AWS Glue, can then be used to analyze your log data, querying it using standard SQL. Using a business intel ligence tool like Amazon QuickSight, you can visualize, explore, and analyze your da ta. Discovering trends and events of interest that may drive improvement. The following questions focus on these considerations for operational excellence . OPS 11:  How do you evolve operations? Dedicate time and resources for continuous incremental improvement to evolve the effec tiveness and efficiency of your operations. Successful evolution of operations is founded in: frequent small improvements; pro viding safe environments and time to experiment, develop, and test improvements; and environments in which learning from failures is encouraged. Operations support for sandbox, development, test, and production environments, with increasing lev el of operational controls, facilitates development and increases the predictability of successful results from changes deployed into production. Resources Refer to the following resources to learn more about our best practices for Opera tional Excellence . Documentation • DevOps and AWS Whitepaper Archived 14 AWS Well-Architected Framework • Operational Excellence Pillar Video • DevOps at Amazon Security The Security pillar includes the security pillar encompasses the ability to protect data, systems, and assets to take advantage of cloud technologies to improve your security. The security pillar provides an overview of design principles, best practices, and ques tions. You can find prescriptive guidance on implementation in the Security Pillar whitepaper. Design Principles There are seven design principles for security in the cloud: • Implement a strong identity foundation: Implement the principle of least privi lege and enforce separation of duties with appropriate authorization for each inter action with your AWS resources. Centralize identity management, and aim to elimi nate reliance on long-term static credentials. • Enable traceability: Monitor, alert, and audit actions and changes to your environ ment in real time. Integrate log and metric collection with systems to automatically investigate and take action. • Apply security at all layers: Apply a defense in depth approach with multiple secu rity controls. Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code). • Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost-effectively. Create se cure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates. • Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropri ate. Archived • Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of mis handling or modification and human error when handling sensitive data. • Prepare for security events: Prepare for an incident by having incident manage ment and investigation policy and processes that align to your organizational re 15 AWS Well-Architected Framework quirements. Run incident response simulations and use tools with automation to in crease your speed for detection, investigation, and recovery. Definition There are six best practice areas for security in the cloud: • Security • Identity and Access Management • Detection • Infrastructure Protection • Data Protection • Incident Response Before you architect any workload, you need to put in place practices that influence security. You will want to control who can do what. In addition, you want to be able to identify security incidents, protect your systems and services, and maintain the con fidentiality and integrity of data through data protection. You should have a well-de fined and practiced process for responding to security incidents. These tools and tech niques are important because they support objectives such as preventing financial loss or complying with regulatory obligations. The AWS Shared Responsibility Model enables organizations that adopt the cloud to achieve their security and compliance goals. Because AWS physically secures the infra structure that supports our cloud services, as an AWS customer you can focus on us ing services to accomplish your goals. The AWS Cloud also provides greater access to security data and an automated approach to responding to security events. Best Practices Security To operate your workload securely, you must apply overarching best practices to every area of security. Take requirements and processes that you have defined in op erational excellence at an organizational and workload level, and apply them to all ar eas. Archived Staying up to date with AWS and industry recommendations and threat intelligence helps you evolve your threat model and control objectives. Automating security processes, testing, and validation allow you to scale your security operations. 16 AWS Well-Architected Framework The following questions focus on these considerations for security. (For a list of secu rity questions and best practices, see the Appendix.). SEC 1:  How do you securely operate your workload? To operate your workload securely, you must apply overarching best practices to every area of security. Take requirements and processes that you have defined in operational excellence at an organizational and workload level, and apply them to all areas. Staying up to date with AWS and industry recommendations and threat intelligence helps you evolve your threat model and control objectives. Automating security processes, testing, and validation allow you to scale your security operations. In AWS, segregating different workloads by account, based on their function and compliance or data sensitivity requirements, is a recommended approach. Identity and Access Management Identity and access management are key parts of an information security program, ensuring that only authorized and authenticated users and components are able to access your resources, and only in a manner that you intend. For example, you should define principals (that is, accounts, users, roles, and services that can perform ac tions in your account), build out policies aligned with these principals, and implement strong credential management. These privilege-management elements form the core of authentication and authorization. In AWS, privilege management is primarily supported by the AWS Identity and Ac cess Management (IAM) service, which allows you to control user and programmat ic access to AWS services and resources. You should apply granular policies, which as sign permissions to a user, group, role, or resource. You also have the ability to require strong password practices, such as complexity level, avoiding re-use, and enforcing multi-factor authentication (MFA). You can use federation with your existing directory service. For workloads that require systems to have access to AWS, IAM enables secure access through roles, instance profiles, identity federation, and temporary credentials. Archived 17 AWS Well-Architected Framework The following questions focus on these considerations for security. SEC 2:  How do you manage identities for people and machines? There are two types of identities you need to manage when approaching operating secure AWS workloads. Understanding the type of identity you need to manage and grant access helps you ensure the right identities have access to the right resources under the right con ditions. Human Identities: Your administrators, developers, operators, and end users require an identity to access your AWS environments and applications. These are members of your organization, or external users with whom you collaborate, and who interact with your AWS resources via a web browser, client application, or interactive command-line tools. Machine Identities: Your service applications, operational tools, and workloads require an identity to make requests to AWS services - for example, to read data. These identities include machines running in your AWS environment such as Amazon EC2 instances or AWS Lambda functions. You may also manage machine identities for external parties who need access. Additionally, you may also have machines outside of AWS that need access to your AWS environment. SEC 3:  How do you manage permissions for people and machines? Manage permissions to control access to people and machine identities that require access to AWS and your workload. Permissions control who can access what, and under what condi tions. Credentials must not be shared between any user or system. User access should be granted using a least-privilege approach with best practices including password re quirements and MFA enforced. Programmatic access including API calls to AWS ser vices should be performed using temporary and limited-privilege credentials such as those issued by the AWS Security Token Service. AWS provides resources that can help you with Identity and access management. To help learn best practices, explore our hands-on labs on managing credentials & au thentication, controlling human access, and controlling programmatic access. Detection You can use detective controls to identify a potential security threat or incident. They are an essential part of governance frameworks and can be used to support a quality process, a legal or compliance obligation, and for threat identification and response efforts. There are different types of detective controls. For example, conducting an in ventory of assets and their detailed attributes promotes more effective decision mak ing (and lifecycle controls) to help establish operational baselines. You can also use internal auditing, an examination of controls related to information systems, to en sure that practices meet policies and requirements and that you have set the correct automated alerting notifications based on defined conditions. These controls are im portant reactive factors that can help your organization identify and understand the scope of anomalous activity. Archived In AWS, you can implement detective controls by processing logs, events, and mon itoring that allows for auditing, automated analysis, and alarming. CloudTrail logs, 18 AWS Well-Architected Framework AWS API calls, and CloudWatch provide monitoring of metrics with alarming, and AWS Config provides configuration history. Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behav ior to help you protect your AWS accounts and workloads. Service-level logs are also available, for example, you can use Amazon Simple Storage Service (Amazon S3) to log access requests. The following questions focus on these considerations for security. SEC 4:  How do you detect and investigate security events? Capture and analyze events from logs and metrics to gain visibility. Take action on security events and potential threats to help secure your workload. Log management is important to a Well-Architected workload for reasons ranging from security or forensics to regulatory or legal requirements. It is critical that you an alyze logs and respond to them so that you can identify potential security incidents. AWS provides functionality that makes log management easier to implement by giv ing you the ability to define a data-retention lifecycle or define where data will be preserved, archived, or eventually deleted. This makes predictable and reliable data handling simpler and more cost effective. Infrastructure Protection Infrastructure protection encompasses control methodologies, such as defense in depth, necessary to meet best practices and organizational or regulatory obligations. Use of these methodologies is critical for successful, ongoing operations in either the cloud or on-premises. In AWS, you can implement stateful and stateless packet inspection, either by using AWS-native technologies or by using partner products and services available through the AWS Marketplace. You should use Amazon Virtual Private Cloud (Amazon VPC) to create a private, secured, and scalable environment in which you can define your topology—including gateways, routing tables, and public and private subnets. The following questions focus on these considerations for security. SEC 5:  How do you protect your network resources? Any workload that has some form of network connectivity, whether it’s the internet or a pri vate network, requires multiple layers of defense to help protect from external and internal network-based threats. SEC 6:  How do you protect your compute resources? Archived Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more. 19 AWS Well-Architected Framework Multiple layers of defense are advisable in any type of environment. In the case of in frastructure protection, many of the concepts and methods are valid across cloud and on-premises models. Enforcing boundary protection, monitoring points of ingress and egress, and comprehensive logging, monitoring, and alerting are all essential to an ef fective information security plan. AWS customers are able to tailor, or harden, the configuration of an Amazon Elastic Compute Cloud (Amazon EC2), Amazon EC2 Container Service (Amazon ECS) contain er, or AWS Elastic Beanstalk instance, and persist this configuration to an immutable Amazon Machine Image (AMI). Then, whether triggered by Auto Scaling or launched manually, all new virtual servers (instances) launched with this AMI receive the hard ened configuration. Data Protection Before architecting any system, foundational practices that influence security should be in place. For example, data classification provides a way to categorize organiza tional data based on levels of sensitivity, and encryption protects data by way of ren dering it unintelligible to unauthorized access. These tools and techniques are impor tant because they support objectives such as preventing financial loss or complying with regulatory obligations. In AWS, the following practices facilitate protection of data: • As an AWS customer you maintain full control over your data. • AWS makes it easier for you to encrypt your data and manage keys, including regu lar key rotation, which can be easily automated by AWS or maintained by you. • Detailed logging that contains important content, such as file access and changes, is available. • AWS has designed storage systems for exceptional resiliency. For example, Amazon S3 Standard, S3 Standard–IA, S3 One Zone-IA, and Amazon Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects. • Versioning, which can be part of a larger data lifecycle management process, can protect against accidental overwrites, deletes, and similar harm. • AWS never initiates the movement of data between Regions. Content placed in a Region will remain in that Region unless you explicitly enable a feature or leverage a service that provides that functionality. Archived 20 AWS Well-Architected Framework The following questions focus on these considerations for security. SEC 7:  How do you classify your data? Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls. SEC 8:  How do you protect your data at rest? Protect your data at rest by implementing multiple controls, to reduce the risk of unautho rized access or mishandling. SEC 9:  How do you protect your data in transit? Protect your data in transit by implementing multiple controls to reduce the risk of unautho rized access or loss. AWS provides multiple means for encrypting data at rest and in transit. We build fea tures into our services that make it easier to encrypt your data. For example, we have implemented server-side encryption (SSE) for Amazon S3 to make it easier for you to store your data in an encrypted form. You can also arrange for the entire HTTPS en cryption and decryption process (generally known as SSL termination) to be handled by Elastic Load Balancing (ELB). Incident Response Even with extremely mature preventive and detective controls, your organization should still put processes in place to respond to and mitigate the potential impact of security incidents. The architecture of your workload strongly affects the ability of your teams to operate effectively during an incident, to isolate or contain systems, and to restore operations to a known good state. Putting in place the tools and ac cess ahead of a security incident, then routinely practicing incident response through game days, will help you ensure that your architecture can accommodate timely in vestigation and recovery. In AWS, the following practices facilitate effective incident response: • Detailed logging is available that contains important content, such as file access and changes. • Events can be automatically processed and trigger tools that automate responses through the use of AWS APIs. Archived • You can pre-provision tooling and a “clean room” using AWS CloudFormation. This allows you to carry out forensics in a safe, isolated environment. 21 AWS Well-Architected Framework The following questions focus on these considerations for security. SEC 10:  How do you anticipate, respond to, and recover from incidents? Preparation is critical to timely and effective investigation, response to, and recovery from security incidents to help minimize disruption to your organization. Ensure that you have a way to quickly grant access for your security team, and auto mate the isolation of instances as well as the capturing of data and state for forensics. Resources Refer to the following resources to learn more about our best practices for Security. Documentation • AWS Cloud Security • AWS Compliance • AWS Security Blog Whitepaper • Security Pillar • AWS Security Overview • AWS Security Best Practices • AWS Risk and Compliance Video • AWS Security State of the Union • Shared Responsibility Overview Reliability The Reliability pillar includes the reliability pillar encompasses the ability of a work load to perform its intended function correctly and consistently when it’s expected to. this includes the ability to operate and test the workload through its total lifecycle. this paper provides in-depth, best practice guidance for implementing reliable work loads on aws. Archived The reliability pillar provides an overview of design principles, best practices, and questions. You can find prescriptive guidance on implementation in the Reliability Pil lar whitepaper. 22 AWS Well-Architected Framework Design Principles There are five design principles for reliability in the cloud: • Automatically recover from failure: By monitoring a workload for key perfor mance indicators (KPIs), you can trigger automation when a threshold is breached. These KPIs should be a measure of business value, not of the technical aspects of the operation of the service. This allows for automatic notification and tracking of failures, and for automated recovery processes that work around or repair the fail ure. With more sophisticated automation, it’s possible to anticipate and remediate failures before they occur. • Test recovery procedures: In an on-premises environment, testing is often con ducted to prove that the workload works in a particular scenario. Testing is not typ ically used to validate recovery strategies. In the cloud, you can test how your work load fails, and you can validate your recovery procedures. You can use automation to simulate different failures or to recreate scenarios that led to failures before. This approach exposes failure pathways that you can test and fix before a real fail ure scenario occurs, thus reducing risk. • Scale horizontally to increase aggregate workload availability: Replace one large resource with multiple small resources to reduce the impact of a single failure on the overall workload. Distribute requests across multiple, smaller resources to en sure that they don’t share a common point of failure. • Stop guessing capacity: A common cause of failure in on-premises workloads is re source saturation, when the demands placed on a workload exceed the capacity of that workload (this is often the objective of denial of service attacks). In the cloud, you can monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning. There are still limits, but some quotas can be controlled and others can be managed (see Manage Service Quotas and Constraints). • Manage change in automation: Changes to your infrastructure should be made us ing automation. The changes that need to be managed include changes to the au tomation, which then can be tracked and reviewed. Definition There are four best practice areas for reliability in the cloud: • Foundations • Workload Architecture Archived • Change Management 23 AWS Well-Architected Framework • Failure Management To achieve reliability you must start with the foundations — an environment where service quotas and network topology accommodate the workload. The workload ar chitecture of the distributed system must be designed to prevent and mitigate fail ures. The workload must handle changes in demand or requirements, and it must be designed to detect failure and automatically heal itself. Best Practices Foundations Foundational requirements are those whose scope extends beyond a single workload or project. Before architecting any system, foundational requirements that influence reliability should be in place. For example, you must have sufficient network band width to your data center. With AWS, most of these foundational requirements are already incorporated or can be addressed as needed. The cloud is designed to be nearly limitless, so it’s the re sponsibility of AWS to satisfy the requirement for sufficient networking and compute capacity, leaving you free to change resource size and allocations on demand. The following questions focus on these considerations for reliability. (For a list of reli ability questions and best practices, see the Appendix.). REL 1:  How do you manage service quotas and constraints? For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can push bits down a fiber-optic cable, or the amount of storage on a physical disk. REL 2:  How do you plan your network topology? Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, pub lic IP address management, private IP address management, and domain name resolution. For cloud-based workload architectures, there are service quotas (which are also re ferred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations to protect services from abuse. Workloads often exist in multiple environments. You must mon itor and manage these quotas for all workload environments. These include multiple cloud environments (both publicly accessible and private) and may include your exist ing data center infrastructure. Plans must include network considerations, such as in trasystem and intersystem connectivity, public IP address management, private IP ad dress management, and domain name resolution. Archived 24 AWS Well-Architected Framework Workload Architecture A reliable workload starts with upfront design decisions for both software and infra structure. Your architecture choices will impact your workload behavior across all five Well-Architected pillars. For reliability, there are specific patterns you must follow. With AWS, workload developers have their choice of languages and technologies to use. AWS SDKs take the complexity out of coding by providing language-specific APIs for AWS services. These SDKs, plus the choice of languages, allow developers to im plement the reliability best practices listed here. Developers can also read about and learn from how Amazon builds and operates software in The Amazon Builders' Li brary. The following questions focus on these considerations for reliability. REL 3:  How do you design your workload service architecture? Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a microservices architecture. Service-oriented architecture (SOA) is the practice of making soft ware components reusable via service interfaces. Microservices architecture goes further to make components smaller and simpler. REL 4:  How do you design interactions in a distributed system to prevent failures? Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not neg atively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF). REL 5:  How do you design interactions in a distributed system to mitigate or withstand failures? Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR). Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. Change Management Archived Changes to your workload or its environment must be anticipated and accommodat ed to achieve reliable operation of the workload. Changes include those imposed on your workload, such as spikes in demand, as well as those from within, such as feature deployments and security patches. 25 AWS Well-Architected Framework Using AWS, you can monitor the behavior of a workload and automate the response to KPIs. For example, your workload can add additional servers as a workload gains more users. You can control who has permission to make workload changes and audit the history of these changes. The following questions focus on these considerations for reliability. REL 6:  How do you monitor workload resources? Logs and metrics are powerful tools to gain insight into the health of your workload. You can configure your workload to monitor logs and metrics and send notifications when thresholds are crossed or significant events occur. Monitoring enables your workload to recognize when low-performance thresholds are crossed or failures occur, so it can recover automatically in response. REL 7:  How do you design your workload to adapt to changes in demand? A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time. REL 8:  How do you implement change? Controlled changes are necessary to deploy new functionality, and to ensure that the work loads and the operating environment are running known software and can be patched or re placed in a predictable manner. If these changes are uncontrolled, then it makes it difficult to predict the effect of these changes, or to address issues that arise because of them. When you architect a workload to automatically add and remove resources in re sponse to changes in demand, this not only increases reliability but also ensures that business success doesn't become a burden. With monitoring in place, your team will be automatically alerted when KPIs deviate from expected norms. Automatic logging of changes to your environment allows you to audit and quickly identify actions that might have impacted reliability. Controls on change management ensure that you can enforce the rules that deliver the reliability you need. Failure Management In any system of reasonable complexity, it is expected that failures will occur. Reliabil ity requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues. Archived With AWS, you can take advantage of automation to react to monitoring data. For ex ample, when a particular metric crosses a threshold, you can trigger an automated ac tion to remedy the problem. Also, rather than trying to diagnose and fix a failed re source that is part of your production environment, you can replace it with a new one and carry out the analysis on the failed resource out of band. Since the cloud enables you to stand up temporary versions of a whole system at low cost, you can use auto mated testing to verify full recovery processes. 26 AWS Well-Architected Framework The following questions focus on these considerations for reliability. REL 9:  How do you back up data? Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO). REL 10:  How do you use fault isolation to protect your workload? Fault isolated boundaries limit the effect of a failure within a workload to a limited number of components. Components outside of the boundary are unaffected by the failure. Using multiple fault isolated boundaries, you can limit the impact on your workload. REL 11:  How do you design your workload to withstand component failures? Workloads with a requirement for high availability and low mean time to recovery (MTTR) must be architected for resiliency. REL 12:  How do you test reliability? After you have designed your workload to be resilient to the stresses of production, testing is the only way to ensure that it will operate as designed, and deliver the resiliency you expect. REL 13:  How do you plan for disaster recovery (DR)? Having backups and redundant workload components in place is the start of your DR strate gy. RTO and RPO are your objectives for restoration of availability. Set these based on busi ness needs. Implement a strategy to meet these objectives, considering locations and func tion of workload resources and data. Regularly back up your data and test your backup files to ensure that you can recov er from both logical and physical errors. A key to managing failure is the frequent and automated testing of workloads to cause failure, and then observe how they recov er. Do this on a regular schedule and ensure that such testing is also triggered after significant workload changes. Actively track KPIs, such as the recovery time objective (RTO) and recovery point objective (RPO), to assess a workload's resiliency (especial ly under failure-testing scenarios). Tracking KPIs will help you identify and mitigate single points of failure. The objective is to thoroughly test your workload-recovery processes so that you are confident that you can recover all your data and continue to serve your customers, even in the face of sustained problems. Your recovery processes should be as well exercised as your normal production processes. Resources Refer to the following resources to learn more about our best practices for Reliability. Documentation • AWS Documentation • AWS Global Infrastructure Archived • AWS Auto Scaling: How Scaling Plans Work 27 AWS Well-Architected Framework • What Is AWS Backup? Whitepaper • Reliability Pillar: AWS Well-Architected • Implementing Microservices on AWS Performance Efficiency The Performance Efficiency pillar includes the ability to use computing resources ef ficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve. The performance efficiency pillar provides an overview of design principles, best prac tices, and questions. You can find prescriptive guidance on implementation in the Per formance Efficiency Pillar whitepaper. Design Principles There are five design principles for performance efficiency in the cloud: • Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consid er consuming the technology as a service. For example, NoSQL databases, media transcoding, and machine learning are all technologies that require specialized ex pertise. In the cloud, these technologies become services that your team can con sume, allowing your team to focus on product development rather than resource provisioning and management. • Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your cus tomers at minimal cost. • Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities. For example, serverless storage services can act as static websites (removing the need for web servers) and event services can host code. This removes the operational burden of managing physical servers, and can lower transactional costs because managed ser vices operate at cloud scale. Archived • Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or config urations. 28 AWS Well-Architected Framework • Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage ap proaches. Definition There are four best practice areas for performance efficiency in the cloud: • Selection • Review • Monitoring • Tradeoffs Take a data-driven approach to building a high-performance architecture. Gather data on all aspects of the architecture, from the high-level design to the selection and con figuration of resource types. Reviewing your choices on a regular basis ensures that you are taking advantage of the continually evolving AWS Cloud. Monitoring ensures that you are aware of any de viance from expected performance. Make trade-offs in your architecture to improve performance, such as using compression or caching, or relaxing consistency require ments. Best Practices Selection The optimal solution for a particular workload varies, and solutions often combine multiple approaches. Well-architected workloads use multiple solutions and enable different features to improve performance. AWS resources are available in many types and configurations, which makes it easier to find an approach that closely matches your workload needs. You can also find op tions that are not easily achievable with on-premises infrastructure. For example, a managed service such as Amazon DynamoDB provides a fully managed NoSQL data base with single-digit millisecond latency at any scale. Archived 29 AWS Well-Architected Framework The following questions focus on these considerations for performance efficiency. (For a list of performance efficiency questions and best practices, see the Appendix.). PERF 1:  How do you select the best performing architecture? Often, multiple approaches are required for optimal performance across a workload. Well architected systems use multiple solutions and features to improve performance. Use a data-driven approach to select the patterns and implementation for your archi tecture and achieve a cost effective solution. AWS Solutions Architects, AWS Refer ence Architectures, and AWS Partner Network (APN) partners can help you select an architecture based on industry knowledge, but data obtained through benchmarking or load testing will be required to optimize your architecture. Your architecture will likely combine a number of different architectural approach es (for example, event-driven, ETL, or pipeline). The implementation of your architec ture will use the AWS services that are specific to the optimization of your architec ture's performance. In the following sections we discuss the four main resource types to consider (compute, storage, database, and network). Compute Selecting compute resources that meet your requirements, performance needs, and provide great efficiency of cost and effort will enable you to accomplish more with the same number of resources. When evaluating compute options, be aware of your requirements for workload performance and cost requirements and use this to make informed decisions. In AWS, compute is available in three forms: instances, containers, and functions: • Instances are virtualized servers, allowing you to change their capabilities with a button or an API call. Because resource decisions in the cloud aren’t fixed, you can experiment with different server types. At AWS, these virtual server instances come in different families and sizes, and they offer a wide variety of capabilities, includ ing solid-state drives (SSDs) and graphics processing units (GPUs). • Containers are a method of operating system virtualization that allow you to run an application and its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or Amazon EC2 can be used if you need con trol over the installation, configuration, and management of your compute environ ment. You can also choose from multiple container orchestration platforms: Ama zon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS). Archived • Functions abstract the execution environment from the code you want to execute. For example, AWS Lambda allows you to execute code without running an instance. 30 AWS Well-Architected Framework The following questions focus on these considerations for performance efficiency. PERF 2:  How do you select your compute solution? The optimal compute solution for a workload varies based on application design, usage pat terns, and configuration settings. Architectures can use different compute solutions for vari ous components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency. When architecting your use of compute you should take advantage of the elasticity mechanisms available to ensure you have sufficient capacity to sustain performance as demand changes. Storage Cloud storage is a critical component of cloud computing, holding the information used by your workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Select from object, block, and file stor age services as well as cloud data migration options for your workload. In AWS, storage is available in three forms: object, block, and file: • Object Storage provides a scalable, durable platform to make data accessible from any internet location for user-generated content, active archive, serverless com puting, Big Data storage or backup and recovery. Amazon Simple Storage Ser vice (Amazon S3) is an object storage service that offers industry-leading scal ability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. • Block Storage provides highly available, consistent, low-latency block storage for each virtual host and is analogous to direct-attached storage (DAS) or a Stor age Area Network (SAN). Amazon Elastic Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2 instances that helps you tune applications with the right storage capacity, performance and cost. • File Storage provides access to a shared file system across multiple systems. File storage solutions like Amazon Elastic File System (EFS) are ideal for use cases, such as large content repositories, development environments, media stores, or user home directories. Amazon FSx makes it easy and cost effective to launch and run popular file systems so you can leverage the rich feature sets and fast performance of widely used open source and commercially-licensed file systems. Archived 31 AWS Well-Architected Framework The following questions focus on these considerations for performance efficiency. PERF 3:  How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, f ile, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently. When you select a storage solution, ensuring that it aligns with your access patterns will be critical to achieving the performance you want. Database The cloud offers purpose-built database services that address different problems pre sented by your workload. You can choose from many purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. By picking the best database to solve a specific problem (or a group of problems), you can break away from restrictive one-size-fits-all monolithic databases and focus on building applications to meet the performance needs of your customers. In AWS you can choose from multiple purpose-built database engines including re lational, key-value, document, in-memory, graph, time series, and ledger databas es. With AWS databases, you don’t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running with self healing storage and automated scaling, so that you can focus on higher value applica tion development. The following questions focus on these considerations for performance efficiency. PERF 4:  How do you select your database solution? The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different fea tures to improve performance. Selecting the wrong database solution and features for a sys tem can lead to lower performance efficiency. Archived Your workload's database approach has a significant impact on performance efficien cy. It's often an area that is chosen according to organizational defaults rather than through a data-driven approach. As with storage, it is critical to consider the access patterns of your workload, and also to consider if other non-database solutions could solve the problem more efficiently (such as using graph, time series, or in-memory storage database). 32 AWS Well-Architected Framework Network Since the network is between all workload components, it can have great impacts, both positive and negative, on workload performance and behavior. There are also workloads that are heavily dependent on network performance such as High Perfor mance Computing (HPC) where deep network understanding is important to increase cluster performance. You must determine the workload requirements for bandwidth, latency, jitter, and throughput. On AWS, networking is virtualized and is available in a number of different types and configurations. This makes it easier to match your networking methods with your needs. AWS offers product features (for example, Enhanced Networking, Amazon EBS-optimized instances, Amazon S3 transfer acceleration, and dynamic Amazon CloudFront) to optimize network traffic. AWS also offers networking features (for ex ample, Amazon Route 53 latency routing, Amazon VPC endpoints, AWS Direct Con nect, and AWS Global Accelerator) to reduce network distance or jitter. The following questions focus on these considerations for performance efficiency. PERF 5:  How do you configure your networking solution? The optimal network solution for a workload varies based on latency, throughput require ments, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, de termine location options. These constraints can be offset with edge locations or resource placement. You must consider location when deploying your network. You can choose to place resources close to where they will be used to reduce distance. Use networking met rics to make changes to networking configuration as the workload evolves. By tak ing advantage of Regions, placement groups, and edge services, you can significant ly improve performance. Cloud based networks can be quickly re-built or modified, so evolving your network architecture over time is necessary to maintain performance efficiency. Review Cloud technologies are rapidly evolving and you must ensure that workload compo nents are using the latest technologies and approaches to continually improve perfor mance. You must continually evaluate and consider changes to your workload com ponents to ensure you are meeting its performance and cost objectives. New tech nologies, such as machine learning and artificial intelligence (AI), can allow you to re imagine customer experiences and innovate across all of your business workloads. Archived Take advantage of the continual innovation at AWS driven by customer need. We re lease new Regions, edge locations, services, and features regularly. Any of these re leases could positively improve the performance efficiency of your architecture. 33 AWS Well-Architected Framework The following questions focus on these considerations for performance efficiency. PERF 6:  How do you evolve your workload to take advantage of new releases? When architecting workloads, there are finite options that you can choose from. However, over time, new technologies and approaches become available that could improve the per formance of your workload. Architectures performing poorly are usually the result of a non-existent or broken performance review process. If your architecture is performing poorly, implement ing a performance review process will allow you to apply Deming’s plan-do-check-act (PDCA) cycle to drive iterative improvement. Monitoring After you implement your workload, you must monitor its performance so that you can remediate any issues before they impact your customers. Monitoring metrics should be used to raise alarms when thresholds are breached. Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide per formance changes, optimize resource utilization, and get a unified view of operational health. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events from workloads that run on AWS and on-premises servers. AWS X-Ray helps developers analyze and debug production, distributed applications. With AWS X-Ray, you can glean insights into how your application is performing and dis cover root causes and identify performance bottlenecks. You can use these insights to react quickly and keep your workload running smoothly. The following questions focus on these considerations for performance efficiency. PERF 7:  How do you monitor your resources to ensure they are performing? System performance can degrade over time. Monitor system performance to identify degra dation and remediate internal or external factors, such as the operating system or applica tion load. Ensuring that you do not see false positives is key to an effective monitoring solution. Automated triggers avoid human error and can reduce the time it takes to fix prob lems. Plan for game days, where simulations are conducted in the production environ ment, to test your alarm solution and ensure that it correctly recognizes issues. Tradeoffs Archived When you architect solutions, think about tradeoffs to ensure an optimal approach. Depending on your situation, you could trade consistency, durability, and space for time or latency, to deliver higher performance. 34 AWS Well-Architected Framework Using AWS, you can go global in minutes and deploy resources in multiple locations across the globe to be closer to your end users. You can also dynamically add read only replicas to information stores (such as database systems) to reduce the load on the primary database. The following questions focus on these considerations for performance efficiency. PERF 8:  How do you use tradeoffs to improve performance? When architecting solutions, determining tradeoffs enables you to select an optimal ap proach. Often you can improve performance by trading consistency, durability, and space for time and latency. As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your trade-offs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance. Resources Refer to the following resources to learn more about our best practices for Perfor mance Efficiency. Documentation • Amazon S3 Performance Optimization • Amazon EBS Volume Performance Whitepaper • Performance Efficiency Pillar Video • AWS re:Invent 2019: Amazon EC2 foundations (CMP211-R2) • AWS re:Invent 2019: Leadership session: Storage state of the union (STG201-L) • AWS re:Invent 2019: Leadership session: AWS purpose-built databases (DAT209-L) • AWS re:Invent 2019: Connectivity to AWS and hybrid AWS network architectures (NET317-R1) Archived • AWS re:Invent 2019: Powering next-gen Amazon EC2: Deep dive into the Nitro sys tem (CMP303-R2) • AWS re:Invent 2019: Scaling up to your first 10 million users (ARC211-R) 35 AWS Well-Architected Framework Cost Optimization The Cost Optimization pillar includes the ability to run systems to deliver business value at the lowest price point. The cost optimization pillar provides an overview of design principles, best practices, and questions. You can find prescriptive guidance on implementation in the Cost Op timization Pillar whitepaper. Design Principles There are five design principles for cost optimization in the cloud: • Implement Cloud Financial Management: To achieve financial success and accel erate business value realization in the cloud, you need to invest in Cloud Financial Management /Cost Optimization. Your organization needs to dedicate time and re sources to build capability in this new domain of technology and usage manage ment. Similar to your Security or Operational Excellence capability, you need to build capability through knowledge building, programs, resources, and processes to become a cost-efficient organization. • Adopt a consumption model: Pay only for the computing resources that you re quire and increase or decrease usage depending on business requirements, not by using elaborate forecasting. For example, development and test environments are typically only used for eight hours a day during the work week. You can stop these resources when they are not in use for a potential cost savings of 75% (40 hours versus 168 hours). • Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it. Use this measure to know the gains you make from increasing output and reducing costs. • Stop spending money on undifferentiated heavy lifting: AWS does the heavy lift ing of data center operations like racking, stacking, and powering servers. It also removes the operational burden of managing operating systems and applications with managed services. This allows you to focus on your customers and business projects rather than on IT infrastructure. • Analyze and attribute expenditure: The cloud makes it easier to accurately identify the usage and cost of systems, which then allows transparent attribution of IT costs to individual workload owners. This helps measure return on investment (ROI) and gives workload owners an opportunity to optimize their resources and reduce costs. Definition Archived There are five best practice areas for cost optimization in the cloud: 36 AWS Well-Architected Framework • Practice Cloud Financial Management • Expenditure and usage awareness • Cost-effective resources • Manage demand and supply resources • Optimize over time As with the other pillars within the Well-Architected Framework, there are trade offs to consider, for example, whether to optimize for speed-to-market or for cost. In some cases, it’s best to optimize for speed—going to market quickly, shipping new features, or simply meeting a deadline—rather than investing in up-front cost opti mization. Design decisions are sometimes directed by haste rather than data, and the temptation always exists to overcompensate “just in case” rather than spend time benchmarking for the most cost-optimal deployment. This might lead to over-provi sioned and under-optimized deployments. However, this is a reasonable choice when you need to “lift and shift” resources from your on-premises environment to the cloud and then optimize afterwards. Investing the right amount of effort in a cost op timization strategy up front allows you to realize the economic benefits of the cloud more readily by ensuring a consistent adherence to best practices and avoiding un necessary over provisioning. The following sections provide techniques and best prac tices for both the initial and ongoing implementation of Cloud Financial Management and cost optimization of your workloads. Best Practices Practice Cloud Financial Management With the adoption of cloud, technology teams innovate faster due to shortened ap proval, procurement, and infrastructure deployment cycles. A new approach to finan cial management in the cloud is required to realize business value and financial suc cess. This approach is Cloud Financial Management, and builds capability across your organization by implementing organizational wide knowledge building, programs, re sources, and processes. Many organizations are composed of many different units with different priorities. The ability to align your organization to an agreed set of financial objectives, and pro vide your organization the mechanisms to meet them, will create a more efficient or ganization. A capable organization will innovate and build faster, be more agile and adjust to any internal or external factors. Archived In AWS you can use Cost Explorer, and optionally Amazon Athena and Amazon Quick Sight with the Cost and Usage Report (CUR), to provide cost and usage awareness throughout your organization. AWS Budgets provides proactive notifications for cost 37 AWS Well-Architected Framework and usage. The AWS blogs provide information on new services and features to en sure you keep up to date with new service releases. The following questions focus on these considerations for cost optimization. (For a list of cost optimization questions and best practices, see the Appendix.). COST 1:  How do you implement cloud financial management? Implementing Cloud Financial Management enables organizations to realize business value and financial success as they optimize their cost and usage and scale on AWS. When building a cost optimization function, use members and supplement the team with experts in CFM and CO. Existing team members will understand how the organi zation currently functions and how to rapidly implement improvements. Also consid er including people with supplementary or specialist skill sets, such as analytics and project management. When implementing cost awareness in your organization, improve or build on exist ing programs and processes. It is much faster to add to what exists than to build new processes and programs. This will result in achieving outcomes much faster. Expenditure and usage awareness The increased flexibility and agility that the cloud enables encourages innovation and fast-paced development and deployment. It eliminates the manual processes and time associated with provisioning on-premises infrastructure, including identifying hardware specifications, negotiating price quotations, managing purchase orders, scheduling shipments, and then deploying the resources. However, the ease of use and virtually unlimited on-demand capacity requires a new way of thinking about ex penditures. Many businesses are composed of multiple systems run by various teams. The capa bility to attribute resource costs to the individual organization or product owners dri ves efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more in formed decisions about where to allocate budget. Archived In AWS, you create an account structure with AWS Organizations or AWS Control Tower, which provides separation and assists in allocation of your costs and usage. You can also use resource tagging to apply business and organization information to your usage and cost. Use AWS Cost Explorer for visibility into your cost and usage, or create customized dashboards and analytics with Amazon Athena and Amazon Quick Sight. Controlling your cost and usage is done by notifications through AWS Budgets, and controls using AWS Identity and Access Management (IAM), and Service Quotas. 38 AWS Well-Architected Framework The following questions focus on these considerations for cost optimization. COST 2:  How do you govern usage? Establish policies and mechanisms to ensure that appropriate costs are incurred while objec tives are achieved. By employing a checks-and-balances approach, you can innovate without overspending. COST 3:  How do you monitor usage and cost? Establish policies and procedures to monitor and appropriately allocate your costs. This al lows you to measure and improve the cost efficiency of this workload. COST 4:  How do you decommission resources? Implement change control and resource management from project inception to end-of-life. This ensures you shut down or terminate unused resources to reduce waste. You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags. You can apply tags that represent organization categories (such as cost centers, workload names, or owners) to organize your costs across multiple services. Ensure you use the right level of detail and granularity in cost and usage reporting and monitoring. For high level insights and trends, use daily granularity with AWS Cost Explorer. For deeper analysis and inspection use hourly granularity in AWS Cost Explorer, or Amazon Athena and Amazon QuickSight with the Cost and Usage Report (CUR) at an hourly granularity. Combining tagged resources with entity lifecycle tracking (employees, projects) makes it possible to identify orphaned resources or projects that are no longer gener ating value to the organization and should be decommissioned. You can set up billing alerts to notify you of predicted overspending. Cost-effective resources Using the appropriate instances and resources for your workload is key to cost sav ings. For example, a reporting process might take five hours to run on a smaller server but one hour to run on a larger server that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more cost over time. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use man aged services to reduce costs. For example, rather than maintaining servers to deliver email, you can use a service that charges on a per-message basis. Archived AWS offers a variety of flexible and cost-effective pricing options to acquire instances from Amazon EC2 and other services in a way that best fits your needs. On-Demand 39 AWS Well-Architected Framework Instances allow you to pay for compute capacity by the hour, with no minimum com mitments required. Savings Plans and Reserved Instances offer savings of up to 75% off On-Demand pricing. With Spot Instances, you can leverage unused Amazon EC2 capacity and offer savings of up to 90% off On-Demand pricing. Spot Instances are appropriate where the system can tolerate using a fleet of servers where individual servers can come and go dynamically, such as stateless web servers, batch processing, or when using HPC and big data. Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data transfer, or completely eliminate costs, such as utilizing Amazon Auro ra on RDS to remove expensive database licensing costs. The following questions focus on these considerations for cost optimization. COST 5:  How do you evaluate cost when you select services? Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed ser vices, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or re move much of your administrative and operational overhead, freeing you to work on appli cations and business-related activities. COST 6:  How do you meet cost targets when you select resource type, size and number? Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number. COST 7:  How do you use pricing models to reduce cost? Use the pricing model that is most appropriate for your resources to minimize expense. COST 8:  How do you plan for data transfer charges? Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time. By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your deployments accordingly. Manage demand and supply resources When you move to the cloud, you pay only for what you need. You can supply re sources to match the workload demand at the time they’re needed, this eliminates the need for costly and wasteful over provisioning. You can also modify the demand, using a throttle, buffer, or queue to smooth the demand and serve it with less re sources resulting in a lower cost, or process it at a later time with a batch service. Archived In AWS, you can automatically provision resources to match the workload demand. Auto Scaling using demand or time-based approaches allow you to add and remove 40 AWS Well-Architected Framework resources as needed. If you can anticipate changes in demand, you can save more money and ensure your resources match your workload needs. You can use Amazon API Gateway to implement throttling, or Amazon SQS to implementing a queue in your workload. These will both allow you to modify the demand on your workload components. The following questions focus on these considerations for cost optimization. COST 9:  How do you manage demand, and supply resources? For a workload that has balanced spend and performance, ensure that everything you pay for is used and avoid significantly underutilizing instances. A skewed utilization metric in ei ther direction has an adverse impact on your organization, in either operational costs (de graded performance due to over-utilization), or wasted AWS expenditures (due to over-pro visioning). When designing to modify demand and supply resources, actively think about the patterns of usage, the time it takes to provision new resources, and the predictabili ty of the demand pattern. When managing demand, ensure you have a correctly sized queue or buffer, and that you are responding to workload demand in the required amount of time. Optimize over time As AWS releases new services and features, it's a best practice to review your existing architectural decisions to ensure they continue to be the most cost effective. As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require. Implementing new features or resource types can optimize your workload incremen tally, while minimizing the effort required to implement the change. This provides continual improvements in efficiency over time and ensures you remain on the most updated technology to reduce operating costs. You can also replace or add new com ponents to the workload with new services. This can provide significant increases in efficiency, so it's essential to regularly review your workload, and implement new ser vices and features. The following questions focus on these considerations for cost optimization. COST 10:  How do you evaluate new services? As AWS releases new services and features, it's a best practice to review your existing archi tectural decisions to ensure they continue to be the most cost effective. Archived When regularly reviewing your deployments, assess how newer services can help save you money. For example, Amazon Aurora on RDS can reduce costs for relational data bases. Using serverless such as Lambda can remove the need to operate and manage instances to run code. 41 AWS Well-Architected Framework Resources Refer to the following resources to learn more about our best practices for Cost Opti mization. Documentation • AWS Documentation Whitepaper • Cost Optimization Pillar Archived 42 AWS Well-Architected Framework The Review Process The review of architectures needs to be done in a consistent manner, with a blame free approach that encourages diving deep. It should be a light weight process (hours not days) that is a conversation and not an audit. The purpose of reviewing an archi tecture is to identify any critical issues that might need addressing or areas that could be improved. The outcome of the review is a set of actions that should improve the experience of a customer using the workload. As discussed in the “On Architecture” section, you will want each team member to take responsibility for the quality of its architecture. We recommend that the team members who build an architecture use the Well-Architected Framework to contin ually review their architecture, rather than holding a formal review meeting. A con tinuous approach allows your team members to update answers as the architecture evolves, and improve the architecture as you deliver features. The AWS Well-Architected Framework is aligned to the way that AWS reviews systems and services internally. It is premised on a set of design principles that influences ar chitectural approach, and questions that ensure that people don’t neglect areas that often featured in Root Cause Analysis (RCA). Whenever there is a significant issue with an internal system, AWS service, or customer, we look at the RCA to see if we could improve the review processes we use. Reviews should be applied at key milestones in the product lifecycle, early on in the design phase to avoid one-way doors 1 that are difficult to change, and then before the go-live date. After you go into production, your workload will continue to evolve as you add new features and change technology implementations. The architecture of a workload changes over time. You will need to follow good hygiene practices to stop its architectural characteristics from degrading as you evolve it. As you make sig nificant architecture changes you should follow a set of hygiene processes including a Well-Architected review. If you want to use the review as a one-time snapshot or independent measurement, you will want to ensure that you have all the right people in the conversation. Often, we find that reviews are the first time that a team truly understands what they have implemented. An approach that works well when reviewing another team's workload is to have a series of informal conversations about their architecture where you can glean the answers to most questions. You can then follow up with one or two meet ings where you can gain clarity or dive deep on areas of ambiguity or perceived risk. Here are some suggested items to facilitate your meetings: • A meeting room with whiteboards Archived 1Many decisions are reversible, two-way doors. Those decisions can use a light weight process. One-way doors are hard or impossible to reverse and require more inspection before making them. 43 AWS Well-Architected Framework • Print outs of any diagrams or design notes • Action list of questions that require out-of-band research to answer (for example, “did we enable encryption or not?”) After you have done a review, you should have a list of issues that you can prioritize based on your business context. You will also want to take into account the impact of those issues on the day-to-day work of your team. If you address these issues early, you could free up time to work on creating business value rather than solving recur ring problems. As you address issues, you can update your review to see how the ar chitecture is improving. While the value of a review is clear after you have done one, you may find that a new team might be resistant at first. Here are some objections that can be handled through educating the team on the benefits of a review: • “We are too busy!” (Often said when the team is getting ready for a big launch.) • If you are getting ready for a big launch you will want it to go smoothly. The re view will allow you to understand any problems you might have missed. • We recommend that you carry out reviews early in the product lifecycle to uncov er risks and develop a mitigation plan aligned with the feature delivery roadmap. • “We don’t have time to do anything with the results!” (Often said when there is an immovable event, such as the Super Bowl, that they are targeting.) • These events can’t be moved. Do you really want to go into it without knowing the risks in your architecture? Even if you don’t address all of these issues you can still have playbooks for handling them if they materialize • “We don’t want others to know the secrets of our solution implementation!” • If you point the team at the questions in the Well-Architected Framework, they will see that none of the questions reveal any commercial or technical propriety information. As you carry out multiple reviews with teams in your organization, you might identify thematic issues. For example, you might see that a group of teams has clusters of is sues in a particular pillar or topic. You will want to look at all your reviews in a holis tic manner, and identify any mechanisms, training, or principal engineering talks that could help address those thematic issues. Archived 44 AWS Well-Architected Framework Conclusion The AWS Well-Architected Framework provides architectural best practices across the f ive pillars for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. The Framework provides a set of questions that allows you to review an existing or proposed architecture. It also provides a set of AWS best prac tices for each pillar. Using the Framework in your architecture will help you produce stable and efficient systems, which allow you to focus on your functional require ments. Archived 45 AWS Well-Architected Framework Contributors The following individuals and organizations contributed to this document: • Rodney Lester: Senior Manager Well-Architected, Amazon Web Services • Brian Carlson: Operations Lead Well-Architected, Amazon Web Services • Ben Potter: Security Lead Well-Architected, Amazon Web Services • Eric Pullen: Performance Lead Well-Architected, Amazon Web Services • Seth Eliot: Reliability Lead Well-Architected, Amazon Web Services • Nathan Besh: Cost Lead Well-Architected, Amazon Web Services • Jon Steele: Sr. Technical Account Manager, Amazon Web Services • Ryan King: Technical Program Manager, Amazon Web Services • Erin Rifkin: Senior Product Manager, Amazon Web Services • Max Ramsay: Principal Security Solutions Architect, Amazon Web Services • Scott Paddock: Security Solutions Architect, Amazon Web Services • Callum Hughes: Solutions Architect, Amazon Web Services Archived 46 AWS Well-Architected Framework Further Reading AWS Cloud Compliance AWS Well-Architected Partner program AWS Well-Architected Tool AWS Well-Architected homepage Cost Optimization Pillar whitepaper Operational Excellence Pillar whitepaper Performance Efficiency Pillar whitepaper Reliability Pillar whitepaper Security Pillar whitepaper The Amazon Builders' Library Archived 47 Archived AWS Well-Architected Framework Document Revisions Table 2.  Major revisions: Date Description July 2020 Review and rewrite of most questions and answers. July 2019 Addition of AWS Well-Architected Tool, links to AWS Well-Architected Labs, and AWS Well-Architected Part ners, minor fixes to enable multiple language version of framework. November 2018 Review and rewrite of most questions and answers, to ensure questions focus on one topic at a time. This caused some previous questions to be split into multiple questions. Added common terms to definitions (work load, component etc). Changed presentation of question in main body to include descriptive text. June 2018 Updates to simplify question text, standardize answers, and improve readability. November 2017 Operational Excellence moved to front of pillars and rewritten so it frames other pillars. Refreshed other pil lars to reflect evolution of AWS. November 2016 Updated the Framework to include operational excel lence pillar, and revised and updated the other pillars to reduce duplication and incorporate learnings from car rying out reviews with thousands of customers. November 2015 Updated the Appendix with current Amazon Cloud Watch Logs information. October 2015 Original publication. 48 AWS Well-Architected Framework Appendix: Questions and Best Practices Operational Excellence Organization OPS 1  How do you determine what your priorities are? Everyone needs to understand their part in enabling business success. Have shared goals in order to set priorities for resources. This will maximize the benefits of your efforts. Best Practices: • Evaluate external customer needs: Involve key stakeholders, including business, devel opment, and operations teams, to determine where to focus efforts on external customer needs. This will ensure that you have a thorough understanding of the operations support that is required to achieve your desired business outcomes. • Evaluate internal customer needs: Involve key stakeholders, including business, devel opment, and operations teams, when determining where to focus efforts on internal cus tomer needs. This will ensure that you have a thorough understanding of the operations support that is required to achieve business outcomes. • Evaluate governance requirements: Ensure that you are aware of guidelines or obliga tions defined by your organization that may mandate or emphasize specific focus. Eval uate internal factors, such as organization policy, standards, and requirements. Validate that you have mechanisms to identify changes to governance. If no governance require ments are identified, ensure that you have applied due diligence to this determination. • Evaluate compliance requirements: Evaluate external factors, such as regulatory compli ance requirements and industry standards, to ensure that you are aware of guidelines or obligations that may mandate or emphasize specific focus. If no compliance requirements are identified, ensure that you apply due diligence to this determination. • Evaluate threat landscape: Evaluate threats to the business (for example, competition, business risk and liabilities, operational risks, and information security threats) and main tain current information in a risk registry. Include the impact of risks when determining where to focus efforts. • Evaluate tradeoffs: Evaluate the impact of tradeoffs between competing interests or al ternative approaches, to help make informed decisions when determining where to focus efforts or choosing a course of action. For example, accelerating speed to market for new features may be emphasized over cost optimization, or you may choose a relational data base for non-relational data to simplify the effort to migrate a system, rather than migrat ing to a database optimized for your data type and updating your application. Archived • Manage benefits and risks: Manage benefits and risks to make informed decisions when determining where to focus efforts. For example, it may be beneficial to deploy a work load with unresolved issues so that significant new features can be made available to cus tomers. It may be possible to mitigate associated risks, or it may become unacceptable to allow a risk to remain, in which case you will take action to address the risk. 49 AWS Well-Architected Framework OPS 2  How do you structure your organization to support your business outcomes? Your teams must understand their part in achieving business outcomes. Teams need to un derstand their roles in the success of other teams, the role of other teams in their success, and have shared goals. Understanding responsibility, ownership, how decisions are made, and who has authority to make decisions will help focus efforts and maximize the benefits from your teams. Best Practices: • Resources have identified owners: Understand who has ownership of each application, workload, platform, and infrastructure component, what business value is provided by that component, and why that ownership exists. Understanding the business value of these in dividual components and how they support business outcomes informs the processes and procedures applied against them. • Processes and procedures have identified owners: Understand who has ownership of the definition of individual processes and procedures, why those specific process and proce dures are used, and why that ownership exists. Understanding the reasons that specific processes and procedures are used enables identification of improvement opportunities. • Operations activities have identified owners responsible for their performance: Under stand who has responsibility to perform specific activities on defined workloads and why that responsibility exists. Understanding who has responsibility to perform activities in forms who will conduct the activity, validate the result, and provide feedback to the owner of the activity. • Team members know what they are responsible for: Understanding the responsibilities of your role and how you contribute to business outcomes informs the prioritization of your tasks and why your role is important. This enables team members to recognize needs and respond appropriately. • Mechanisms exist to identify responsibility and ownership: Where no individual or team is identified, there are defined escalation paths to someone with the authority to assign ownership or plan for that need to be addressed. • Mechanisms exist to request additions, changes, and exceptions: You are able to make requests to owners of processes, procedures, and resources. Make informed decisions to approve requests where viable and determined to be appropriate after an evaluation of benefits and risks. • Responsibilities between teams are predefined or negotiated: There are defined or ne gotiated agreements between teams describing how they work with and support each oth er (for example, response times, service level objectives, or service level agreements). Un derstanding the impact of the teams’ work on business outcomes, and the outcomes of other teams and organizations, informs the prioritization of their tasks and enables them to respond appropriately. Archived 50 AWS Well-Architected Framework OPS 3  How does your organizational culture support your business outcomes? Provide support for your team members so that they can be more effective in taking action and supporting your business outcome. Best Practices: • Executive Sponsorship: Senior leadership clearly sets expectations for the organization and evaluates success. Senior leadership is the sponsor, advocate, and driver for the adop tion of best practices and evolution of the organization • Team members are empowered to take action when outcomes are at risk: The workload owner has defined guidance and scope empowering team members to respond when out comes are at risk. Escalation mechanisms are used to get direction when events are outside of the defined scope. • Escalation is encouraged: Team members have mechanisms and are encouraged to esca late concerns to decision makers and stakeholders if they believe outcomes are at risk. Es calation should be performed early and often so that risks can be identified, and prevent ed from causing incidents. • Communications are timely, clear, and actionable: Mechanisms exist and are used to pro vide timely notice to team members of known risks and planned events. Necessary con text, details, and time (when possible) are provided to support determining if action is nec essary, what action is required, and to take action in a timely manner. For example, provid ing notice of software vulnerabilities so that patching can be expedited, or providing no tice of planned sales promotions so that a change freeze can be implemented to avoid the risk of service disruption. • Experimentation is encouraged: Experimentation accelerates learning and keeps team members interested and engaged. An undesired result is a successful experiment that has identified a path that will not lead to success. Team members are not punished for suc cessful experiments with undesired results. Experimentation is required for innovation to happen and turn ideas into outcomes. • Team members are enabled and encouraged to maintain and grow their skill sets: Teams must grow their skill sets to adopt new technologies, and to support changes in de mand and responsibilities in support of your workloads. Growth of skills in new technolo gies is frequently a source of team member satisfaction and supports innovation. Support your team members’ pursuit and maintenance of industry certifications that validate and acknowledge their growing skills. Cross train to promote knowledge transfer and reduce the risk of significant impact when you lose skilled and experienced team members with institutional knowledge. Provide dedicated structured time for learning. • Resource teams appropriately: Maintain team member capacity, and provide tools and resources, to support your workload needs. Overtasking team members increases the risk of incidents resulting from human error. Investments in tools and resources (for example, providing automation for frequently executed activities) can scale the effectiveness of your team, enabling them to support additional activities. Archived • Diverse opinions are encouraged and sought within and across teams: Leverage cross organizational diversity to seek multiple unique perspectives. Use this perspective to in crease innovation, challenge your assumptions, and reduce the risk of confirmation bias. Grow inclusion, diversity, and accessibility within your teams to gain beneficial perspec tives. 51 AWS Well-Architected Framework Prepare OPS 4  How do you design your workload so that you can understand its state? Design your workload so that it provides the information necessary across all components (for example, metrics, logs, and traces) for you to understand its internal state. This enables you to provide effective responses when appropriate. Best Practices: • Implement application telemetry: Instrument your application code to emit informa tion about its internal state, status, and achievement of business outcomes. For example, queue depth, error messages, and response times. Use this information to determine when a response is required. • Implement and configure workload telemetry: Design and configure your workload to emit information about its internal state and current status. For example, API call volume, HTTP status codes, and scaling events. Use this information to help determine when a re sponse is required. • Implement user activity telemetry: Instrument your application code to emit informa tion about user activity, for example, click streams, or started, abandoned, and completed transactions. Use this information to help understand how the application is used, patterns of usage, and to determine when a response is required. • Implement dependency telemetry: Design and configure your workload to emit informa tion about the status (for example, reachability or response time) of resources it depends on. Examples of external dependencies can include, external databases, DNS, and network connectivity. Use this information to determine when a response is required. • Implement transaction traceability: Implement your application code and configure your workload components to emit information about the flow of transactions across the work load. Use this information to determine when a response is required and to assist you in identifying the factors contributing to an issue. Archived 52 AWS Well-Architected Framework OPS 5  How do you reduce defects, ease remediation, and improve flow into production? Adopt approaches that improve flow of changes into production, that enable refactoring, fast feedback on quality, and bug fixing. These accelerate beneficial changes entering pro duction, limit issues deployed, and enable rapid identification and remediation of issues in troduced through deployment activities. Best Practices: • Use version control: Use version control to enable tracking of changes and releases. • Test and validate changes: Test and validate changes to help limit and detect errors. Au tomate testing to reduce errors caused by manual processes, and reduce the level of effort to test. • Use configuration management systems: Use configuration management systems to make and track configuration changes. These systems reduce errors caused by manual processes and reduce the level of effort to deploy changes. • Use build and deployment management systems: Use build and deployment manage ment systems. These systems reduce errors caused by manual processes and reduce the level of effort to deploy changes. • Perform patch management: Perform patch management to gain features, address issues, and remain compliant with governance. Automate patch management to reduce errors caused by manual processes, and reduce the level of effort to patch. • Share design standards: Share best practices across teams to increase awareness and maximize the benefits of development efforts. • Implement practices to improve code quality: Implement practices to improve code qual ity and minimize defects. For example, test-driven development, code reviews, and stan dards adoption. • Use multiple environments: Use multiple environments to experiment, develop, and test your workload. Use increasing levels of controls as environments approach production to gain confidence your workload will operate as intended when deployed. • Make frequent, small, reversible changes: Frequent, small, and reversible changes reduce the scope and impact of a change. This eases troubleshooting, enables faster remediation, and provides the option to roll back a change. • Fully automate integration and deployment: Automate build, deployment, and testing of the workload. This reduces errors caused by manual processes and reduces the effort to deploy changes. Archived 53 AWS Well-Architected Framework OPS 6  How do you mitigate deployment risks? Adopt approaches that provide fast feedback on quality and enable rapid recovery from changes that do not have desired outcomes. Using these practices mitigates the impact of is sues introduced through the deployment of changes. Best Practices: • Plan for unsuccessful changes: Plan to revert to a known good state, or remediate in the production environment if a change does not have the desired outcome. This preparation reduces recovery time through faster responses. • Test and validate changes: Test changes and validate the results at all lifecycle stages to confirm new features and minimize the risk and impact of failed deployments. • Use deployment management systems: Use deployment management systems to track and implement change. This reduces errors cause by manual processes and reduces the ef fort to deploy changes. • Test using limited deployments: Test with limited deployments alongside existing sys tems to confirm desired outcomes prior to full scale deployment. For example, use deploy ment canary testing or one-box deployments. • Deploy using parallel environments: Implement changes onto parallel environments, and then transition over to the new environment. Maintain the prior environment until there is confirmation of successful deployment. Doing so minimizes recovery time by enabling roll back to the previous environment. • Deploy frequent, small, reversible changes: Use frequent, small, and reversible changes to reduce the scope of a change. This results in easier troubleshooting and faster remedia tion with the option to roll back a change. • Fully automate integration and deployment: Automate build, deployment, and testing of the workload. This reduces errors cause by manual processes and reduces the effort to de ploy changes. • Automate testing and rollback: Automate testing of deployed environments to confirm desired outcomes. Automate rollback to previous known good state when outcomes are not achieved to minimize recovery time and reduce errors caused by manual processes. Archived 54 AWS Well-Architected Framework OPS 7  How do you know that you are ready to support a workload? Evaluate the operational readiness of your workload, processes and procedures, and person nel to understand the operational risks related to your workload. Best Practices: • Ensure personnel capability: Have a mechanism to validate that you have the appropri ate number of trained personnel to provide support for operational needs. Train personnel and adjust personnel capacity as necessary to maintain effective support. • Ensure consistent review of operational readiness: Ensure you have a consistent review of your readiness to operate a workload. Reviews must include, at a minimum, the oper ational readiness of the teams and the workload, and security requirements. Implement review activities in code and trigger automated review in response to events where ap propriate, to ensure consistency, speed of execution, and reduce errors caused by manual processes. • Use runbooks to perform procedures: Runbooks are documented procedures to achieve specific outcomes. Enable consistent and prompt responses to well-understood events by documenting procedures in runbooks. Implement runbooks as code and trigger the execu tion of runbooks in response to events where appropriate, to ensure consistency, speed re sponses, and reduce errors caused by manual processes. • Use playbooks to investigate issues: Enable consistent and prompt responses to issues that are not well understood, by documenting the investigation process in playbooks. Playbooks are the predefined steps performed to identify the factors contributing to a fail ure scenario. The results from any process step are used to determine the next steps to take until the issue is identified or escalated. • Make informed decisions to deploy systems and changes: Evaluate the capabilities of the team to support the workload and the workload's compliance with governance. Evaluate these against the benefits of deployment when determining whether to transition a sys tem or change into production. Understand the benefits and risks to make informed deci sions. Archived 55 AWS Well-Architected Framework Operate OPS 8  How do you understand the health of your workload? Define, capture, and analyze workload metrics to gain visibility to workload events so that you can take appropriate action. Best Practices: • Identify key performance indicators: Identify key performance indicators (KPIs) based on desired business outcomes (for example, order rate, customer retention rate, and profit versus operating expense) and customer outcomes (for example, customer satisfaction). Evaluate KPIs to determine workload success. • Define workload metrics: Define workload metrics to measure the achievement of KPIs (for example, abandoned shopping carts, orders placed, cost, price, and allocated workload expense). Define workload metrics to measure the health of the workload (for example, interface response time, error rate, requests made, requests completed, and utilization). Evaluate metrics to determine if the workload is achieving desired outcomes, and to un derstand the health of the workload. • Collect and analyze workload metrics: Perform regular proactive reviews of metrics to identify trends and determine where appropriate responses are needed. • Establish workload metrics baselines: Establish baselines for metrics to provide expected values as the basis for comparison and identification of under and over performing com ponents. Identify thresholds for improvement, investigation, and intervention. • Learn expected patterns of activity for workload: Establish patterns of workload activity to identify anomalous behavior so that you can respond appropriately if required. • Alert when workload outcomes are at risk: Raise an alert when workload outcomes are at risk so that you can respond appropriately if necessary. • Alert when workload anomalies are detected: Raise an alert when workload anomalies are detected so that you can respond appropriately if necessary. • Validate the achievement of outcomes and the effectiveness of KPIs and metrics : Cre ate a business-level view of your workload operations to help you determine if you are sat isfying needs and to identify areas that need improvement to reach business goals. Vali date the effectiveness of KPIs and metrics and revise them if necessary. Archived 56 AWS Well-Architected Framework OPS 9  How do you understand the health of your operations? Define, capture, and analyze operations metrics to gain visibility to operations events so that you can take appropriate action. Best Practices: • Identify key performance indicators: Identify key performance indicators (KPIs) based on desired business (for example, new features delivered) and customer outcomes (for exam ple, customer support cases). Evaluate KPIs to determine operations success. • Define operations metrics: Define operations metrics to measure the achievement of KPIs (for example, successful deployments, and failed deployments). Define operations met rics to measure the health of operations activities (for example, mean time to detect an in cident (MTTD), and mean time to recovery (MTTR) from an incident). Evaluate metrics to determine if operations are achieving desired outcomes, and to understand the health of your operations activities. • Collect and analyze operations metrics: Perform regular, proactive reviews of metrics to identify trends and determine where appropriate responses are needed. • Establish operations metrics baselines: Establish baselines for metrics to provide expect ed values as the basis for comparison and identification of under and over performing op erations activities. • Learn the expected patterns of activity for operations: Establish patterns of operations activities to identify anomalous activity so that you can respond appropriately if necessary. • Alert when operations outcomes are at risk: Raise an alert when operations outcomes are at risk so that you can respond appropriately if necessary. • Alert when operations anomalies are detected: Raise an alert when operations anomalies are detected so that you can respond appropriately if necessary. • Validate the achievement of outcomes and the effectiveness of KPIs and metrics : Cre ate a business-level view of your operations activities to help you determine if you are sat isfying needs and to identify areas that need improvement to reach business goals. Vali date the effectiveness of KPIs and metrics and revise them if necessary. Archived 57 AWS Well-Architected Framework OPS 10  How do you manage workload and operations events? Prepare and validate procedures for responding to events to minimize their disruption to your workload. Best Practices: • Use processes for event, incident, and problem management: Have processes to address observed events, events that require intervention (incidents), and events that require in tervention and either recur or cannot currently be resolved (problems). Use these process es to mitigate the impact of these events on the business and your customers by ensuring timely and appropriate responses. • Have a process per alert: Have a well-defined response (runbook or playbook), with a specifically identified owner, for any event for which you raise an alert. This ensures effec tive and prompt responses to operations events and prevents actionable events from be ing obscured by less valuable notifications. • Prioritize operational events based on business impact: Ensure that when multiple events require intervention, those that are most significant to the business are addressed f irst. For example, impacts can include loss of life or injury, financial loss, or damage to reputation or trust. • Define escalation paths: Define escalation paths in your runbooks and playbooks, includ ing what triggers escalation, and procedures for escalation. Specifically identify owners for each action to ensure effective and prompt responses to operations events. • Enable push notifications: Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and again when the services return to normal operating conditions, to enable users to take appropriate action. • Communicate status through dashboards: Provide dashboards tailored to their target au diences (for example, internal technical teams, leadership, and customers) to communicate the current operating status of the business and provide metrics of interest. • Automate responses to events: Automate responses to events to reduce errors caused by manual processes, and to ensure prompt and consistent responses. Archived 58 AWS Well-Architected Framework Evolve OPS 11  How do you evolve operations? Dedicate time and resources for continuous incremental improvement to evolve the effec tiveness and efficiency of your operations. Best Practices: • Have a process for continuous improvement: Regularly evaluate and prioritize opportuni ties for improvement to focus efforts where they can provide the greatest benefits. • Perform post-incident analysis: Review customer-impacting events, and identify the con tributing factors and preventative actions. Use this information to develop mitigations to limit or prevent recurrence. Develop procedures for prompt and effective responses. Com municate contributing factors and corrective actions as appropriate, tailored to target au diences. • Implement feedback loops: Include feedback loops in your procedures and workloads to help you identify issues and areas that need improvement. • Perform Knowledge Management: Mechanisms exist for your team members to discover the information that they are looking for in a timely manner, access it, and identify that it’s current and complete. Mechanisms are present to identify needed content, content in need of refresh, and content that should be archived so that it’s no longer referenced. • Define drivers for improvement: Identify drivers for improvement to help you evaluate and prioritize opportunities. • Validate insights: Review your analysis results and responses with cross-functional teams and business owners. Use these reviews to establish common understanding, identify addi tional impacts, and determine courses of action. Adjust responses as appropriate. • Perform operations metrics reviews: Regularly perform retrospective analysis of opera tions metrics with cross-team participants from different areas of the business. Use these reviews to identify opportunities for improvement, potential courses of action, and to share lessons learned. • Document and share lessons learned: Document and share lessons learned from the exe cution of operations activities so that you can use them internally and across teams. • Allocate time to make improvements: Dedicate time and resources within your processes to make continuous incremental improvements possible. Archived 59 AWS Well-Architected Framework Security Security SEC 1  How do you securely operate your workload? To operate your workload securely, you must apply overarching best practices to every area of security. Take requirements and processes that you have defined in operational excellence at an organizational and workload level, and apply them to all areas. Staying up to date with AWS and industry recommendations and threat intelligence helps you evolve your threat model and control objectives. Automating security processes, testing, and validation allow you to scale your security operations. Best Practices: • Separate workloads using accounts: Organize workloads in separate accounts and group accounts based on function or a common set of controls rather than mirroring your com pany’s reporting structure. Start with security and infrastructure in mind to enable your or ganization to set common guardrails as your workloads grow. • Secure AWS account: Secure access to your accounts, for example by enabling MFA and restrict use of the root user, and configure account contacts. • Identify and validate control objectives: Based on your compliance requirements and risks identified from your threat model, derive and validate the control objectives and con trols that you need to apply to your workload. Ongoing validation of control objectives and controls help you measure the effectiveness of risk mitigation. • Keep up to date with security threats: Recognize attack vectors by staying up to date with the latest security threats to help you define and implement appropriate controls. • Keep up to date with security recommendations: Stay up to date with both AWS and in dustry security recommendations to evolve the security posture of your workload. • Automate testing and validation of security controls in pipelines: Establish secure base lines and templates for security mechanisms that are tested and validated as part of your build, pipelines, and processes. Use tools and automation to test and validate all security controls continuously. For example, scan items such as machine images and infrastructure as code templates for security vulnerabilities, irregularities, and drift from an established baseline at each stage. • Identify and prioritize risks using a threat model: Use a threat model to identify and maintain an up-to-date register of potential threats. Prioritize your threats and adapt your security controls to prevent, detect, and respond. Revisit and maintain this in the context of the evolving security landscape. Archived • Evaluate and implement new security services and features regularly: AWS and APN Partners constantly release new features and services that allow you to evolve the security posture of your workload. 60 AWS Well-Architected Framework Identity and Access Management SEC 2  How do you manage identities for people and machines? There are two types of identities you need to manage when approaching operating secure AWS workloads. Understanding the type of identity you need to manage and grant access helps you ensure the right identities have access to the right resources under the right con ditions. Human Identities: Your administrators, developers, operators, and end users require an identity to access your AWS environments and applications. These are members of your organization, or external users with whom you collaborate, and who interact with your AWS resources via a web browser, client application, or interactive command-line tools. Machine Identities: Your service applications, operational tools, and workloads require an identity to make requests to AWS services - for example, to read data. These identities include machines running in your AWS environment such as Amazon EC2 instances or AWS Lambda functions. You may also manage machine identities for external parties who need access. Additionally, you may also have machines outside of AWS that need access to your AWS environment. Best Practices: • Use strong sign-in mechanisms: Enforce minimum password length, and educate users to avoid common or re-used passwords. Enforce multi-factor authentication (MFA) with soft ware or hardware mechanisms to provide an additional layer. • Use temporary credentials: Require identities to dynamically acquire temporary creden tials. For workforce identities, use AWS Single Sign-On, or federation with IAM roles to ac cess AWS accounts. For machine identities, require the use of IAM roles instead of long term access keys. • Store and use secrets securely: For workforce and machine identities that require secrets such as passwords to third party applications, store them with automatic rotation using the latest industry standards in a specialized service. • Rely on a centralized identity provider: For workforce identities, rely on an identity provider that enables you to manage identities in a centralized place. This enables you to create, manage, and revoke access from a single location making it easier to manage ac cess. This reduces the requirement for multiple credentials and provides an opportunity to integrate with HR processes. • Audit and rotate credentials periodically: When you cannot rely on temporary credentials and require long term credentials, audit credentials to ensure that the defined controls (for example, MFA) are enforced, rotated regularly, and have appropriate access level. • Leverage user groups and attributes: Place users with common security requirements in groups defined by your identity provider, and put mechanisms in place to ensure that user attributes that may be used for access control (e.g., department or location) are cor rect and updated. Use these groups and attributes, rather than individual users, to control access. This allows you to manage access centrally by changing a user’s group member ship or attributes once, rather than updating many individual policies when a user’s access needs change. Archived 61 AWS Well-Architected Framework SEC 3  How do you manage permissions for people and machines? Manage permissions to control access to people and machine identities that require access to AWS and your workload. Permissions control who can access what, and under what condi tions. Best Practices: • Define access requirements: Each component or resource of your workload needs to be accessed by administrators, end users, or other components. Have a clear definition of who or what should have access to each component, choose the appropriate identity type and method of authentication and authorization. • Grant least privilege access: Grant only the access that identities require by allowing ac cess to specific actions on specific AWS resources under specific conditions. Rely on groups and identity attributes to dynamically set permissions at scale, rather than defining per missions for individual users. For example, you can allow a group of developers access to manage only resources for their project. This way, when a developer is removed from the group, access for the developer is revoked everywhere that group was used for access con trol, without requiring any changes to the access policies. • Establish emergency access process: A process that allows emergency access to your workload in the unlikely event of an automated process or pipeline issue. This will help you rely on least privilege access, but ensure users can obtain the right level of access when they require it. For example, establish a process for administrators to verify and approve their request. • Reduce permissions continuously: As teams and workloads determine what access they need, remove permissions they no longer use and establish review processes to achieve least privilege permissions. Continuously monitor and reduce unused identities and per missions. • Define permission guardrails for your organization: Establish common controls that re strict access to all identities in your organization. For example, you can restrict access to specific AWS Regions, or prevent your operators from deleting common resources, such as an IAM role used for your central security team. • Manage access based on life cycle: Integrate access controls with operator and applica tion life cycle and your centralized federation provider. For example, remove a user’s ac cess when they leave the organization or change roles. • Analyze public and cross account access: Continuously monitor findings that highlight public and cross account access. Reduce public access and cross account access to only re sources that require this type of access. • Share resources securely: Govern the consumption of shared resources across accounts or within your AWS Organization. Monitor shared resources and review shared resource ac cess. Archived 62 AWS Well-Architected Framework Detection SEC 4  How do you detect and investigate security events? Capture and analyze events from logs and metrics to gain visibility. Take action on security events and potential threats to help secure your workload. Best Practices: • Configure service and application logging: Configure logging throughout the workload, including application logs, resource logs, and AWS service logs. For example, ensure that AWS CloudTrail, Amazon CloudWatch Logs, Amazon GuardDuty and AWS Security Hub are enabled for all accounts within your organization. • Analyze logs, findings, and metrics centrally: All logs, metrics, and telemetry should be collected centrally, and automatically analyzed to detect anomalies and indicators of unauthorized activity. A dashboard can provide you easy to access insight into real-time health. For example, ensure that Amazon GuardDuty and Security Hub logs are sent to a central location for alerting and analysis. • Automate response to events: Using automation to investigate and remediate events re duces human effort and error, and enables you to scale investigation capabilities. Regular reviews will help you tune automation tools, and continuously iterate. For example, auto mate responses to Amazon GuardDuty events by automating the first investigation step, then iterate to gradually remove human effort. • Implement actionable security events: Create alerts that are sent to and can be actioned by your team. Ensure that alerts include relevant information for the team to take action. For example, ensure that Amazon GuardDuty and AWS Security Hub alerts are sent to the team to action, or sent to response automation tooling with the team remaining informed by messaging from the automation framework. Archived 63 AWS Well-Architected Framework Infrastructure Protection SEC 5  How do you protect your network resources? Any workload that has some form of network connectivity, whether it’s the internet or a pri vate network, requires multiple layers of defense to help protect from external and internal network-based threats. Best Practices: • Create network layers: Group components that share reachability requirements into lay ers. For example, a database cluster in a VPC with no need for internet access should be placed in subnets with no route to or from the internet. In a serverless workload operating without a VPC, similar layering and segmentation with microservices can achieve the same goal. • Control traffic at all layers: Apply controls with a defense in depth approach for both in bound and outbound traffic. For example, for Amazon Virtual Private Cloud (VPC) this in cludes security groups, Network ACLs, and subnets. For AWS Lambda, consider running in your private VPC with VPC-based controls. • Automate network protection: Automate protection mechanisms to provide a self-de fending network based on threat intelligence and anomaly detection. For example, intru sion detection and prevention tools that can pro-actively adapt to current threats and re duce their impact. • Implement inspection and protection: Inspect and filter your traffic at each layer. For ex ample, use a web application firewall to help protect against inadvertent access at the ap plication network layer. For Lambda functions, third-party tools can add application-layer firewalling to your runtime environment. Archived 64 AWS Well-Architected Framework SEC 6  How do you protect your compute resources? Compute resources in your workload require multiple layers of defense to help protect from external and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more. Best Practices: • Perform vulnerability management: Frequently scan and patch for vulnerabilities in your code, dependencies, and in your infrastructure to help protect against new threats. • Reduce attack surface: Reduce your attack surface by hardening operating systems, mini mizing components, libraries, and externally consumable services in use. • Implement managed services: Implement services that manage resources, such as Ama zon RDS, AWS Lambda, and Amazon ECS, to reduce your security maintenance tasks as part of the shared responsibility model. • Automate compute protection: Automate your protective compute mechanisms including vulnerability management, reduction in attack surface, and management of resources. • Enable people to perform actions at a distance: Removing the ability for interactive ac cess reduces the risk of human error, and the potential for manual configuration or man agement. For example, use a change management workflow to deploy EC2 instances us ing infrastructure as code, then manage EC2 instances using tools instead of allowing di rect access or a bastion host. • Validate software integrity: Implement mechanisms (for example, code signing) to vali date that the software, code, and libraries used in the workload are from trusted sources and have not been tampered with. Archived 65 AWS Well-Architected Framework Data Protection SEC 7  How do you classify your data? Classification provides a way to categorize data, based on criticality and sensitivity in order to help you determine appropriate protection and retention controls. Best Practices: • Identify the data within your workload: This includes the type and classification of data, the associated business processes. data owner, applicable legal and compliance require ments, where it’s stored, and the resulting controls that are needed to be enforced. This may include classifications to indicate if the data is intended to be publicly available, if the data is internal use only such as customer personally identifiable information (PII), or if the data is for more restricted access such as intellectual property, legally privileged or marked sensititve, and more. • Define data protection controls: Protect data according to its classification level. For ex ample, secure data classified as public by using relevant recommendations while protect ing sensitive data with additional controls. • Automate identification and classification: Automate identification and classification of data to reduce the risk of human error from manual interactions. • Define data lifecycle management: Your defined lifecycle strategy should be based on sensitivity level, as well as legal and organization requirements. Aspects including the du ration you retain data for, data destruction, data access management, data transformation, and data sharing should be considered. Archived 66 AWS Well-Architected Framework SEC 8  How do you protect your data at rest? Protect your data at rest by implementing multiple controls, to reduce the risk of unautho rized access or mishandling. Best Practices: • Implement secure key management: Encryption keys must be stored securely, with strict access control, for example, by using a key management service such as AWS KMS. Consid er using different keys, and access control to the keys, combined with the AWS IAM and re source policies, to align with data classification levels and segregation requirements. • Enforce encryption at rest: Enforce your encryption requirements based on the latest standards and recommendations to help protect your data at rest. • Automate data at rest protection: Use automated tools to validate and enforce data at rest protection continuously, for example, verify that there are only encrypted storage re sources. • Enforce access control: Enforce access control with least privileges and mechanisms, in cluding backups, isolation, and versioning, to help protect your data at rest. Prevent opera tors from granting public access to your data. • Use mechanisms to keep people away from data: Keep all users away from directly ac cessing sensitive data and systems under normal operational circumstances. For example, provide a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not used, determine which controls and processes are required to adequately provide a normally disabled break-glass access mechanism. SEC 9  How do you protect your data in transit? Protect your data in transit by implementing multiple controls to reduce the risk of unautho rized access or loss. Best Practices: • Implement secure key and certificate management: Store encryption keys and certifi cates securely and rotate them at appropriate time intervals while applying strict access control; for example, by using a certificate management service, such as AWS Certificate Manager (ACM). • Enforce encryption in transit: Enforce your defined encryption requirements based on ap propriate standards and recommendations to help you meet your organizational, legal, and compliance requirements. • Automate detection of unintended data access: Use tools such as GuardDuty to automat ically detect attempts to move data outside of defined boundaries based on data classifi cation level, for example, to detect a trojan that is copying data to an unknown or untrust ed network using the DNS protocol. Archived • Authenticate network communications: Verify the identity of communications by using protocols that support authentication, such as Transport Layer Security (TLS) or IPsec. 67 AWS Well-Architected Framework Incident Response SEC 10  How do you anticipate, respond to, and recover from incidents? Preparation is critical to timely and effective investigation, response to, and recovery from security incidents to help minimize disruption to your organization. Best Practices: • Identify key personnel and external resources: Identify internal and external personnel, resources, and legal obligations that would help your organization respond to an incident. • Develop incident management plans: Create plans to help you respond to, communicate during, and recover from an incident. For example, you can start an incident response plan with the most likely scenarios for your workload and organization. Include how you would communicate and escalate both internally and externally. • Prepare forensic capabilities: Identify and prepare forensic investigation capabilities that are suitable, including external specialists, tools, and automation. • Automate containment capability: Automate containment and recovery of an incident to reduce response times and organizational impact. • Pre-provision access: Ensure that incident responders have the correct access pre-provi sioned into AWS to reduce the time for investigation through to recovery. • Pre-deploy tools: Ensure that security personnel have the right tools pre-deployed into AWS to reduce the time for investigation through to recovery. • Run game days: Practice incident response game days (simulations) regularly, incorporate lessons learned into your incident management plans, and continuously improve. Archived 68 AWS Well-Architected Framework Reliability Foundations REL 1  How do you manage service quotas and constraints? For cloud-based workload architectures, there are service quotas (which are also referred to as service limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can push bits down a fiber-optic cable, or the amount of storage on a physical disk. Best Practices: • Aware of service quotas and constraints: You are aware of your default quotas and quo ta increase requests for your workload architecture. You additionally know which resource constraints, such as disk or network, are potentially impactful. • Manage service quotas across accounts and regions: If you are using multiple AWS ac counts or AWS Regions, ensure that you request the appropriate quotas in all environ ments in which your production workloads run. • Accommodate fixed service quotas and constraints through architecture: Be aware of unchangeable service quotas and physical resources, and architect to prevent these from impacting reliability. • Monitor and manage quotas: Evaluate your potential usage and increase your quotas ap propriately allowing for planned growth in usage. • Automate quota management: Implement tools to alert you when thresholds are being approached. By using AWS Service Quotas APIs, you can automate quota increase requests. • Ensure that a sufficient gap exists between the current quotas and the maximum us age to accommodate failover: When a resource fails, it may still be counted against quo tas until its successfully terminated. Ensure that your quotas cover the overlap of all failed resources with replacements before the failed resources are terminated. You should con sider an Availability Zone failure when calculating this gap. Archived 69 AWS Well-Architected Framework REL 2  How do you plan your network topology? Workloads often exist in multiple environments. These include multiple cloud environments (both publicly accessible and private) and possibly your existing data center infrastructure. Plans must include network considerations such as intra- and inter-system connectivity, pub lic IP address management, private IP address management, and domain name resolution. Best Practices: • Use highly available network connectivity for your workload public endpoints: These endpoints and the routing to them must be highly available. To achieve this, use highly available DNS, content delivery networks (CDNs), API Gateway, load balancing, or reverse proxies. • Provision redundant connectivity between private networks in the cloud and on premises environments: Use multiple AWS Direct Connect (DX) connections or VPN tun nels between separately deployed private networks. Use multiple DX locations for high availability. If using multiple AWS Regions, ensure redundancy in at least two of them. You might want to evaluate AWS Marketplace appliances that terminate VPNs. If you use AWS Marketplace appliances, deploy redundant instances for high availability in different Avail ability Zones. • Ensure IP subnet allocation accounts for expansion and availability: Amazon VPC IP ad dress ranges must be large enough to accommodate workload requirements, including factoring in future expansion and allocation of IP addresses to subnets across Availability Zones. This includes load balancers, EC2 instances, and container-based applications. • Prefer hub-and-spoke topologies over many-to-many mesh: If more than two network address spaces (for example, VPCs and on-premises networks) are connected via VPC peer ing, AWS Direct Connect, or VPN, then use a hub-and-spoke model, like that provided by AWS Transit Gateway. • Enforce non-overlapping private IP address ranges in all private address spaces where they are connected: The IP address ranges of each of your VPCs must not overlap when peered or connected via VPN. You must similarly avoid IP address conflicts between a VPC and on-premises environments or with other cloud providers that you use. You must also have a way to allocate private IP address ranges when needed. Archived 70 AWS Well-Architected Framework Workload Architecture REL 3  How do you design your workload service architecture? Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a microservices architecture. Service-oriented architecture (SOA) is the practice of making soft ware components reusable via service interfaces. Microservices architecture goes further to make components smaller and simpler. Best Practices: • Choose how to segment your workload: Monolithic architecture should be avoided. In stead, you should choose between SOA and microservices. When making each choice, bal ance the benefits against the complexities—what is right for a new product racing to first launch is different than what a workload built to scale from the start needs. The benefits of using smaller segments include greater agility, organizational flexibility, and scalability. Complexities include possible increased latency, more complex debugging, and increased operational burden • Build services focused on specific business domains and functionality: SOA builds ser vices with well-delineated functions defined by business needs. Microservices use domain models and bounded context to limit this further so that each service does just one thing. Focusing on specific functionality enables you to differentiate the reliability requirements of different services, and target investments more specifically. A concise business problem and having a small team associated with each service also enables easier organizational scaling. • Provide service contracts per API: Service contracts are documented agreements between teams on service integration and include a machine-readable API definition, rate limits, and performance expectations. A versioning strategy allows clients to continue using the existing API and migrate their applications to the newer API when they are ready. Deploy ment can happen anytime, as long as the contract is not violated. The service provider team can use the technology stack of their choice to satisfy the API contract. Similarly, the service consumer can use their own technology. Archived 71 AWS Well-Architected Framework REL 4  How do you design interactions in a distributed system to prevent failures? Distributed systems rely on communications networks to interconnect components, such as servers or services. Your workload must operate reliably despite data loss or latency in these networks. Components of the distributed system must operate in a way that does not neg atively impact other components or the workload. These best practices prevent failures and improve mean time between failures (MTBF). Best Practices: • Identify which kind of distributed system is required: Hard real-time distributed systems require responses to be given synchronously and rapidly, while soft real-time systems have a more generous time window of minutes or more for response. Offline systems handle responses through batch or asynchronous processing. Hard real-time distributed systems have the most stringent reliability requirements. • Implement loosely coupled dependencies: Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increas ing resiliency and agility • Make all responses idempotent: An idempotent service promises that each request is completed exactly once, such that making multiple identical requests has the same ef fect as making a single request. An idempotent service makes it easier for a client to im plement retries without fear that a request will be erroneously processed multiple times. To do this, clients can issue API requests with an idempotency token—the same token is used whenever the request is repeated. An idempotent service API uses the token to return a response identical to the response that was returned the first time that the request was completed. • Do constant work: Systems can fail when there are large, rapid changes in load. For exam ple, a health check system that monitors the health of thousands of servers should send the same size payload (a full snapshot of the current state) each time. Whether no servers are failing, or all of them, the health check system is doing constant work with no large, rapid changes. Archived 72 AWS Well-Architected Framework REL 5  How do you design interactions in a distributed system to mitigate or withstand failures? Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a way that does not negatively impact other components or the workload. These best practices enable workloads to withstand stresses or failures, more quickly recover from them, and mitigate the impact of such impairments. The result is improved mean time to recovery (MTTR). Best Practices: • Implement graceful degradation to transform applicable hard dependencies into soft dependencies: When a component's dependencies are unhealthy, the component itself can still function, although in a degraded manner. For example, when a dependency call fails, failover to a predetermined static response. • Throttle requests: This is a mitigation pattern to respond to an unexpected increase in de mand. Some requests are honored but those over a defined limit are rejected and return a message indicating they have been throttled. The expectation on clients is that they will back off and abandon the request or try again at a slower rate. • Control and limit retry calls: Use exponential backoff to retry after progressively longer intervals. Introduce jitter to randomize those retry intervals, and limit the maximum num ber of retries. • Fail fast and limit queues: If the workload is unable to respond successfully to a request, then fail fast. This allows the releasing of resources associated with a request, and permits the service to recover if it’s running out of resources. If the workload is able to respond successfully but the rate of requests is too high, then use a queue to buffer requests in stead. However, do not allow long queues that can result in serving stale requests that the client has already given up on. • Set client timeouts: Set timeouts appropriately, verify them systematically, and do not re ly on default values as they are generally set too high • Make services stateless where possible: Services should either not require state, or should offload state such that between different client requests, there is no dependence on lo cally stored data on disk or in memory. This enables servers to be replaced at will without causing an availability impact. Amazon ElastiCache or Amazon DynamoDB are good desti nations for offloaded state. • Implement emergency levers: These are rapid processes that may mitigate availability impact on your workload. They can be operated in the absence of a root cause. An ideal emergency lever reduces the cognitive burden on the resolvers to zero by providing fully deterministic activation and deactivation criteria. Example levers include blocking all robot traffic or serving a static response. Levers are often manual, but they can also be automat ed. Archived 73 AWS Well-Architected Framework Change Management REL 6  How do you monitor workload resources? Logs and metrics are powerful tools to gain insight into the health of your workload. You can configure your workload to monitor logs and metrics and send notifications when thresholds are crossed or significant events occur. Monitoring enables your workload to recognize when low-performance thresholds are crossed or failures occur, so it can recover automatically in response. Best Practices: • Monitor all components for the workload (Generation): Monitor the components of the workload with Amazon CloudWatch or third-party tools. Monitor AWS services with Per sonal Health Dashboard • Define and calculate metrics (Aggregation): Store log data and apply filters where neces sary to calculate metrics, such as counts of a specific log event, or latency calculated from log event timestamps • Send notifications (Real-time processing and alarming): Organizations that need to know, receive notifications when significant events occur • Automate responses (Real-time processing and alarming): Use automation to take action when an event is detected, for example, to replace failed components • Storage and Analytics: Collect log files and metrics histories and analyze these for broader trends and workload insights • Conduct reviews regularly: Frequently review how workload monitoring is implemented and update it based on significant events and changes • Monitor end-to-end tracing of requests through your system: Use AWS X-Ray or third party tools so that developers can more easily analyze and debug distributed systems to understand how their applications and its underlying services are performing Archived 74 AWS Well-Architected Framework REL 7  How do you design your workload to adapt to changes in demand? A scalable workload provides elasticity to add or remove resources automatically so that they closely match the current demand at any given point in time. Best Practices: • Use automation when obtaining or scaling resources: When replacing impaired resources or scaling your workload, automate the process by using managed AWS services, such as Amazon S3 and AWS Auto Scaling. You can also use third-party tools and AWS SDKs to au tomate scaling. • Obtain resources upon detection of impairment to a workload: Scale resources reactive ly when necessary if availability is impacted, to restore workload availability. • Obtain resources upon detection that more resources are needed for a workload: Scale resources proactively to meet demand and avoid availability impact. • Load test your workload: Adopt a load testing methodology to measure if scaling activity meets workload requirements. REL 8  How do you implement change? Controlled changes are necessary to deploy new functionality, and to ensure that the work loads and the operating environment are running known software and can be patched or re placed in a predictable manner. If these changes are uncontrolled, then it makes it difficult to predict the effect of these changes, or to address issues that arise because of them. Best Practices: • Use runbooks for standard activities such as deployment: Runbooks are the predefined steps used to achieve specific outcomes. Use runbooks to perform standard activities, whether done manually or automatically. Examples include deploying a workload, patch ing it, or making DNS modifications. • Integrate functional testing as part of your deployment: Functional tests are run as part of automated deployment. If success criteria are not met, the pipeline is halted or rolled back. • Integrate resiliency testing as part of your deployment: Resiliency tests (as part of chaos engineering) are run as part of the automated deployment pipeline in a pre-prod environ ment. • Deploy using immutable infrastructure: This is a model that mandates that no updates, security patches, or configuration changes happen in-place on production workloads. When a change is needed, the architecture is built onto new infrastructure and deployed into production. Archived • Deploy changes with automation: Deployments and patching are automated to eliminate negative impact. 75 AWS Well-Architected Framework Failure Management REL 9  How do you back up data? Back up data, applications, and configuration to meet your requirements for recovery time objectives (RTO) and recovery point objectives (RPO). Best Practices: • Identify and back up all data that needs to be backed up, or reproduce the data from sources: Amazon S3 can be used as a backup destination for multiple data sources. AWS services such as Amazon EBS, Amazon RDS, and Amazon DynamoDB have built in capabil ities to create backups. Third-party backup software can also be used. Alternatively, if the data can be reproduced from other sources to meet RPO, you might not require a backup • Secure and encrypt backups: Detect access using authentication and authorization, such as AWS IAM, and detect data integrity compromise by using encryption. • Perform data backup automatically: Configure backups to be taken automatically based on a periodic schedule, or by changes in the dataset. RDS instances, EBS volumes, Dy namoDB tables, and S3 objects can all be configured for automatic backup. AWS Market place solutions or third-party solutions can also be used. • Perform periodic recovery of the data to verify backup integrity and processes: Validate that your backup process implementation meets your recovery time objectives (RTO) and recovery point objectives (RPO) by performing a recovery test. REL 10  How do you use fault isolation to protect your workload? Fault isolated boundaries limit the effect of a failure within a workload to a limited number of components. Components outside of the boundary are unaffected by the failure. Using multiple fault isolated boundaries, you can limit the impact on your workload. Best Practices: • Deploy the workload to multiple locations: Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These loca tions can be as diverse as required. • Automate recovery for components constrained to a single location: If components of the workload can only run in a single Availability Zone or on-premises data center, you must implement the capability to do a complete rebuild of the workload within your de fined recovery objectives. • Use bulkhead architectures: Like the bulkheads on a ship, this pattern ensures that a fail ure is contained to a small subset of requests/users so the number of impaired requests is limited, and most can continue without error. Bulkheads for data are usually called parti tions or shards, while bulkheads for services are known as cells. Archived 76 AWS Well-Architected Framework REL 11  How do you design your workload to withstand component failures? Workloads with a requirement for high availability and low mean time to recovery (MTTR) must be architected for resiliency. Best Practices: • Monitor all components of the workload to detect failures: Continuously monitor the health of your workload so that you and your automated systems are aware of degrada tion or complete failure as soon as they occur. Monitor for key performance indicators (KPIs) based on business value. • Fail over to healthy resources: Ensure that if a resource failure occurs, that healthy re sources can continue to serve requests. For location failures (such as Availability Zone or AWS Region) ensure you have systems in place to failover to healthy resources in unim paired locations. • Automate healing on all layers: Upon detection of a failure, use automated capabilities to perform actions to remediate. • Use static stability to prevent bimodal behavior: Bimodal behavior is when your work load exhibits different behavior under normal and failure modes, for example, relying on launching new instances if an Availability Zone fails. You should instead build workloads that are statically stable and operate in only one mode. In this case, provision enough in stances in each Availability Zone to handle the workload load if one AZ were removed and then use Elastic Load Balancing or Amazon Route 53 health checks to shift load away from the impaired instances. • Send notifications when events impact availability: Notifications are sent upon the de tection of significant events, even if the issue caused by the event was automatically re solved. Archived 77 AWS Well-Architected Framework REL 12  How do you test reliability? After you have designed your workload to be resilient to the stresses of production, testing is the only way to ensure that it will operate as designed, and deliver the resiliency you expect. Best Practices: • Use playbooks to investigate failures: Enable consistent and prompt responses to fail ure scenarios that are not well understood, by documenting the investigation process in playbooks. Playbooks are the predefined steps performed to identify the factors contribut ing to a failure scenario. The results from any process step are used to determine the next steps to take until the issue is identified or escalated. • Perform post-incident analysis: Review customer-impacting events, and identify the con tributing factors and preventative action items. Use this information to develop mitiga tions to limit or prevent recurrence. Develop procedures for prompt and effective respons es. Communicate contributing factors and corrective actions as appropriate, tailored to target audiences. Have a method to communicate these causes to others as needed. • Test functional requirements: These include unit tests and integration tests that validate required functionality. • Test scaling and performance requirements: This includes load testing to validate that the workload meets scaling and performance requirements. • Test resiliency using chaos engineering: Run tests that inject failures regularly into pre production and production environments. Hypothesize how your workload will react to the failure, then compare your hypothesis to the testing results and iterate if they do not match. Ensure that production testing does not impact users. • Conduct game days regularly: Use game days to regularly exercise your failure procedures as close to production as possible (including in production environments) with the peo ple who will be involved in actual failure scenarios. Game days enforce measures to ensure that production testing does not impact users. Archived 78 AWS Well-Architected Framework REL 13  How do you plan for disaster recovery (DR)? Having backups and redundant workload components in place is the start of your DR strate gy. RTO and RPO are your objectives for restoration of availability. Set these based on busi ness needs. Implement a strategy to meet these objectives, considering locations and func tion of workload resources and data. Best Practices: • Define recovery objectives for downtime and data loss: The workload has a recovery time objective (RTO) and recovery point objective (RPO). • Use defined recovery strategies to meet the recovery objectives: A disaster recovery (DR) strategy has been defined to meet objectives. • Test disaster recovery implementation to validate the implementation: Regularly test failover to DR to ensure that RTO and RPO are met. • Manage configuration drift at the DR site or region: Ensure that the infrastructure, data, and configuration are as needed at the DR site or region. For example, check that AMIs and service quotas are up to date. • Automate recovery: Use AWS or third-party tools to automate system recovery and route traffic to the DR site or region. Archived 79 AWS Well-Architected Framework Performance Efficiency Selection PERF 1  How do you select the best performing architecture? Often, multiple approaches are required for optimal performance across a workload. Well-ar chitected systems use multiple solutions and features to improve performance. Best Practices: • Understand the available services and resources: Learn about and understand the wide range of services and resources available in the cloud. Identify the relevant services and configuration options for your workload, and understand how to achieve optimal perfor mance. • Define a process for architectural choices: Use internal experience and knowledge of the cloud, or external resources such as published use cases, relevant documentation, or whitepapers to define a process to choose resources and services. You should define a process that encourages experimentation and benchmarking with the services that could be used in your workload. • Factor cost requirements into decisions : Workloads often have cost requirements for op eration. Use internal cost controls to select resource types and sizes based on predicted re source need. • Use policies or reference architectures: Maximize performance and efficiency by evaluat ing internal policies and existing reference architectures and using your analysis to select services and configurations for your workload. • Use guidance from your cloud provider or an appropriate partner: Use cloud company resources, such as solutions architects, professional services, or an appropriate partner to guide your decisions. These resources can help review and improve your architecture for optimal performance. • Benchmark existing workloads: Benchmark the performance of an existing workload to understand how it performs on the cloud. Use the data collected from benchmarks to dri ve architectural decisions. • Load test your workload: Deploy your latest workload architecture on the cloud using dif ferent resource types and sizes. Monitor the deployment to capture performance metrics that identify bottlenecks or excess capacity. Use this performance information to design or improve your architecture and resource selection. Archived 80 AWS Well-Architected Framework PERF 2  How do you select your compute solution? The optimal compute solution for a workload varies based on application design, usage pat terns, and configuration settings. Architectures can use different compute solutions for vari ous components and enable different features to improve performance. Selecting the wrong compute solution for an architecture can lead to lower performance efficiency. Best Practices: • Evaluate the available compute options: Understand the performance characteristics of the compute-related options available to you. Know how instances, containers, and func tions work, and what advantages, or disadvantages, they bring to your workload. • Understand the available compute configuration options: Understand how various op tions complement your workload, and which configuration options are best for your sys tem. Examples of these options include instance family, sizes, features (GPU, I/O), function sizes, container instances, and single versus multi-tenancy. • Collect compute-related metrics: One of the best ways to understand how your compute systems are performing is to record and track the true utilization of various resources. This data can be used to make more accurate determinations about resource requirements. • Determine the required configuration by right-sizing: Analyze the various performance characteristics of your workload and how these characteristics relate to memory, network, and CPU usage. Use this data to choose resources that best match your workload's profile. For example, a memory-intensive workload, such as a database, could be served best by the r-family of instances. However, a bursting workload can benefit more from an elastic container system. • Use the available elasticity of resources: The cloud provides the flexibility to expand or reduce your resources dynamically through a variety of mechanisms to meet changes in demand. Combined with compute-related metrics, a workload can automatically respond to changes and utilize the optimal set of resources to achieve its goal. • Re-evaluate compute needs based on metrics: Use system-level metrics to identify the behavior and requirements of your workload over time. Evaluate your workload's needs by comparing the available resources with these requirements and make changes to your compute environment to best match your workload's profile. For example, over time a sys tem might be observed to be more memory-intensive than initially thought, so moving to a different instance family or size could improve both performance and efficiency. Archived 81 AWS Well-Architected Framework PERF 3  How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, f ile, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints. Well-architected systems use multiple storage solutions and enable different features to improve performance and use resources efficiently. Best Practices: • Understand storage characteristics and requirements: Understand the different charac teristics (for example, shareable, file size, cache size, access patterns, latency, throughput, and persistence of data) that are required to select the services that best fit your workload, such as object storage, block storage, file storage, or instance storage. • Evaluate available configuration options: Evaluate the various characteristics and config uration options and how they relate to storage. Understand where and how to use provi sioned IOPS, SSDs, magnetic storage, object storage, archival storage, or ephemeral stor age to optimize storage space and performance for your workload. • Make decisions based on access patterns and metrics: Choose storage systems based on your workload's access patterns and configure them by determining how the workload accesses data. Increase storage efficiency by choosing object storage over block storage. Configure the storage options you choose to match your data access patterns. Archived 82 AWS Well-Architected Framework PERF 4  How do you select your database solution? The optimal database solution for a system varies based on requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems use different database solutions for various subsystems and enable different fea tures to improve performance. Selecting the wrong database solution and features for a sys tem can lead to lower performance efficiency. Best Practices: • Understand data characteristics: Understand the different characteristics of data in your workload. Determine if the workload requires transactions, how it interacts with data, and what its performance demands are. Use this data to select the best performing database approach for your workload (for example, relational databases, NoSQL Key-value, docu ment, wide column, graph, time series, or in-memory storage). • Evaluate the available options: Evaluate the services and storage options that are avail able as part of the selection process for your workload's storage mechanisms. Understand how, and when, to use a given service or system for data storage. Learn about available configuration options that can optimize database performance or efficiency, such as provi sioned IOPs, memory and compute resources, and caching. • Collect and record database performance metrics: Use tools, libraries, and systems that record performance measurements related to database performance. For example, mea sure transactions per second, slow queries, or system latency introduced when accessing the database. Use this data to understand the performance of your database systems. • Choose data storage based on access patterns: Use the access patterns of the workload to decide which services and technologies to use. For example, utilize a relational database for workloads that require transactions, or a key-value store that provides higher through put but is eventually consistent where applicable. • Optimize data storage based on access patterns and metrics: Use performance charac teristics and access patterns that optimize how data is stored or queried to achieve the best possible performance. Measure how optimizations such as indexing, key distribution, data warehouse design, or caching strategies impact system performance or overall effi ciency. Archived 83 AWS Well-Architected Framework PERF 5  How do you configure your networking solution? The optimal network solution for a workload varies based on latency, throughput require ments, jitter, and bandwidth. Physical constraints, such as user or on-premises resources, de termine location options. These constraints can be offset with edge locations or resource placement. Best Practices: • Understand how networking impacts performance: Analyze and understand how net work-related decisions impact workload performance. For example, network latency often impacts the user experience, and using the wrong protocols can starve network capacity through excessive overhead. • Evaluate available networking features: Evaluate networking features in the cloud that may increase performance. Measure the impact of these features through testing, metrics, and analysis. For example, take advantage of network-level features that are available to reduce latency, network distance, or jitter. • Choose appropriately sized dedicated connectivity or VPN for hybrid workloads: When there is a requirement for on-premise communication, ensure that you have adequate bandwidth for workload performance. Based on bandwidth requirements, a single dedicat ed connection or a single VPN might not be enough, and you must enable traffic load bal ancing across multiple connections. • Leverage load-balancing and encryption offloading: Distribute traffic across multiple resources or services to allow your workload to take advantage of the elasticity that the cloud provides. You can also use load balancing for offloading encryption termination to improve performance and to manage and route traffic effectively. • Choose network protocols to improve performance: Make decisions about protocols for communication between systems and networks based on the impact to the workload’s performance. • Choose your workload’s location based on network requirements: Use the cloud loca tion options available to reduce network latency or improve throughput. Utilize AWS Re gions, Availability Zones, placement groups, and edge locations such as Outposts, Local Regions, and Wavelength, to reduce network latency or improve throughput. • Optimize network configuration based on metrics: Use collected and analyzed data to make informed decisions about optimizing your network configuration. Measure the im pact of those changes and use the impact measurements to make future decisions. Archived 84 AWS Well-Architected Framework Review PERF 6  How do you evolve your workload to take advantage of new releases? When architecting workloads, there are finite options that you can choose from. However, over time, new technologies and approaches become available that could improve the per formance of your workload. Best Practices: • Stay up-to-date on new resources and services: Evaluate ways to improve performance as new services, design patterns, and product offerings become available. Determine which of these could improve performance or increase the efficiency of the workload through ad hoc evaluation, internal discussion, or external analysis. • Define a process to improve workload performance: Define a process to evaluate new services, design patterns, resource types, and configurations as they become available. For example, run existing performance tests on new instance offerings to determine their po tential to improve your workload. • Evolve workload performance over time: As an organization, use the information gath ered through the evaluation process to actively drive adoption of new services or resources when they become available. Archived 85 AWS Well-Architected Framework Monitoring PERF 7  How do you monitor your resources to ensure they are performing? System performance can degrade over time. Monitor system performance to identify degra dation and remediate internal or external factors, such as the operating system or applica tion load. Best Practices: • Record performance-related metrics: Use a monitoring and observability service to record performance-related metrics. For example, record database transactions, slow queries, I/O latency, HTTP request throughput, service latency, or other key data. • Analyze metrics when events or incidents occur: In response to (or during) an event or incident, use monitoring dashboards or reports to understand and diagnose the impact. These views provide insight into which portions of the workload are not performing as ex pected. • Establish Key Performance Indicators (KPIs) to measure workload performance: Identi fy the KPIs that indicate whether the workload is performing as intended. For example, an API-based workload might use overall response latency as an indication of overall perfor mance, and an e-commerce site might choose to use the number of purchases as its KPI. • Use monitoring to generate alarm-based notifications: Using the performance-related key performance indicators (KPIs) that you defined, use a monitoring system that gener ates alarms automatically when these measurements are outside expected boundaries. • Review metrics at regular intervals: As routine maintenance, or in response to events or incidents, review which metrics are collected. Use these reviews to identify which metrics were key in addressing issues and which additional metrics, if they were being tracked, would help to identify, address, or prevent issues. • Monitor and alarm proactively: Use key performance indicators (KPIs), combined with monitoring and alerting systems, to proactively address performance-related issues. Use alarms to trigger automated actions to remediate issues where possible. Escalate the alarm to those able to respond if automated response is not possible. For example, you may have a system that can predict expected key performance indicators (KPI) values and alarm when they breach certain thresholds, or a tool that can automatically halt or roll back deployments if KPIs are outside of expected values. Archived 86 AWS Well-Architected Framework Tradeoffs PERF 8  How do you use tradeoffs to improve performance? When architecting solutions, determining tradeoffs enables you to select an optimal ap proach. Often you can improve performance by trading consistency, durability, and space for time and latency. Best Practices: • Understand the areas where performance is most critical: Understand and identify ar eas where increasing the performance of your workload will have a positive impact on ef ficiency or customer experience. For example, a website that has a large amount of cus tomer interaction can benefit from using edge services to move content delivery closer to customers. • Learn about design patterns and services: Research and understand the various design patterns and services that help improve workload performance. As part of the analysis, identify what you could trade to achieve higher performance. For example, using a cache service can help to reduce the load placed on database systems; however, it requires some engineering to implement safe caching or possible introduction of eventual consistency in some areas. • Identify how tradeoffs impact customers and efficiency: When evaluating perfor mance-related improvements, determine which choices will impact your customers and workload efficiency. For example, if using a key-value data store increases system perfor mance, it is important to evaluate how the eventually consistent nature of it will impact customers. • Measure the impact of performance improvements: As changes are made to improve performance, evaluate the collected metrics and data. Use this information to determine impact that the performance improvement had on the workload, the workload’s compo nents, and your customers. This measurement helps you understand the improvements that result from the tradeoff, and helps you determine if any negative side-effects were in troduced. • Use various performance-related strategies: Where applicable, utilize multiple strategies to improve performance. For example, using strategies like caching data to prevent exces sive network or database calls, using read-replicas for database engines to improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering and streaming of results as they are available to avoid blocking. Archived 87 AWS Well-Architected Framework Cost Optimization Practice Cloud Financial Management COST 1  How do you implement cloud financial management? Implementing Cloud Financial Management enables organizations to realize business value and financial success as they optimize their cost and usage and scale on AWS. Best Practices: • Establish a cost optimization function: Create a team that is responsible for establishing and maintaining cost awareness across your organization. The team requires people from finance, technology, and business roles across the organization. • Establish a partnership between finance and technology: Involve finance and technolo gy teams in cost and usage discussions at all stages of your cloud journey. Teams regularly meet and discuss topics such as organizational goals and targets, current state of cost and usage, and financial and accounting practices. • Establish cloud budgets and forecasts: Adjust existing organizational budgeting and fore casting processes to be compatible with the highly variable nature of cloud costs and us age. Processes must be dynamic using trend based or business driver-based algorithms, or a combination. • Implement cost awareness in your organizational processes: Implement cost awareness into new or existing processes that impact usage, and leverage existing processes for cost awareness. Implement cost awareness into employee training. • Report and notify on cost optimization: Configure AWS Budgets to provide notifications on cost and usage against targets. Have regular meetings to analyze this workload's cost efficiency and to promote cost aware culture. • Monitor cost proactively: Implement tooling and dashboards to monitor cost proactively for the workload. Do not just look at costs and categories when you receive notifications. This helps to identify positive trends and promote them throughout your organization. • Keep up to date with new service releases: Consult regularly with experts or APN Partners to consider which services and features provide lower cost. Review AWS blogs and other information sources. Archived 88 AWS Well-Architected Framework Expenditure and usage awareness COST 2  How do you govern usage? Establish policies and mechanisms to ensure that appropriate costs are incurred while objec tives are achieved. By employing a checks-and-balances approach, you can innovate without overspending. Best Practices: • Develop policies based on your organization requirements: Develop policies that define how resources are managed by your organization. Policies should cover cost aspects of re sources and workloads, including creation, modification and decommission over the re source lifetime. • Implement goals and targets: Implement both cost and usage goals for your workload. Goals provide direction to your organization on cost and usage, and targets provide mea surable outcomes for your workloads. • Implement an account structure: Implement a structure of accounts that maps to your or ganization. This assists in allocating and managing costs throughout your organization. • Implement groups and roles: Implement groups and roles that align to your policies and control who can create, modify, or decommission instances and resources in each group. For example, implement development, test, and production groups. This applies to AWS services and third-party solutions. • Implement cost controls: Implement controls based on organization policies and defined groups and roles. These ensure that costs are only incurred as defined by organization re quirements: for example, control access to regions or resource types with IAM policies. • Track project lifecycle: Track, measure, and audit the lifecycle of projects, teams, and en vironments to avoid using and paying for unnecessary resources. Archived 89 AWS Well-Architected Framework COST 3  How do you monitor usage and cost? Establish policies and procedures to monitor and appropriately allocate your costs. This al lows you to measure and improve the cost efficiency of this workload. Best Practices: • Configure detailed information sources: Configure the AWS Cost and Usage Report, and Cost Explorer hourly granularity, to provide detailed cost and usage information. Configure your workload to have log entries for every delivered business outcome. • Identify cost attribution categories: Identify organization categories that could be used to allocate cost within your organization. • Establish organization metrics: Establish the organization metrics that are required for this workload. Example metrics of a workload are customer reports produced or web pages served to customers. • Configure billing and cost management tools: Configure AWS Cost Explorer and AWS Budgets inline with your organization policies. • Add organization information to cost and usage: Define a tagging schema based on or ganization, and workload attributes, and cost allocation categories. Implement tagging across all resources. Use Cost Categories to group costs and usage according to organiza tion attributes. • Allocate costs based on workload metrics: Allocate the workload's costs by metrics or business outcomes to measure workload cost efficiency. Implement a process to ana lyze the AWS Cost and Usage Report with Amazon Athena, which can provide insight and charge back capability. COST 4  How do you decommission resources? Implement change control and resource management from project inception to end-of-life. This ensures you shut down or terminate unused resources to reduce waste. Best Practices: • Track resources over their life time: Define and implement a method to track resources and their associations with systems over their life time. You can use tagging to identify the workload or function of the resource. • Implement a decommissioning process: Implement a process to identify and decommis sion orphaned resources. • Decommission resources: Decommission resources triggered by events such as periodic audits, or changes in usage. Decommissioning is typically performed periodically, and is manual or automated. Archived • Decommission resources automatically: Design your workload to gracefully handle re source termination as you identify and decommission non-critical resources, resources that are not required, or resources with low utilization. 90 AWS Well-Architected Framework Cost-effective resources COST 5  How do you evaluate cost when you select services? Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed ser vices, such as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By selecting the appropriate building blocks and managed services, you can optimize this workload for cost. For example, using managed services, you can reduce or re move much of your administrative and operational overhead, freeing you to work on appli cations and business-related activities. Best Practices: • Identify organization requirements for cost: Work with team members to define the bal ance between cost optimization and other pillars, such as performance and reliability, for this workload. • Analyze all components of this workload: Ensure every workload component is analyzed, regardless of current size or current costs. Review effort should reflect potential benefit, such as current and projected costs. • Perform a thorough analysis of each component: Look at overall cost to the organization of each component. Look at total cost of ownership by factoring in cost of operations and management, especially when using managed services. Review effort should reflect poten tial benefit: for example, time spent analyzing is proportional to component cost. • Select software with cost effective licensing: Open source software will eliminate soft ware licensing costs, which can contribute significant costs to workloads. Where licensed software is required, avoid licenses bound to arbitrary attributes such as CPUs, look for li censes that are bound to output or outcomes. The cost of these licenses scales more close ly to the benefit they provide. • Select components of this workload to optimize cost in line with organization prior ities: Factor in cost when selecting all components. This includes using application level and managed services, such as Amazon RDS, Amazon DynamoDB, Amazon SNS, and Ama zon SES to reduce overall organization cost. Use serverless and containers for compute, such as AWS Lambda, Amazon S3 for static websites, and Amazon ECS. Minimize license costs by using open source software, or software that does not have license fees: for exam ple, Amazon Linux for compute workloads or migrate databases to Amazon Aurora. • Perform cost analysis for different usage over time: Workloads can change over time. Some services or features are more cost effective at different usage levels. By performing the analysis on each component over time and at projected usage, you ensure the work load remains cost effective over its lifetime.. Archived 91 AWS Well-Architected Framework COST 6  How do you meet cost targets when you select resource type, size and number? Ensure that you choose the appropriate resource size and number of resources for the task at hand. You minimize waste by selecting the most cost effective type, size, and number. Best Practices: • Perform cost modeling: Identify organization requirements and perform cost modeling of the workload and each of its components. Perform benchmark activities for the workload under different predicted loads and compare the costs. The modeling effort should reflect potential benefit: for example, time spent is proportional to component cost. • Select resource type and size based on data: Select resource size or type based on data about the workload and resource characteristics: for example, compute, memory, through put, or write intensive. This selection is typically made using a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload. • Select resource type and size automatically based on metrics: Use metrics from the cur rently running workload to select the right size and type to optimize for cost. Appropriate ly provision throughput, sizing, and storage for services such as Amazon EC2, Amazon Dy namoDB, Amazon EBS (PIOPS), Amazon RDS, Amazon EMR, and networking. This can be done with a feedback loop such as automatic scaling or by custom code in the workload. COST 7  How do you use pricing models to reduce cost? Use the pricing model that is most appropriate for your resources to minimize expense. Best Practices: • Perform pricing model analysis: Analyze each component of the workload. Determine if the component and resources will be running for extended periods (for commitment dis counts), or dynamic and short running (for spot or on-demand). Perform an analysis on the workload using the Recommendations feature in AWS Cost Explorer. • Implement regions based on cost: Resource pricing can be different in each region. Fac toring in region cost ensures you pay the lowest overall price for this workload • Select third party agreements with cost efficient terms: Cost efficient agreements and terms ensure the cost of these services scales with the benefits they provide. Select agree ments and pricing that scale when they provide additional benefits to your organization. • Implement pricing models for all components of this workload: Permanently running resources should utilize reserved capacity such as Savings Plans or reserved Instances. Short term capacity is configured to use Spot Instances, or Spot Fleet. On demand is only used for short-term workloads that cannot be interrupted and do not run long enough for reserved capacity, between 25% to 75% of the period, depending on the resource type. Archived • Perform pricing model analysis at the master account level: Use Cost Explorer Savings Plans and Reserved Instance recommendations to perform regular analysis at the master account level for commitment discounts. 92 AWS Well-Architected Framework COST 8  How do you plan for data transfer charges? Ensure that you plan and monitor data transfer charges so that you can make architectural decisions to minimize costs. A small yet effective architectural change can drastically reduce your operational costs over time. Best Practices: • Perform data transfer modeling: Gather organization requirements and perform data transfer modeling of the workload and each of its components. This identifies the lowest cost point for its current data transfer requirements. • Select components to optimize data transfer cost: All components are selected, and ar chitecture is designed to reduce data transfer costs. This includes using components such as WAN optimization and Multi-AZ configurations • Implement services to reduce data transfer costs: Implement services to reduce data transfer: for example, using a CDN such as Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using AWS Direct Connect instead of VPN for connectivity to AWS. Manage demand and supply resources COST 9  How do you manage demand, and supply resources? For a workload that has balanced spend and performance, ensure that everything you pay for is used and avoid significantly underutilizing instances. A skewed utilization metric in ei ther direction has an adverse impact on your organization, in either operational costs (de graded performance due to over-utilization), or wasted AWS expenditures (due to over-pro visioning). Best Practices: • Perform an analysis on the workload demand: Analyze the demand of the workload over time. Ensure the analysis covers seasonal trends and accurately represents operating con ditions over the full workload lifetime. Analysis effort should reflect potential benefit: for example, time spent is proportional to the workload cost. • Implement a buffer or throttle to manage demand: Buffering and throttling modify the demand on your workload, smoothing out any peaks. Implement throttling when your clients perform retries. Implement buffering to store the request and defer processing un til a later time. Ensure your throttles and buffers are designed so clients receive a response in the required time. • Supply resources dynamically: Resources are provisioned in a planned manner. This can be demand- based, such as through automatic scaling, or time-based, where demand is predictable and resources are provided based on time. These methods result in the least amount of over or under provisioning. Archived 93 AWS Well-Architected Framework Optimize over time COST 10  How do you evaluate new services? As AWS releases new services and features, it's a best practice to review your existing archi tectural decisions to ensure they continue to be the most cost effective. Best Practices: • Develop a workload review process: Develop a process that defines the criteria and process for workload review. The review effort should reflect potential benefit: for exam ple, core workloads or workloads with a value of over 10% of the bill are reviewed quarter ly, while workloads below 10% are reviewed annually. • Review and analyze this workload regularly: Existing workloads are regularly reviewed as per defined processes. Archived 94
Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Securing Data at Rest with Encryption This paper has been archived Ken Beer and Ryan Holland November 2013 For the latest technical content, refer t o Wh i t epapers & Guides the AWS page: https://aws.amazon.com/whitepapers https://aws.amazon.com/whitepapers/ Page 1 of 15 Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Abstract Organizational policies, or industry or government regulations, might require the use of encryption at rest to protect your data. The flexible nature of Amazon Web Services (AWS) allows you to choose from a variety of different options that meet your needs. This whitepaper provides an overview of different methods for encrypting your data at rest available today. Introduction Amazon Web Services (AWS) delivers a secure, scalable cloud computing platform with high availability, offering the flexibility for you to build a wide range of applications. If you require an additional layer of security for the data you store in the cloud, there are several options for encrypting data at rest—ranging from completely automated AWS encryption solutions to manual, client-side options. Choosing the right solutions depends on which AWS service you’re using and your requirements for key management. This whitepaper provides an overview of various methods for encrypting data at rest in AWS. Links to additional resources are provided for a deeper understanding of how to actually implement the encryption methods discussed. The Key to Encryption: Who Controls the Keys? This paper has been archived For the latest Wh i t Encryption on any system requires three components: (i) data to encrypt; (ii) a method to encrypt the data using a cryptographic algorithm; and (iii) encryption keys to be used in conjunction with the data and the algorithm. Most modern programming languages provide libraries with a wide range of available cryptographic algorithms, like the Advanced Encryption Standard (AES). Choosing the right algorithm involves evaluating security, performance and compliance requirements specific to your application. While the selection of an encryption algorithm is important, protecting the keys from unauthorized access is critical. Managing the security of encryption keys is often performed using a key management infrastructure (KMI). A KMI is itself composed of two sub-components: the storage layer that protects the plaintext keys and the management layer that authorizes key usage. A common way to protect keys in a KMI is to use a hardware security module (HSM). An HSM is a dedicated storage and data processing device that performs cryptographic operations using keys on the device. An HSM typically provides tamper evidence or resistance to protect keys from unauthorized use. A software-based authorization layer controls who can administer the HSM and which users or applications can use which keys in the HSM. technical content, epapers & refer t Guides o the AWS page: https://aws.amazon.com/whitepapers As you deploy encryption for various data classifications in AWS, it is important to understand exactly who has access to your encryption keys or data and under what conditions. There are three different models for how you and/or AWS provide the encryption method and the KMI. A. You control the encryption method and the entire KMI. B. You control the encryption method; AWS provides the storage component of the KMI while you provide the management layer of the KMI. C. AWS controls the encryption method and the entire KMI. Page 2 of 15 This paper has been archived For the latest technical content, refer t o the AWS Wh i t epapers & Guides page: https://aws.amazon.com/whitepapers Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Page 3 of 15 Model A: You control the encryption method and the entire KMI In this model, you use your own KMI to generate, store and manage access to keys as well as control all encryption methods in your applications. This physical location of the KMI and the encryption method can be outside of AWS or in an Amazon Elastic Compute Cloud (Amazon EC2) instance you own. The encryption method can be a combination of open-source tools, AWS SDKs, or third-party software and/or hardware. The important security property of this model is that you have full control over the encryption keys and the execution environment that utilizes those keys in the encryption code. AWS has no access to your keys and cannot perform encryption or decryption on your behalf. You are responsible for the proper storage, management, and use of keys to ensure the confidentiality, integrity, and availability of your data. Data can be encrypted in AWS services as described in the following sections. Amazon S3 You can encrypt data using any encryption method you want, and then upload the encrypted data using the Amazon Simple Storage Service (Amazon S3) APIs. Most common application languages include cryptographic libraries that allow you to perform encryption in your applications. Two commonly available open source tools are Bouncy Castle and OpenSSL. Once you have encrypted an object and safely stored the key in your KMI, the encrypted object can be uploaded to Amazon S3 directly with a PUT request. To decrypt this data, you issue the GET request in the Amazon S3 API and then pass the encrypted data to your local application for decryption. AWS provides an alternative to these open source encryption tools with the Amazon S3 encryption client; itself an open source set of APIs embedded into the AWS SDKs. This client lets you supply a key from your KMI that can be used to encrypt or decrypt your data as part of the call to Amazon S3. The SDK leverages Java Cryptography Extensions (JCE) in your application to take your symmetric or asymmetric key as input and encrypt the object prior to uploading to Amazon S3. The process is reversed when the SDK is used to retrieve an object; the downloaded encrypted object from Amazon S3 is passed to the client along with the key from your KMI. The underlying JCE in your application decrypts the object. The Amazon S3 encryption client is integrated into the AWS SDKs for Java, Ruby, and .NET and provides a transparent drop-in replacement for any cryptographic code you may have used previously with your application that interacts with Amazon S3. While Amazon provides the encryption method, you control the security of your data because you control the keys for that engine to use. If you’re using the Amazon S3 encryption client on-premises, AWS never has access to your keys or unencrypted data. If you’re using the client in an application running in Amazon EC2, a best practice is to pass keys to the client using secure transport (e.g. SSL or SSH) from your KMI to help ensure confidentiality. For more information, see the AWS SDK for Java documentation and Using Client-Side Encryption in the Amazon S3 Developer Guide. Figure 1 shows how these two methods of client-side encryption work for Amazon S3 data. Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Figure 1: Amazon S3 client-side encryption from on-premises system or from within your Amazon EC2 application There are third-party solutions available that can simplify the key management process when encrypting data to Amazon S3. CloudBerry Explorer PRO for Amazon S3 and CloudBerry Backup both offer a client-side encryption option This paper has been archived that applies a user-defined password to the encryption scheme to protect files stored on Amazon S3. For programmatic encryption needs, SafeNet ProtectApp for Java integrates with the SafeNet KeySecure KMI to provide client-side encryption in your application. The KeySecure KMI provides secure key storage and policy enforcement for keys that are passed to the ProtectApp Java client compatible with the AWS SDK. The KeySecure KMI can run as an on-premises appliance or as a virtual appliance in Amazon EC2. Figure 2 shows how the SafeNet solution can be used to encrypt data stored on Amazon S3. For the latest Wh i t technical content, epapers & refer t Guides o the AWS page: https://aws.amazon.com/whitepapers Figure 2: Amazon S3 client-side encryption from on-premises system or from within your application in Amazon EC2 using SafeNet ProtectApp and SafeNet KeySecure KMI Page 4 of 15 Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Amazon EBS Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2 instances. Amazon EBS volumes are network-attached, and persist independently from the life of an instance. Because Amazon EBS volumes are presented to an instance as a block device, you can leverage most standard encryption tools for file system-level or block-level encryption. Some common block-level open source encryption solutions for Linux are Loop AES, dm-crypt (with or without) LUKS, and TrueCrypt. Each of these operates below the file system layer using kernel space device drivers to perform encryption and decryption of data. These tools are useful when you want all data written to a volume to be encrypted regardless of what directory the data is stored in. Another option would be to use file system-level encryption, which works by stacking an encrypted file system on top of an existing file system. This method is typically used to encrypt a specific directory. eCryptfs and EncFs are two Linux-based open source examples of file system-level encryption tools. These solutions all require you to provide keys, either manually or from your KMI. An important caveat with both block-level and file system-level encryption tools is that they can only be used to encrypt data volumes that are not Amazon EBS boot volumes. This is because these tools don’t allow you to automatically make a trusted key available to the boot volume at startup. Encrypting Amazon EBS volumes attached to Windows instances can be done using BitLocker or Encrypted File System (EFS) as well as open source applications like TrueCrypt. In either case, you still need to provide keys to these encryption methods and you can only encrypt data volumes. There are AWS partner solutions that can help automate the process of encrypting Amazon EBS volumes as well as supplying and protecting the necessary keys. Both Trend Micro SecureCloud and SafeNet ProtectV are two such partner This paper has been archived products that encrypt Amazon EBS volumes and include a KMI. Both products are able to encrypt boot volumes in addition to data volumes. These solutions also support use-cases where Amazon EBS volumes attach to auto-scaled Amazon EC2 instances. Figure 3 shows how the SafeNet and Trend Micro solutions can be used to encrypt data stored on Amazon EBS using keys managed on-premises, via SaaS, or in software running on EC2. For the latest Wh i t technical content, epapers & refer t Guides o the AWS page: https://aws.amazon.com/whitepapers Figure 3: Encryption in Amazon EBS using SafeNet ProtectV or Trend Micro SecureCloud Page 5 of 15 This paper has been archived For the latest technical content, refer t o the AWS Wh i t epapers & Guides page: https://aws.amazon.com/whitepapers Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Page 6 of 15 AWS Storage Gateway AWS Storage Gateway is a service connecting an on-premises software appliance with Amazon S3. It can be exposed to your network as an iSCSI disk to facilitate copying data from other sources. Data on disk volumes attached to the AWS Storage Gateway will be automatically uploaded to Amazon S3 based on policy. You can encrypt source data on the disk volumes using any of the file encryption methods described previously (e.g. Bouncy Castle or OpenSSL) before it reaches the disk. You can also use a block-level encryption tool (e.g. BitLocker or dm-crypt/LUKS) on the iSCSI endpoint that AWS Storage Gateway exposes to encrypt all data on the disk volume. Alternatively, two AWS partner solutions, Trend Micro SecureCloud and SafeNet StorageSecure, can perform both the encryption and key management for the iSCSI disk volume exposed by AWS Storage Gateway. These partners provide an easy, check box solution to both encrypt data and manage the necessary keys that is similar in design to how their Amazon EBS encryption solutions work. Amazon RDS Encryption of data in Amazon Relational Database Service (Amazon RDS) using client-side technology requires you to consider how you want data queries to work. Because Amazon RDS doesn’t expose the attached disk it uses for data storage, transparent disk encryption using techniques described in the previous Amazon EBS section are not available to you. However, selective encryption of database fields in your application can be done using any of the standard encryption libraries mentioned previously (e.g. Bouncy Castle, OpenSSL) in your application before the data is passed to your Amazon RDS instance. While this specific field data would not easily support range queries in the database, queries based on unencrypted fields can still return useful results. The encrypted fields of the returned results can be decrypted by your local application for presentation. To support more efficient querying of encrypted data, you can store a keyed hash message authentication code (HMAC) of an encrypted field in your schema as well as supplying a key for the hash function. Subsequent queries of protected fields that contain the HMAC of the data being sought would not disclose the plaintext values in the query. This allows the database to perform a query against the encrypted data in your database without disclosing the plaintext values in the query. Any of the encryption methods you choose must be performed on your own application instance before data is sent to the Amazon RDS instance. CipherCloud and Voltage Secure are two AWS partners with solutions that simplify protecting the confidentiality of data in Amazon RDS. Both vendors have the ability to encrypt data using format-preserving encryption (FPE) that allows ciphertext to be inserted into the database without breaking the schema. They also support tokenization options with integrated lookup tables. In either case your data is encrypted or tokenized in your application before being written to the Amazon RDS instance. These partners provide options to index and search against databases with encrypted or tokenized fields. The unencrypted or untokenized data can be read from the database by other applications without needing to distribute keys or mapping tables to those applications to unlock the encrypted or tokenized fields. For example, you could move data from Amazon RDS to the Amazon Redshift data warehousing solution and run queries against the non-sensitive fields while keeping sensitive fields encrypted or tokenized. Figure 4 shows how the Voltage solution can be used within Amazon EC2 to encrypt data before being written to the Amazon RDS instance. The encryption keys are pulled from the Voltage KMI located in your data center by the Voltage client running on your applications on Amazon EC2. Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Figure 4: Encrypting data in your Amazon EC2 applications before writing to Amazon RDS using Voltage SecureData This paper For the latest Wh i has been t archived technical content, epapers & CipherCloud for Amazon Web Services is a solution that works similar to the way the Voltage client does for applications running in Amazon EC2 that need to send encrypted data to and from Amazon RDS. CipherCloud provides a JDBC driver that can be installed on the application, regardless of whether it’s running in EC2 or in your datacenter. In addition, the CipherCloud for Any App solution can be deployed as an inline gateway to intercept data as it is being sent to and from your Amazon RDS instance. Figure 5 shows how the CipherCloud solution can be deployed this way to encrypt or tokenize data leaving your data center before being written to the Amazon RDS instance. refer t Guides o the AWS page: https://aws.amazon.com/whitepapers Figure 5: Encrypting data in your data center before writing to Amazon RDS using CipherCloud Encryption Gateway Page 7 of 15 Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Amazon EMR Amazon Elastic MapReduce (Amazon EMR) provides an easy-to-use Hadoop implementation running on Amazon EC2. Performing encryption throughout the MapReduce operation involves encryption and key management at four distinct points: 1. The source data 2. Hadoop Distributed File System (HDFS) 3. Shuffle phase 4. Output data If the source data is not encrypted, then this step can be skipped and SSL can be used to help protect data in transit to the Amazon EMR cluster. If the source data is encrypted, then your MapReduce job will need to be able to decrypt the data as it is ingested. If your job flow uses Java and the source data is in Amazon S3, you can use any of the client decryption methods described in the previous Amazon S3 sections. The storage used for the HDFS mount point is the ephemeral storage of the cluster nodes. Depending upon the instance type there may be more than one mount. Encrypting these mount points requires the use of an Amazon EMR bootstrap script that will:  stop the Hadoop service; This paper has been archived  install a file system encryption tool on the instance;  create an encrypted directory to mount the encrypted file system on top of the existing mount points;  restart the Hadoop service. For the latest Wh i t technical content, epapers & refer t Guides o the AWS page: https://aws.amazon.com/whitepapers You could, for example, perform these steps using the open source eCryptfs package and an ephemeral key generated in your code on each of the HDFS mounts. You don’t need to worry about persistent storage of this encryption key, as the data it encrypts does not persist beyond the life of the HDFS instance. The shuffle phase involves passing data between cluster nodes before the reduce step. To encrypt this data in transit, you can enable SSL with a configure Hadoop bootstrap option when you create your cluster. Finally, to enable encryption of the output data, your MapReduce job should encrypt the output using a key sourced from your KMI. This data can be sent to Amazon S3 for storage in encrypted form. The AWS partner Gazzang simplifies managing the encryption and KMI to provide end-to-end encryption of data through an Amazon EMR job. Their Gazzang CloudEncrypt™ solution allows you to upload data to Amazon S3 using their integrated client-side encryption and then add a bootstrap script to your Amazon EMR cluster. CloudEncrypt will encrypt data used by all the HDFS mount points as well as handle key management both for input data retrieved from and output data written to Amazon S3. Figure 6 shows how the Gazzang solution can be used to perform end-to-end encryption of data used within Amazon EMR. Page 8 of 15 Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Figure 6: End-to-end encryption of data in Amazon EMR using Gazzang CloudEncrypt™ This paper has been archived Model B: You control the encryption method; AWS provides the storage component of the KMI while you provide the management layer of the KMI For the latest Wh i t technical content, epapers & refer t Guides o the AWS page: https://aws.amazon.com/whitepapers This model is similar to Model A in that you manage the encryption method, but differs in that the keys are stored in an AWS CloudHSM appliance rather than in a key storage system you manage on-premises. While the keys are stored in the AWS environment, they are inaccessible to any employee at AWS. This is because only you have access to the cryptographic partitions within the dedicated HSM to use the keys. The AWS CloudHSM appliance has both physical and logical tamper detection and response mechanisms that trigger zeroization of the appliance. Zeroization erases the HSM’s volatile memory where any keys in the process of being decrypted were stored and destroys the key that encrypts stored objects, effectively causing all keys on the HSM to be inaccessible and unrecoverable. When determining if using AWS CloudHSM is appropriate for your deployment, it is important to understand the role that an HSM plays in encrypting data. An HSM can be used to generate and store key material and can perform encryption and decryption operations, but it does not perform any key lifecycle management functions (e.g. access control policy, key rotation). This means that a compatible KMI may be needed in addition to the AWS CloudHSM appliance before deploying your application. The KMI you provide can be deployed either on-premises or within Amazon EC2 and can communicate to the AWS CloudHSM instance securely over SSL to help protect data and encryption keys. Because the AWS CloudHSM service uses SafeNet Luna appliances, any key management server that supports the SafeNet Luna platform can also be used with AWS CloudHSM. Any of the encryption options described for AWS services in Model A can work with AWS CloudHSM as long as the solution supports the SafeNet Luna platform. This allows you to run your KMI within the AWS compute environment while maintaining a root of trust in a hardware appliance to which only you have access. Page 9 of 15 Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Applications must be able to access your AWS CloudHSM appliance in an Amazon Virtual Private Cloud (Amazon VPC). The AWS CloudHSM client, provided by SafeNet, interacts with the AWS CloudHSM appliance to encrypt data from your application. Encrypted data can then be sent to any AWS service for storage. Database, disk volume, and file encryption applications can all be supported with AWS CloudHSM and your custom application. Figure 7 shows how the AWS CloudHSM solution works with your applications in an Amazon VPC, Amazon EC2 instance. This paper has been archived For the latest Wh i t technical content, epapers & refer t o the AWS Guides page: https://aws.amazon.com/whitepapers Figure 7: AWS CloudHSM deployed in Amazon VPC In order to achieve the highest availability and durability of keys in your AWS CloudHSM appliance, we recommend deploying multiple AWS CloudHSM applications across Availability Zones or in conjunction with an on-premises SafeNet Luna appliance that you manage. The SafeNet Luna solution support secure replication of keying material across appliances. For more information, see AWS CloudHSM on the AWS website. Page 10 of 15 Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Model C: AWS controls the encryption method and the entire KMI In this model, AWS provides server-side encryption of your data, transparently managing the encryption method and the keys. Envelope encryption is used in AWS for server-side encryption. Figure 8 describes envelope encryption. 1. A data key is generated by the AWS service at the time you request your data to be encrypted. 2. Data key is used to encrypt your data. 3. The data key is then encrypted with a key-encrypting key unique to the service storing your data. This paper For the latest Wh has been i t archived 4. The encrypted data key and the encrypted data are then stored by the AWS storage service on your behalf. technical content, epapers & refer t o the AWS Guides page: https://aws.amazon.com/whitepapers Figure 8: Envelope encryption The key-encrypting keys used to encrypt data keys are securely stored and managed separately from the data and the data keys. Strict access controls are placed on the encryption keys designed to prevent unauthorized use by AWS employees. When you need access to your plaintext data, this process is reversed. The encrypted data key is decrypted using the key-encrypting key; the data key is then used to decrypt your data. The following AWS services offer server-side encryption: Amazon S3 You can set an API flag or check a box in the AWS Management Console to have data encrypted before it is written to disk in Amazon S3. Each object is encrypted with a unique key. As an additional safeguard, this key itself is encrypted with a periodically rotated master key unique to Amazon S3 that is securely stored in separate systems under AWS control. Amazon S3 server-side encryption uses 256-bit Advanced Encryption Standard (AES) keys for both object and master keys. This feature is offered at no additional cost beyond what you pay for using Amazon S3. Page 11 of 15 This paper has been archived For the latest technical content, refer t o the AWS Wh i t epapers & Guides page: https://aws.amazon.com/whitepapers Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Page 12 of 15 Amazon Glacier Data is always automatically encrypted before it’s written to disk using 256-bit AES keys unique to the Amazon Glacier service that are securely stored in separate systems under AWS control. This feature is offered at no additional cost beyond what you pay for using Amazon Glacier. AWS Storage Gateway The AWS Storage Gateway securely transfers your data to AWS over SSL and stores data encrypted at rest in Amazon S3 or Amazon Glacier using their respective server side encryption schemes. Amazon EMR S3DistCp is an Amazon EMR feature that moves large amounts of data from Amazon S3 into HDFS, from HDFS to Amazon S3, and between Amazon S3 buckets. S3DistCp supports the ability to request Amazon S3 to use server-side encryption when it writes EMR data to an Amazon S3 bucket you manage. This feature is offered at no additional cost beyond what you pay for using Amazon S3 to store your Amazon EMR data. Oracle on Amazon RDS You can choose to license the Oracle Advanced Security option for Oracle on Amazon RDS to leverage the native Transparent Data Encryption (TDE) and Native Network Encryption (NNE) features. The Oracle encryption module creates data and key-encrypting keys to encrypt the database. The key-encrypting keys specific to your Oracle instance on Amazon RDS are themselves encrypted by a periodically-rotated 256-bit AES master key. This master key is unique to the Amazon RDS service and is securely stored in separate systems under AWS control. Microsoft SQL Server on Amazon RDS You can choose to provision Transparent Data Encryption (TDE) for Microsoft SQL Server on Amazon RDS. The SQL Server encryption module creates data and key-encrypting keys to encrypt the database. The key-encrypting keys specific to your SQL Server instance on Amazon RDS are themselves encrypted by a periodically-rotated, regional 256-bit AES master key. This master key is unique to the Amazon RDS service and is securely stored in separate systems under AWS control. . This feature is offered at no additional cost beyond what you pay for using Microsoft SQL Server on Amazon RDS. Amazon Redshift When creating an Amazon Redshift cluster you can optionally choose to encrypt all data in user-created tables. There are two options to choose from for server-side encryption of an Amazon Redshift cluster. 1. In the first option, data blocks (included backups) are encrypted using random 256-bit AES keys. These keys are themselves encrypted using a random 256-bit AES database key. This database key is itself encrypted by a 256 bit AES cluster master key that is unique to your cluster. The cluster master key is encrypted with a periodically rotated regional master key unique to the Amazon Redshift service that is securely stored in separate systems under AWS control. This feature is offered at no additional cost beyond what you pay for using Amazon Redshift. 2. With the second option, the 256-bit AES cluster master key used to encrypt your database keys is generated in your AWS CloudHSM or using a SafeNet Luna HSM appliance on-premises. This cluster master key is then encrypted by a master key that never leaves your HSM. When the Amazon Redshift cluster starts up, the cluster master key is decrypted in your HSM and used to decrypt the database key, which is sent securely to the Amazon Redshift hosts to reside only in memory for the life of the cluster. If the cluster ever restarts, the cluster master key is again retrieved securely from your HSM—it is never stored on disk in plaintext. This option lets you more tightly control the hierarchy and lifecycle of the keys used to encrypt your data. This feature is offered at Amazon Web Services – Encrypting Data at Rest in AWS November 2013 no additional cost beyond what you pay for using Amazon Redshift (and AWS CloudHSM if you choose that option for storing keys). In addition to encrypting data generated within your Amazon Redshift cluster, you can also load encrypted data into Amazon Redshift from Amazon S3 that was previously encrypted using the Amazon S3 encryption client and keys you provide. Amazon Redshift supports the decryption and re-encryption of data going between Amazon S3 and Amazon Redshift to protect the full lifecycle of your data. These server-side encryption features across multiple AWS services enable you to easily encrypt your data simply by making a configuration setting in the AWS Management Console, CLI, or API request for the given AWS service. The authorized use of encryption keys is automatically and securely managed by AWS. Because unauthorized access to those keys may lead to the disclosure of your data, we have built systems and processes that minimize the chance of unauthorized access. Conclusion We have presented three different models for how encryption keys are managed and where they are used. If you take all responsibility for the encryption method and the KMI, you can have granular control over how your applications encrypt data. However, that granular control comes at a cost—both in terms of deployment effort and an inability to have AWS services tightly integrate with your applications’ encryption methods. At the other end of the spectrum AWS offers fully-integrated check box encryption for several services that store your data. This paper has been archived Table 1 below summarizes the available options for encrypting data at rest across AWS. We recommend you determine which encryption and key management model is most appropriate for your data classifications in the context of the AWS service you are using. For the latest Wh i t technical content, epapers & refer t Guides o the AWS page: https://aws.amazon.com/whitepapers Page 13 of 15 This paper has been archived For the latest technical content, refer t o the AWS Wh i t epapers & Guides page: https://aws.amazon.com/whitepapers Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Page 14 of 15 Encryption Method and KMI Model A Model B Model C AWS Service Client-Side Solutions Using Customer Managed Keys Client-Side Partner Solutions with KMI for Customer-Managed Keys Client-Side Solutions for Customer-Managed Keys in AWS CloudHSM Server-Side Encryption Using AWS-Managed Keys Amazon S3 Bouncy Castle, OpenSSL, Amazon S3 encryption client in the AWS SDK for Java SafeNet ProtectApp for Java Custom Amazon VPC-EC2 application integrated with AWS CloudHSM client Amazon S3 server-side encryption Amazon Glacier N/A N/A Custom Amazon VPC-EC2 application integrated with AWS CloudHSM client All data is automatically encrypted using server-side encryption AWS Storage Gateway Linux Block Level: - Loop-AES, dm-crypt (with or without LUKS), and TrueCrypt Linux File System: - eCryptfs and EncFs Windows Block Level: -TrueCrypt Windows File System: - BitLocker Trend Micro SecureCloud, SafeNet StorageSecure N/A Amazon S3 server-side encryption Amazon EBS Linux Block Level: - Loop-AES, dm crypt+LUKS and TrueCrypt Linux File System: - eCryptfs and EncFs Windows Block Level: -TrueCrypt Windows File System: - BitLocker, EFS Trend Micro SecureCloud, SafeNet ProtectV Custom Amazon VPC-EC2 application integrated with AWS CloudHSM client N/A Oracle on Amazon RDS Bouncy Castle, OpenSSL CipherCloud Database Gateway and Voltage SecureData Custom Amazon VPC-EC2 application integrated with AWS CloudHSM client Transparent Data Encryption (TDE) and Native Network Encryption (NNE) with optional Oracle Advanced Security license TDE for Microsoft SQL Server Microsoft SQL Server on Amazon RDS Bouncy Castle, OpenSSL CipherCloud Database Gateway and Voltage SecureData Custom Amazon VPC-EC2 application integrated with AWS CloudHSM client N/A Amazon Redshift N/A N/A Encrypted Amazon Redshift clusters with your master key managed in AWS CloudHSM or on-premises Safenet Luna HSM Encrypted Amazon Redshift clusters with AWS managed master key Amazon EMR eCryptfs Gazzang CloudEncrypt Custom Amazon VPC-EC2 application integrated with AWS CloudHSM client S3DistCp using Amazon S3 server-side encryption to protect persistently stored data Table 1: Summary of Data at Rest Encryption Options This paper has been archived For the latest technical content, refer t o the AWS Wh i t epapers & Guides page: https://aws.amazon.com/whitepapers Amazon Web Services – Encrypting Data at Rest in AWS November 2013 Page 15 of 15 References and Further Reading AWS Resources  Client-Side Data Encryption with the AWS SDK for Java and Amazon S3 http://aws.amazon.com/articles/2850096021478074  AWS CloudHSM https://aws.amazon.com/cloudhsm/  Amazon EMR S3DistCp to encrypt data in Amazon S3 http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html  Transparent Data Encryption for Oracle on Amazon RDS http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.html#Appendix.Oracle.O ptions.AdvSecurity  Transparent Data Encryption for Microsoft SQL Server on Amazon RDS http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.Gener al.Options  Amazon Redshift encryption http://aws.amazon.com/redshift/faqs/#0210  AWS Security Blog http://blogs.aws.amazon.com/security Partner Resources  Bouncy Castle Java crypto library http://www.bouncycastle.org/  OpenSSL crypto library http://www.openssl.org/  CloudBerry Explorer PRO for Amazon S3 encryption http://www.cloudberrylab.com/amazon-s3-explorer-pro-cloudfront-IAM.aspx  SafeNet encryption products for Amazon S3, Amazon EBS, and AWS CloudHSM http://www.safenet-inc.com/  Trend Micro SecureCloud http://www.trendmicro.com/us/enterprise/cloud-solutions/secure-cloud/index.html  CipherCloud for AWS and CipherCloud for Any App http://www.ciphercloud.com/  Voltage Security SecureData Enterprise http://www.voltage.com/products/securedata-enterprise/  Gazzang CloudEncrypt http://www.gazzang.com/products/cloudencrypt 
Web Application Hosting in the AWS Cloud First Published May 2010 Updated August 20, 2021 Notices Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. © 2021 Amazon Web Services, Inc. or its affiliates. All rights reserved. Contents An overview of traditional web hosting ............................................................................... 1 Web application hosting in the cloud using AWS ............................................................... 2 How AWS can solve common web application hosting issues ....................................... 2 An AWS Cloud architecture for web hosting ................................................................... 4 Key components of an AWS web hosting architecture ................................................... 6 Key considerations when using AWS for web hosting ..................................................... 16 Conclusion ......................................................................................................................... 18 Contributors ....................................................................................................................... 19 Further reading .................................................................................................................. 19 Document versions ............................................................................................................ 19 Abstract Traditional on-premises web architectures require complex solutions and accurate reserved capacity forecast in order to ensure reliability. Dense peak traffic periods and wild swings in traffic patterns result in low utilization rates of expensive hardware. This yields high operating costs to maintain idle hardware, and an inefficient use of capital for underused hardware. Amazon Web Services (AWS) provides a reliable, scalable, secure, and highly performing infrastructure for the most demanding web applications. This infrastructure matches IT costs with customer traffic patterns in near-real time. This whitepaper is meant for IT Managers and System Architects who want to understand how to run traditional web architectures in the cloud to achieve elasticity, scalability, and reliability. Amazon Web Services Web Application Hosting in the AWS Cloud An overview of traditional web hosting Scalable web hosting is a well-known problem space. The following image depicts a traditional web hosting architecture that implements a common three-tier web application model. In this model, the architecture is separated into presentation, application, and persistence layers. Scalability is provided by adding hosts at these layers. The architecture also has built-in performance, failover, and availability features. The traditional web hosting architecture is easily ported to the AWS Cloud with only a few modifications. A traditional web hosting architecture Page 1 Amazon Web Services Web Application Hosting in the AWS Cloud The following sections look at why and how such an architecture should be and could be deployed in the AWS Cloud. Web application hosting in the cloud using AWS The first question you should ask concerns the value of moving a classic web application hosting solution into the AWS Cloud. If you decide that the cloud is right for you, you’ll need a suitable architecture. This section helps you evaluate an AWS Cloud solution. It compares deploying your web application in the cloud to an on-premises deployment, presents an AWS Cloud architecture for hosting your application, and discusses the key components of the AWS Cloud Architecture solution. How AWS can solve common web application hosting issues If you’re responsible for running a web application, you could face a variety of infrastructure and architectural issues for which AWS can provide seamless and cost effective solutions. The following are some of the benefits of using AWS over a traditional hosting model. A cost-effective alternative to oversized fleets needed to handle peaks In the traditional hosting model, you have to provision servers to handle peak capacity. Unused cycles are wasted outside of peak periods. Web applications hosted by AWS can leverage on-demand provisioning of additional servers, so you can constantly adjust capacity and costs to actual traffic patterns. For example, the following graph shows a web application with a usage peak from 9AM to 3PM, and less usage for the remainder of the day. An automatic scaling approach based on actual traffic trends, which provisions resources only when needed, would result in less wasted capacity and a greater than 50 percent reduction in cost. Page 2 Amazon Web Services Web Application Hosting in the AWS Cloud An example of wasted capacity in a classic hosting model A scalable solution to handling unexpected traffic peaks A more dire consequence of the slow provisioning associated with a traditional hosting model is the inability to respond in time to unexpected traffic spikes. There are a number of stories about web applications becoming unavailable because of an unexpected spike in traffic after the site is mentioned in popular media. In the AWS Cloud, the same on-demand capability that helps web applications scale to match regular traffic spikes can also handle an unexpected load. New hosts can be launched and are readily available in a matter of minutes, and they can be taken offline just as quickly when traffic returns to normal. An on-demand solution for test, load, beta, and preproduction environments The hardware costs of building and maintaining a traditional hosting environment for a production web application don’t stop with the production fleet. Often, you need to create preproduction, beta, and testing fleets to ensure the quality of the web application at each stage of the development lifecycle. While you can make various optimizations to ensure the highest possible use of this testing hardware, these parallel fleets are not always used optimally, and a lot of expensive hardware sits unused for long periods of time. Page 3 Amazon Web Services Web Application Hosting in the AWS Cloud In the AWS Cloud, you can provision testing fleets as and when you need them. This not only eliminates the need for pre-provisioning resources days or months prior to the actual usage, but gives you the flexibility to tear down the infrastructure components when you do not need them. Additionally, you can simulate user traffic on the AWS Cloud during load testing. You can also use these parallel fleets as a staging environment for a new production release. This enables quick switchover from current production to a new application version with little or no service outages. An AWS Cloud architecture for web hosting The following figure provides another look at that classic web application architecture and how it can leverage the AWS Cloud computing infrastructure. Page 4 Amazon Web Services Web Application Hosting in the AWS Cloud An example of a web hosting architecture on AWS Page 5 Amazon Web Services Web Application Hosting in the AWS Cloud 1. DNS services with Amazon Route 53 – Provides DNS services to simplify domain management. 2. Edge caching with Amazon CloudFront – Edge caches high-volume content to decrease the latency to customers. 3. Edge security for Amazon CloudFront with AWS WAF – Filters malicious traffic, including cross site scripting (XSS) and SQL injection via customer defined rules. 4. Load balancing with Elastic Load Balancing (ELB) – Enables you to spread load across multiple Availability Zones and AWS Auto Scaling groups for redundancy and decoupling of services. 5. DDoS protection with AWS Shield – Safeguards your infrastructure against the most common network and transport layer DDoS attacks automatically. 6. Firewalls with security groups – Moves security to the instance to provide a stateful, host-level firewall for both web and application servers. 7. Caching with Amazon ElastiCache – Provides caching services with Redis or Memcached to remove load from the app and database, and lower latency for frequent requests. 8. Managed database with Amazon Relational Database Service (Amazon RDS) – Creates a highly available, multi-AZ database architecture with six possible DB engines. 9. Static storage and backups with Amazon Simple Storage Service (Amazon S3) – Enables simple HTTP-based object storage for backups and static assets like images and video. Key components of an AWS web hosting architecture The following sections outline some of the key components of a web hosting architecture deployed in the AWS Cloud, and explain how they differ from a traditional web hosting architecture. Page 6 Amazon Web Services Web Application Hosting in the AWS Cloud Network management In the AWS Cloud, the ability to segment your network from that of other customers enables a more secure and scalable architecture. While security groups provide host level security (see the Host security section), Amazon Virtual Private Cloud (Amazon VPC) enables you to launch resources in a logically isolated and virtual network that you define. Amazon VPC is a service that gives you full control over the details of your networking setup in AWS. Examples of this control include creating internet subnets for web servers, and private subnets with no internet access for your databases. Amazon VPC enables you to create hybrid architectures by using hardware virtual private networks (VPNs), and use the AWS Cloud as an extension of your own data center. Amazon VPC also includes IPv6 support in addition to traditional IPv4 support for your network. Content delivery When your web traffic is geo-dispersed, it’s not always feasible and certainly not cost effective to replicate your entire infrastructure across the globe. A Content Delivery Network (CDN) provides you the ability to utilize its global network of edge locations to deliver a cached copy of web content such as videos, webpages, images and so on to your customers. To reduce response time, the CDN utilizes the nearest edge location to the customer or originating request location to reduce the response time. Throughput is dramatically increased given that the web assets are delivered from cache. For dynamic data, many CDNs can be configured to retrieve data from the origin servers. You can use CloudFront to deliver your website, including dynamic, static, and streaming content, using a global network of edge locations. CloudFront automatically routes requests for your content to the nearest edge location, so content is delivered with the best possible performance. CloudFront is optimized to work with other AWS services, like Amazon S3 and Amazon Elastic Compute Cloud (Amazon EC2). CloudFront also works seamlessly with any origin server that is not an AWS origin server, which stores the original, definitive versions of your files. Like other AWS services, there are no contracts or monthly commitments for using CloudFront – you pay only for as much or as little content as you actually deliver through the service. Page 7 Amazon Web Services Web Application Hosting in the AWS Cloud Additionally, any existing solutions for edge caching in your web application infrastructure should work well in the AWS Cloud. Managing public DNS Moving a web application to the AWS Cloud requires some Domain Name System (DNS) changes. To help you manage DNS routing, AWS provides Amazon Route 53, a highly available and scalable cloud DNS web service. Route 53 is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names such as “www.example.com” into numeric IP addresses such as 192.0.2.1, that computers use to connect to each other. Route 53 is fully compliant with IPv6 as well. Host security In addition to inbound network traffic filtering at the edge, AWS also recommends web applications apply network traffic filtering at the host level. Amazon EC2 provides a feature named security groups. A security group is analogous to an inbound network firewall, for which you can specify the protocols, ports, and source IP ranges that are allowed to reach your EC2 instances. You can assign one or more security groups to each EC2 instance. Each security group allows appropriate traffic in to each instance. Security groups can be configured so that only specific subnets, IP addresses, and resources have access to an EC2 instance. Alternatively, they can reference other security groups to limit access to EC2 instances that are in specific groups. In the AWS web hosting architecture in Figure 3, the security group for the web server cluster might allow access only from the web-layer Load Balancer and only over TCP on ports 80 and 443 (HTTP and HTTPS). The application server security group, on the other hand, might allow access only from the application-layer Load Balancer. In this model, your support engineers would also need to access the EC2 instances, what can be achieved with AWS Systems Manager Session Manager. For a deeper discussion on security, the AWS Cloud Security, which contains security bulletins, certification information, and security whitepapers that explain the security capabilities of AWS. Page 8 Amazon Web Services Web Application Hosting in the AWS Cloud Load balancing across clusters Hardware load balancers are a common network appliance used in traditional web application architectures. AWS provides this capability through the Elastic Load Balancing (ELB) service. ELB automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, AWS Lambda functions, and virtual appliances. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers four types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant. Finding other hosts and services In the traditional web hosting architecture, most of your hosts have static IP addresses. In the AWS Cloud, most of your hosts have dynamic IP addresses. Although every EC2 instance can have both public and private DNS entries and will be addressable over the internet, the DNS entries and the IP addresses are assigned dynamically when you launch the instance. They cannot be manually assigned. Static IP addresses (Elastic IP addresses in AWS terminology) can be assigned to running instances after they are launched. You should use Elastic IP addresses for instances and services that require consistent endpoints, such as primary databases, central file servers, and EC2-hosted load balancers. Caching within the web application In-memory application caches can reduce load on services and improve performance and scalability on the database tier by caching frequently used information. Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in memory cache in the cloud. You can configure the in-memory cache you create to automatically scale with load and to automatically replace failed nodes. ElastiCache is protocol-compliant with Memcached and Redis, which simplifies cloud migrations for customers running these services on-premises. Database configuration, backup, and failover Many web applications contain some form of persistence, usually in the form of a relational or non-relational database. AWS offers both relational and non-relational Page 9 Amazon Web Services Web Application Hosting in the AWS Cloud database services. Alternatively, you can deploy your own database software on an EC2 instance. The following table summarizes these options, which are discussed in greater detail in this section. Table 1 — Relational and non-relational database solutions Relational database solutions Non-relational database solutions Managed database service Amazon RDS for MySQL Oracle SQL Server MariaDB PostgreSQL Amazon Aurora Amazon DynamoDB Amazon Keyspaces Amazon Neptune Amazon QLDB Amazon Timestream Self-managed Hosting a relational database management system (DBMS) on an Amazon EC2 instance Hosting a non-relational database solution on an EC2 instance Amazon RDS Amazon RDS gives you access to the capabilities of a familiar MySQL, PostgreSQL, Oracle, and Microsoft SQL Server database engine. The code, applications, and tools that you already use can be used with Amazon RDS. Amazon RDS automatically patches the database software and backs up your database, and it stores backups for a user-defined retention period. It also supports point-in-time recovery. You can benefit from the flexibility of being able to scale the compute resources or storage capacity associated with your relational database instance by making a single API call. Amazon RDS Multi-AZ deployments increase your database availability and protect your database against unplanned outages. Amazon RDS Read Replicas provide read only replicas of your database, so you can scale out beyond the capacity of a single database deployment for read-heavy database workloads. As with all AWS services, no upfront investments are required, and you pay only for the resources you use. Page 10 Amazon Web Services Web Application Hosting in the AWS Cloud Hosting a relational database management system (RDBMS) on an Amazon EC2 instance In addition to the managed Amazon RDS offering, you can install your choice of RDBMS (such as MySQL, Oracle, SQL Server, or DB2) on an EC2 instance and manage it yourself. AWS customers hosting a database on Amazon EC2 successfully use a variety of primary/standby and replication models, including mirroring for read only copies and log shipping for always-ready passive standbys. When managing your own database software directly on Amazon EC2, you should also consider the availability of fault-tolerant and persistent storage. For this purpose, we recommend that databases running on Amazon EC2 use Amazon Elastic Block Store (Amazon EBS) volumes, which are similar to network-attached storage. For EC2 instances running a database, you should place all database data and logs on EBS volumes. These will remain available even if the database host fails. This configuration allows for a simple failover scenario, in which a new EC2 instance can be launched if a host fails, and the existing EBS volumes can be attached to the new instance. The database can then pick up where it left off. EBS volumes automatically provide redundancy within the Availability Zone. If the performance of a single EBS volume is not sufficient for your databases needs, volumes can be striped to increase input/output operations per second (IOPS) performance for your database. For demanding workloads, you can also use EBS Provisioned IOPS, where you specify the IOPS required. If you use Amazon RDS, the service manages its own storage so you can focus on managing your data. Non-relational databases In addition to support for relational databases, AWS also offers a number of managed non-relational databases: • Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. Using the AWS Management Console or the DynamoDB API, you can scale capacity up or down without downtime or performance degradation. Because DynamoDB handles the administrative burdens of operating and scaling distributed databases to AWS, Page 11 Amazon Web Services Web Application Hosting in the AWS Cloud you don’t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. • Amazon DocumentDB (with MongoDB compatibility) is a database service that is purpose-built for JSON data management at scale, fully managed and runs on AWS, and enterprise-ready with high durability. • Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra-compatible database service. With Amazon Keyspaces, you can run your Cassandra workloads on AWS using the same Cassandra application code and developer tools that you use today. • Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. • Amazon Quantum Ledger Database (QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority. Amazon QLDB can be used to track each and every application data change and maintains a complete and verifiable history of changes over time. • Amazon Timestream is a fast, scalable, and serverless time series database service for IoT and operational applications that makes it easy to store and analyze trillions of events per day up to 1,000 times faster and at as little as 1/10th the cost of relational databases. Additionally, you can use Amazon EC2 to host other non-relational database technologies you may be working with. Storage and backup of data and assets There are numerous options within the AWS Cloud for storing, accessing, and backing up your web application data and assets. Amazon S3 provides a highly available and redundant object store. S3 is a great storage solution for static objects, such as images, videos, and other static media. S3 also supports edge caching and streaming of these assets by interacting with CloudFront. Page 12 Amazon Web Services Web Application Hosting in the AWS Cloud For attached file system-like storage, EC2 instances can have EBS volumes attached. These act like mountable disks for running EC2 instances. Amazon EBS is great for data that needs to be accessed as block storage and that requires persistence beyond the life of the running instance, such as database partitions and application logs. In addition to having a lifetime that is independent of the EC2 instance, you can take snapshots of EBS volumes and store them in S3. Because EBS snapshots only back up changes since the previous snapshot, more frequent snapshots can reduce snapshot times. You can also use an EBS snapshot as a baseline for replicating data across multiple EBS volumes and attaching those volumes to other running instances. EBS volumes can be as large as 16TB, and multiple EBS volumes can be striped for even larger volumes or for increased input/output (I/O) performance. To maximize the performance of your I/O-intensive applications, you can use Provisioned IOPS volumes. Provisioned IOPS volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads that are sensitive to storage performance and consistency in random access I/O throughput. You specify an IOPS rate when you create the volume and Amazon EBS provisions that rate for the lifetime of the volume. Amazon EBS currently supports IOPS per volume ranging from maximum of 16000 (for all instance types) up to 64,000 (for instances built on Nitro System). You can stripe multiple volumes together to deliver thousands of IOPS per instance to your application. Apart from this, for higher throughput and mission critical workloads requiring sub-millisecond latency, you can use io2 block express volume type which can support up-to 256,000 IOPS with a maximum storage capacity of 64TB. Automatically scaling the fleet One of the key differences between the AWS Cloud architecture and the traditional hosting model is that AWS can automatically scale the web application fleet on demand to handle changes in traffic. In the traditional hosting model, traffic forecasting models are generally used to provision hosts ahead of projected traffic. In AWS, instances can be provisioned on the fly according to a set of triggers for scaling the fleet out and back in. The Auto Scaling service can create capacity groups of servers that can grow or shrink on demand. Auto Scaling also works directly with Amazon CloudWatch for metrics data Page 13 Amazon Web Services Web Application Hosting in the AWS Cloud and with Elastic Load Balancing to add and remove hosts for load distribution. For example, if the web servers are reporting greater than 80 percent CPU utilization over a period of time, an additional web server could be quickly deployed and then automatically added to the load balancer for immediate inclusion in the load balancing rotation. As shown in the AWS web hosting architecture model, you can create multiple Auto Scaling groups for different layers of the architecture, so that each layer can scale independently. For example, the web server Auto Scaling group might trigger scaling in and out in response to changes in network I/O, whereas the application server Auto Scaling group might scale out and in according to CPU utilization. You can set minimums and maximums to help ensure 24/7 availability and to cap the usage within a group. Auto Scaling triggers can be set both to grow and to shrink the total fleet at a given layer to match resource utilization to actual demand. In addition to the Auto Scaling service, you can scale Amazon EC2 fleets directly through the Amazon EC2 API, which allows for launching, terminating, and inspecting instances. Additional security features The number and sophistication of Distributed Denial of Service (DDoS) attacks are rising. Traditionally, these attacks are difficult to fend off. They often end up being costly in both mitigation time and power spent, as well as the opportunity cost from lost visits to your website during the attack. There are a number of AWS factors and services that can help you defend against such attacks. One of them is the scale of the AWS network. The AWS infrastructure is quite large, and enables you to leverage our scale to optimize your defense. Several services, including Elastic Load Balancing, Amazon CloudFront, and Amazon Route 53, are effective at scaling your web application in response to a large increase in traffic. The infrastructure protection services in particular help with your defense strategy: • AWS Shield is a managed DDoS protection service that helps safeguard against various forms of DDoS attack vectors. The standard offering of AWS Shield is free and automatically active throughout your account. This standard offering helps to defend against the most common network and transportation layer attacks. In addition to this level, the advanced offering grants higher levels of Page 14 Amazon Web Services Web Application Hosting in the AWS Cloud protection against your web application by providing you with near real-time visibility into an ongoing attack, as well as integrating at higher levels with the services mentioned earlier. Additionally, you get access to the AWS DDoS Response Team (DRT) to help mitigate large-scale and sophisticated attacks against your resources. • AWS WAF (Web Application Firewall) is designed to protect your web applications from attacks that can compromise availability or security, or otherwise consume excessive resources. AWS WAF works in line with CloudFront or Application Load Balancer, along with your custom rules, to defend against attacks such as cross-site scripting, SQL injection, and DDoS. As with most AWS services, AWS WAF comes with a fully featured API that can help automate the creation and editing of rules for your AWS WAF instance as your security needs change. • AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Failover with AWS Another key advantage of AWS over traditional web hosting is the Availability Zones that give you easy access to redundant deployment locations. Availability Zones are physically distinct locations that are engineered to be insulated from failures in other Availability Zones. They provide inexpensive, low-latency network connectivity to other Availability Zones in the same AWS Region. As the AWS web hosting architecture diagram shows, AWS recommends that you deploy EC2 hosts across multiple Availability Zones to make your web application more fault tolerant. It’s important to ensure that there are provisions for migrating single points of access across Availability Zones in the case of failure. For example, you should set up a database standby in a second Availability Zone so that the persistence of data remains consistent and highly available, even during an unlikely failure scenario. You can do this on Amazon EC2 or Amazon RDS with the click of a button. Page 15 Amazon Web Services Web Application Hosting in the AWS Cloud While some architectural changes are often required when moving an existing web application to the AWS Cloud, there are significant improvements to scalability, reliability, and cost-effectiveness that make using the AWS Cloud well worth the effort. The next section discusses those improvements. Key considerations when using AWS for web hosting There are some key differences between the AWS Cloud and a traditional web application hosting model. The previous section highlighted many of the key areas that you should consider when deploying a web application to the cloud. This section points out some of the key architectural shifts that you need to consider when you bring any application into the cloud. No more physical network appliances You cannot deploy physical network appliances in AWS. For example, firewalls, routers, and load balancers for your AWS applications can no longer reside on physical devices, but must be replaced with software solutions. There is a wide variety of enterprise quality software solutions, whether for load balancing or establishing a VPN connection. This is not a limitation of what can be run on the AWS Cloud, but it is an architectural change to your application if you use these devices today. Firewalls everywhere Where you once had a simple demilitarized zone (DMZ) and then open communications among your hosts in a traditional hosting model, AWS enforces a more secure model, in which every host is locked down. One of the steps in planning an AWS deployment is the analysis of traffic between hosts. This analysis will guide decisions on exactly what ports need to be opened. You can create security groups for each type of host in your architecture. You can also create a large variety of simple and tiered security models to enable the minimum access among hosts within your architecture. The use of network access control lists within Amazon VPC can help lock down your network at the subnet level. Page 16 Amazon Web Services Web Application Hosting in the AWS Cloud Consider the availability of multiple data centers Think of Availability Zones within an AWS Region as multiple data centers. EC2 instances in different Availability Zones are both logically and physically separated, and they provide an easy-to-use model for deploying your application across data centers for both high availability and reliability. Amazon VPC as a Regional service enables you to leverage Availability Zones while keeping all of your resources in the same logical network. Treat hosts as ephemeral and dynamic Probably the most important shift in how you might architect your AWS application is that Amazon EC2 hosts should be considered ephemeral and dynamic. Any application built for the AWS Cloud should not assume that a host will always be available and should be designed with the knowledge that any data in the EC2 instant stores will be lost if an EC2 instance fails. When a new host is brought up, you shouldn’t make assumptions about the IP address or location within an Availability Zone of the host. Your configuration model must be flexible, and your approach to bootstrapping a host must take the dynamic nature of the cloud into account. These techniques are critical for building and running a highly scalable and fault-tolerant application. Consider containers and serverless This whitepaper primarily focuses on a more traditional web architecture. However, consider modernizing your web applications by moving to Containers and Serverless technologies, leveraging services like AWS Fargate and AWS Lambda to enable you to abstracts away the use of virtual machines to perform compute tasks. With serverless computing, infrastructure management tasks like capacity provisioning and patching are handled by AWS, so you can build more agile applications that allow you to innovate and respond to change faster. Page 17 Amazon Web Services Web Application Hosting in the AWS Cloud Consider automated deployment • Amazon Lightsail is an easy-to-use virtual private server (VPS) that offers you everything needed to build an application or website, plus a cost-effective, monthly plan. Lightsail is ideal for simpler workloads, quick deployments, and getting started on AWS. It’s designed to help you start small, and then scale as you grow. • AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, NGINX, Passenger, and IIS. You can simply upload your code, and Elastic Beanstalk automatically handles the deployment, capacity provisioning, load balancing, automatic scaling, and application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. • AWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner automatically builds and deploys the web application and load balances traffic with encryption. App Runner also scales up or down automatically to meet your traffic needs. • AWS Amplify is a set of tools and services that can be used together or on their own, to help front-end web and mobile developers build scalable full stack applications, powered by AWS. With Amplify, you can configure app backends and connect your app in minutes, deploy static web apps in a few clicks, and easily manage app content outside the AWS Management Console. Conclusion There are numerous architectural and conceptual considerations when you are contemplating migrating your web application to the AWS Cloud. The benefits of having a cost-effective, highly scalable, and fault-tolerant infrastructure that grows with your business far outstrips the efforts of migrating to the AWS Cloud. Page 18 Amazon Web Services Web Application Hosting in the AWS Cloud Contributors The following individuals and organizations contributed to this document: • Amir Khairalomoum, Senior Solutions Architect, AWS • Dinesh Subramani, Senior Solutions Architect, AWS • Jack Hemion, Senior Solutions Architect, AWS • Jatin Joshi, Cloud Support Engineer, AWS • Jorge Fonseca, Senior Solutions Architect, AWS • Shinduri K S, Solutions Architect, AWS Further reading • Deploy Django-based application onto Amazon LightSail • Deploying a high availability Drupal website to Elastic Beanstalk • Deploying a high availability PHP application to Elastic Beanstalk • Deploying a Node.js application with DynamoDB to Elastic Beanstalk • Getting Started with Linux Web Applications in the AWS Cloud • Host a Static Website • Hosting a static website using Amazon S3 • Tutorial: Deploying an ASP.NET core application with Elastic Beanstalk • Tutorial: How to deploy a .NET sample application using Elastic Beanstalk Document versions Date Description August 20, 2021 Multiple sections and diagrams updated with new services, features, and updated service limits. Page 19 Amazon Web Services Web Application Hosting in the AWS Cloud Date Description September 2019 Updated icon label for “Caching with ElastiCache” July 2017 Multiple sections added and updated for new services. Updated diagrams for additional clarity and services. Addition of VPC as the standard networking method in AWS in “Network Management.” Added section on DDoS protection and mitigation in “Additional Security Features.” Added a small section on serverless architectures for web hosting. September 2012 Multiple sections updated to improve clarity. Updated diagrams to use AWS icons. Addition of “Managing Public DNS” section for detail on Amazon Route 53. “Finding Other Hosts and Services” section updated for clarity. “Database Configuration, Backup, and Failover” section updated for clarity and DynamoDB. “Storage and Backup of Data and Assets” section expanded to cover EBS Provisioned IOPS volumes. May 2010 First publication Page 20 
Practicing Continuous Integration and Continuous Delivery on AWS Accelerating Software Delivery with DevOps First Published June 1, 2017 Updated October 27, 2021 This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Notices Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. © 2021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Contents The challenge of software delivery ..................................................................................... 1 What is continuous integration and continuous delivery/deployment? .............................. 2 Continuous integration ..................................................................................................... 2 Continuous delivery and deployment .............................................................................. 2 Continuous delivery is not continuous deployment ......................................................... 3 Benefits of continuous delivery ........................................................................................ 3 Implementing continuous integration and continuous delivery .......................................... 4 A pathway to continuous integration/continuous delivery ............................................... 5 Teams ............................................................................................................................... 9 Testing stages in continuous integration and continuous delivery ............................... 10 Building the pipeline ....................................................................................................... 13 Pipeline integration with AWS CodeBuild ...................................................................... 22 Pipeline integration with Jenkins.................................................................................... 23 Deployment methods ........................................................................................................ 24 All at once (in-place deployment) .................................................................................. 26 Rolling deployment ......................................................................................................... 26 Immutable and blue/green deployments ....................................................................... 26 Database schema changes ............................................................................................... 27 Summary of best practices ................................................................................................ 28 Conclusion ......................................................................................................................... 29 Further reading .................................................................................................................. 29 Contributors ....................................................................................................................... 30 Document revisions ........................................................................................................... 30 Abstract This paper explains the features and benefits of using continuous integration and continuous delivery (CI/CD) along with Amazon Web Services (AWS) tooling in your software development environment. Continuous integration and continuous delivery are best practices and a vital part of a DevOps initiative. This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS The challenge of software delivery Enterprises today face the challenges of rapidly changing competitive landscapes, evolving security requirements, and performance scalability. Enterprises must bridge the gap between operations stability and rapid feature development. Continuous integration and continuous delivery (CI/CD) are practices that enable rapid software changes while maintaining system stability and security. Amazon realized early on that the business needs of delivering features for Amazon.com retail customers, Amazon subsidiaries and Amazon Web Services (AWS) would require new and innovative ways of delivering software. At the scale of a company like Amazon, thousands of independent software teams must be able to work in parallel to deliver software quickly, securely, reliably, and with zero tolerance for outages. By learning how to deliver software at high velocity, Amazon and other forward-thinking organizations pioneered DevOps. DevOps is a combination of cultural philosophies, practices, and tools that increase an organization’s ability to deliver applications and services at high velocity. Using DevOps principles, organizations can evolve and improve products at a faster pace than organizations that use traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market. Some of these principles, such as two-pizza teams and microservices/service-oriented architecture (SOA), are out of the scope of this whitepaper. This whitepaper discusses the CI/CD capability that Amazon has built and continuously improved. CI/CD is key to delivering software features rapidly and reliably. This version has been archived. For the latest version of this document, visit: AWS now offers these CI/CD capabilities as a set of developer services: AWS CodeStar, AWS CodeCommit, AWS CodePipeline, AWS CodeBuild, AWS CodeDeploy, and AWS CodeArtifact. Developers and IT operations professionals practicing DevOps can use these services to rapidly, safely, and securely deliver software. Together they help you securely store and apply version control to your application's source code. You can use AWS CodeStar to rapidly orchestrate an end-to-end software release workflow using these services. For an existing environment, CodePipeline has the flexibility to integrate each service independently with your existing tools. These are highly available, easily integrated services that can be accessed through the AWS Management Console, AWS application programming interfaces (APIs), and AWS software development toolkits (SDKs) like any other AWS service. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 1 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS What is continuous integration and continuous delivery/deployment? This section discusses the practices of continuous integration and continuous delivery and explains the difference between continuous delivery and continuous deployment. Continuous integration Continuous integration (CI) is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. CI most often refers to the build or integration stage of the software release process and requires both an automation component (for example a CI or build service) and a cultural component (for example learning to integrate frequently). The key goals of CI are to find and address bugs more quickly, improve software quality, and reduce the time it takes to validate and release new software updates. Continuous integration focuses on smaller commits and smaller code changes to integrate. A developer commits code at regular intervals, at minimum once a day. The developer pulls code from the code repository to ensure the code on the local host is merged before pushing to the build server. At this stage the build server runs the various tests and either accepts or rejects the code commit. The basic challenges of implementing CI include more frequent commits to the common codebase, maintaining a single source code repository, automating builds, and automating testing. Additional challenges include testing in similar environments to production, providing visibility of the process to the team, and allowing developers to easily obtain any version of the application. This version has been archived. Continuous delivery and deployment For the latest version of this document, visit: Continuous delivery (CD) is a software development practice where code changes are automatically built, tested, and prepared for production release. It expands on continuous integration by deploying all code changes to a testing environment, a production environment, or both after the build stage has been completed. Continuous delivery can be fully automated with a workflow process or partially automated with manual steps at critical points. When continuous delivery is properly implemented, developers always have a deployment-ready build artifact that has passed through a standardized test process. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 2 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS With continuous deployment, revisions are deployed to a production environment automatically without explicit approval from a developer, making the entire software release process automated. This, in turn, allows for a continuous customer feedback loop early in the product lifecycle. Continuous delivery is not continuous deployment One misconception about continuous delivery is that it means every change committed is applied to production immediately after passing automated tests. However, the point of continuous delivery is not to apply every change to production immediately, but to ensure that every change is ready to go to production. Before deploying a change to production, you can implement a decision process to ensure that the production deployment is authorized and audited. This decision can be made by a person and then executed by the tooling. Using continuous delivery, the decision to go live becomes a business decision, not a technical one. The technical validation happens on every commit. Rolling out a change to production is not a disruptive event. Deployment doesn’t require the technical team to stop working on the next set of changes, and it doesn’t need a project plan, handover documentation, or a maintenance window. Deployment becomes a repeatable process that has been carried out and proven multiple times in testing environments. Benefits of continuous delivery CD provides numerous benefits for your software development team including automating the process, improving developer productivity, improving code quality, and delivering updates to your customers faster. This version has been archived. For the latest version of this document, visit: Automate the software release process CD provides a method for your team to check in code that is automatically built, tested, and prepared for release to production so that your software delivery is efficient, resilient, rapid, and secure. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Improve developer productivity CD practices help your team’s productivity by freeing developers from manual tasks, untangling complex dependencies, and returning focus to delivering new features in 3 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS software. Instead of integrating their code with other parts of the business and spending cycles on how to deploy this code to a platform, developers can focus on coding logic that delivers the features you need. Improve code quality CD can help you discover and address bugs early in the delivery process before they grow into larger problems later. Your team can easily perform additional types of code tests because the entire process has been automated. With the discipline of more testing more frequently, teams can iterate faster with immediate feedback on the impact of changes. This enables teams to drive quality code with a high assurance of stability and security. Developers will know through immediate feedback whether the new code works and whether any breaking changes or bugs were introduced. Mistakes caught early on in the development process are the easiest to fix. Deliver updates faster CD helps your team deliver updates to customers quickly and frequently. When CI/CD is implemented, the velocity of the entire team, including the release of features and bug fixes, is increased. Enterprises can respond faster to market changes, security challenges, customer needs, and cost pressures. For example, if a new security feature is required, your team can implement CI/CD with automated testing to introduce the fix quickly and reliably to production systems with high confidence. What used to take weeks and months can now be done in days or even hours. Implementing continuous integration and continuous delivery This version has been archived. This section discusses the ways in which you can begin to implement a CI/CD model in For the latest version of this document, visit: your organization. This whitepaper doesn’t discuss how an organization with a mature DevOps and cloud transformation model builds and uses a CI/CD pipeline. To help you on your DevOps journey, AWS has a number of certified DevOps Partners who can provide resources and tooling. For more information on preparing for a move to the AWS Cloud, refer to the AWS Building a Cloud Operating Model. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 4 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS A pathway to continuous integration/continuous delivery CI/CD can be pictured as a pipeline (refer to the following figure), where new code is submitted on one end, tested over a series of stages (source, build, staging, and production), and then published as production-ready code. If your organization is new to CI/CD it can approach this pipeline in an iterative fashion. This means that you should start small, and iterate at each stage so that you can understand and develop your code in a way that will help your organization grow. CI/CD pipeline Each stage of the CI/CD pipeline is structured as a logical unit in the delivery process. In addition, each stage acts as a gate that vets a certain aspect of the code. As the code progresses through the pipeline, the assumption is that the quality of the code is higher in the later stages because more aspects of it continue to be verified. Problems uncovered in an early stage stop the code from progressing through the pipeline. Results from the tests are immediately sent to the team, and all further builds and releases are stopped if software does not pass the stage. This version has been archived. These stages are suggestions. You can adapt the stages based on your business need. Some stages can be repeated for multiple types of testing, security, and performance. Depending on the complexity of your project and the structure of your teams, some stages can be repeated several times at different levels. For example, the end product of one team can become a dependency in the project of the next team. This means that For the latest version of this document, visit: the first team’s end product is subsequently staged as an artifact in the next team’s project. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html The presence of a CI/CD pipeline will have a large impact on maturing the capabilities of your organization. The organization should start with small steps and not try to build a fully mature pipeline, with multiple environments, many testing phases, and automation in all stages at the start. Keep in mind that even organizations that have highly mature CI/CD environments still need to continuously improve their pipelines. 5 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Building a CI/CD-enabled organization is a journey, and there are many destinations along the way. The next section discusses a possible pathway that your organization could take, starting with continuous integration through the levels of continuous delivery. Continuous integration Continuous integration—source and build The first phase in the CI/CD journey is to develop maturity in continuous integration. You should make sure that all of the developers regularly commit their code to a central repository (such as one hosted in CodeCommit or GitHub) and merge all changes to a release branch for the application. No developer should be holding code in isolation. If a feature branch is needed for a certain period of time, it should be kept up to date by merging from upstream as often as possible. Frequent commits and merges with complete units of work are recommended for the team to develop discipline and are encouraged by the process. A developer who merges code early and often, will likely have fewer integration issues down the road. This version has been archived. You should also encourage developers to create unit tests as early as possible for their applications and to run these tests before pushing the code to the central repository. Errors caught early in the software development process are the cheapest and easiest to fix. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html When the code is pushed to a branch in a source code repository, a workflow engine monitoring that branch will send a command to a builder tool to build the code and run the unit tests in a controlled environment. The build process should be sized appropriately to handle all activities, including pushes and tests that might happen during the commit stage, for fast feedback. Other quality checks, such as unit test coverage, style check, and static analysis, can happen at this stage as well. Finally, the builder tool creates one or more binary builds and other artifacts, like images, stylesheets, and documents for the application. 6 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Continuous delivery: creating a staging environment Continuous delivery—staging Continuous delivery (CD) is the next phase and entails deploying the application code in a staging environment, which is a replica of the production stack, and running more functional tests. The staging environment could be a static environment premade for testing, or you could provision and configure a dynamic environment with committed infrastructure and configuration code for testing and deploying the application code. Continuous delivery: creating a production environment This version has been archived. For the latest version of this document, visit: Continuous delivery—production https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html In the deployment/delivery pipeline sequence, after the staging environment, is the production environment, which is also built using infrastructure as code (IaC). 7 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Continuous deployment Continuous deployment The final phase in the CI/CD deployment pipeline is continuous deployment, which may include full automation of the entire software release process including deployment to the production environment. In a fully mature CI/CD environment, the path to the production environment is fully automated, which allows code to be deployed with high confidence. Maturity and beyond As your organization matures, it will continue to develop the CI/CD model to include more of the following improvements: • More staging environments for specific performance, compliance, security, and user interface (UI) tests • Unit tests of infrastructure and configuration code along with the application code This version has been archived. • Integration with other systems and processes such as code review, issue tracking, and event notification For the latest version of this document, visit: • Integration with database schema migration (if applicable) • Additional steps for auditing and business approval https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Even the most mature organizations that have complex multi-environment CI/CD pipelines continue to look for improvements. DevOps is a journey, not a destination. Feedback about the pipeline is continuously collected and improvements in speed, scale, security, and reliability are achieved as a collaboration between the different parts of the development teams. 8 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Teams AWS recommends organizing three developer teams for implementing a CI/CD environment: an application team, an infrastructure team, and a tools team (refer to the following figure). This organization represents a set of best practices that have been developed and applied in fast-moving startups, large enterprise organizations, and in Amazon itself. The teams should be no larger than groups that two pizzas can feed, or about 10-12 people. This follows the communication rule that meaningful conversations hit limits as group sizes increase and lines of communication multiply. Application, infrastructure, and tools teams The application team creates the application. Application developers own the backlog, stories, and unit tests, and they develop features based on a specified application Application team This version has been archived. target. This team’s organizational goal is to minimize the time these developers spend For the latest version of this document, visit: on non-core application tasks. In addition to having functional programming skills in the application language, the application team should have platform skills and an understanding of system configuration. This will enable them to focus solely on developing features and hardening the application. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 9 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Infrastructure team The infrastructure team writes the code that both creates and configures the infrastructure needed to run the application. This team might use native AWS tools, such as AWS CloudFormation, or generic tools, such as Chef, Puppet, or Ansible. The infrastructure team is responsible for specifying what resources are needed, and it works closely with the application team. The infrastructure team might consist of only one or two people for a small application. The team should have skills in infrastructure provisioning methods, such as AWS CloudFormation or HashiCorp Terraform. The team should also develop configuration automation skills with tools such as Chef, Ansible, Puppet, or Salt. Tools team The tools team builds and manages the CI/CD pipeline. They are responsible for the infrastructure and tools that make up the pipeline. They are not part of the two-pizza team; however, they create a tool that is used by the application and infrastructure teams in the organization. The organization needs to continuously mature its tools team, so that the tools team stays one step ahead of the maturing application and infrastructure teams. The tools team must be skilled in building and integrating all parts of the CI/CD pipeline. This includes building source control repositories, workflow engines, build environments, testing frameworks, and artifact repositories. This team may choose to implement software such as AWS CodeStar, AWS CodePipeline, AWS CodeCommit, AWS CodeDeploy, AWS CodeBuild and AWS CodeArtifact, along with Jenkins, GitHub, Artifactory, TeamCity, and other similar tools. Some organizations might call this a DevOps team, but AWS discourages this and instead encourages thinking of DevOps as the sum of the people, processes, and tools in software delivery. This version has been archived. For the latest version of this document, visit: Testing stages in continuous integration and continuous delivery https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html The three CI/CD teams should incorporate testing into the software development lifecycle at the different stages of the CI/CD pipeline. Overall, testing should start as early as possible. The following testing pyramid is a concept provided by Mike Cohn in the book Succeeding with Agile. It shows the various software tests in relation to their cost and the speed at which they run. 10 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS CI/CD testing pyramid Unit tests are on the bottom of the pyramid. They are both the fastest to run and the least expensive. Therefore, unit tests should make up the bulk of your testing strategy. A good rule of thumb is about 70 percent. Unit tests should have near-complete code coverage because bugs caught in this phase can be fixed quickly and cheaply. Service, component, and integration tests are above unit tests on the pyramid. These tests require detailed environments and therefore, are more costly in infrastructure requirements and slower to run. Performance and compliance tests are the next level. They require production-quality environments and are more expensive yet. UI and user acceptance tests are at the top of the pyramid and require production-quality environments as well. This version has been archived. All of these tests are part of a complete strategy to assure high-quality software. However, for speed of development, emphasis is on the number of tests and the coverage in the bottom half of the pyramid. For the latest version of this document, visit: The following sections discuss the CI/CD stages. Setting up the source https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html At the beginning of the project, it’s essential to set up a source where you can store your raw code and configuration and schema changes. In the source stage, choose a source code repository such as one hosted in GitHub or AWS CodeCommit. 11 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Setting up and running builds Build automation is essential to the CI process. When setting up build automation, the first task is to choose the right build tool. There are many build tools, such as: • Ant, Maven, and Gradle for Java • Make for C/C++ • Grunt for JavaScript • Rake for Ruby The build tool that will work best for you depends on the programming language of your project and the skill set of your team. After you choose the build tool, all the dependencies need to be clearly defined in the build scripts, along with the build steps. It’s also a best practice to version the final build artifacts, which makes it easier to deploy and to keep track of issues. Building In the build stage, the build tools will take as input any change to the source code repository, build the software, and run the following types of tests: Unit testing – Tests a specific section of code to ensure the code does what it is expected to do. The unit testing is performed by software developers during the development phase. At this stage, a static code analysis, data flow analysis, code coverage, and other software verification processes can be applied. Static code analysis – This test is performed without actually executing the application after the build and unit testing. This analysis can help to find coding errors and security holes, and it also can ensure conformance to coding guidelines. This version has been archived. Staging For the latest version of this document, visit: In the staging phase, full environments are created that mirror the eventual production environment. The following tests are performed: Integration testing – Verifies the interfaces between components against software design. Integration testing is an iterative process and facilitates building robust interfaces and system integrity. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 12 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Component testing – Tests message passing between various components and their outcomes. A key goal of this testing could be idempotency in component testing. Tests can include extremely large data volumes, or edge situations and abnormal inputs. System testing – Tests the system end-to-end and verifies if the software satisfies the business requirement. This might include testing the user interface (UI), API, backend logic, and end state. Performance testing – Determines the responsiveness and stability of a system as it performs under a particular workload. Performance testing also is used to investigate, measure, validate, or verify other quality attributes of the system, such as scalability, reliability, and resource usage. Types of performance tests might include load tests, stress tests, and spike tests. Performance tests are used for benchmarking against predefined criteria. Compliance testing – Checks whether the code change complies with the requirements of a nonfunctional specification and/or regulations. It determines if you are implementing and meeting the defined standards. User acceptance testing – Validates the end-to-end business flow. This testing is executed by an end user in a staging environment and confirms whether the system meets the requirements of the requirement specification. Typically, customers employ alpha and beta testing methodologies at this stage. Production This version has been archived. Finally, after passing the previous tests, the staging phase is repeated in a production environment. In this phase, a final Canary test can be completed by deploying the new code only on a small subset of servers or even one server, or one AWS Region before deploying code to the entire production environment. Specifics on how to safely deploy to production are covered in the Deployment Methods section. For the latest version of this document, visit: The next section discusses building the pipeline to incorporate these stages and tests. Building the pipeline https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html This section discusses building the pipeline. Start by establishing a pipeline with just the components needed for CI and then transition later to a continuous delivery pipeline with more components and stages. This section also discusses how you can consider using AWS Lambda functions and manual approvals for large projects, plan for multiple teams, branches, and AWS Regions. 13 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Starting with a minimum viable pipeline for continuous integration Your organization’s journey toward continuous delivery begins with a minimum viable pipeline (MVP). As discussed in Implementing continuous integration and continuous delivery, teams can start with a very simple process, such as implementing a pipeline that performs a code style check or a single unit test without deployment. A key component is a continuous delivery orchestration tool. To help you build this pipeline, Amazon developed AWS CodeStar. AWS CodeStar setup page This version has been archived. For the latest version of this document, visit: AWS CodeStar uses AWS CodePipeline, AWS CodeBuild, AWS CodeCommit, and AWS CodeDeploy with an integrated setup process, tools, templates, and dashboard. AWS CodeStar provides everything you need to quickly develop, build, and deploy applications on AWS. This allows you to start releasing code faster. Customers who are already familiar with the AWS Management Console and seek a higher level of control can manually configure their developer tools of choice and can provision individual AWS services as needed. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 14 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS AWS CodeStar dashboard This version has been archived. AWS CodePipeline is a CI/CD service that can be used through AWS CodeStar or through the AWS Management Console for fast and reliable application and infrastructure updates. AWS CodePipeline builds, tests, and deploys your code every time there is a code change, based on the release process models you define. This enables you to rapidly and reliably deliver features and updates. You can easily build out an end-to-end solution by using our pre-built plugins for popular third-party services like GitHub or by integrating your own custom plugins into any stage of your release process. With AWS CodePipeline you only pay for what you use. There are no upfront fees or long-term commitments. For the latest version of this document, visit: The steps of AWS CodeStar and AWS CodePipeline map directly to the source, build, staging, and production CI/CD stages. While continuous delivery is desirable, you could start out with a simple two-step pipeline that checks the source repository and performs a build action: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 15 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS AWS CodePipeline - source and build stages For AWS CodePipeline, the source stage can accept inputs from GitHub, AWS CodeCommit, and Amazon Simple Storage Service (Amazon S3). Automating the build process is a critical first step for implementing continuous delivery and moving toward continuous deployment. Eliminating human involvement in producing build artifacts removes the burden from your team, minimizes errors introduced by manual packaging, and allows you to start packaging consumable artifacts more often. AWS CodePipeline works seamlessly with AWS CodeBuild, a fully managed build service, to make it easier to set up a build step within your pipeline that packages your code and runs unit tests. With AWS CodeBuild, you don’t need to provision, manage, or This version has been archived. For the latest version of this document, visit: scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently so your builds are not left waiting in a queue. AWS CodePipeline also integrates with build servers such as Jenkins, Solano CI, and TeamCity. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html For example, in the following build stage, three actions (unit testing, code style checks, and code metrics collection) run in parallel. Using AWS CodeBuild, these steps can be added as new projects without any further effort in building or installing build servers to handle the load. 16 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS CodePipeline — build functionality The source and build stages shown in the figure AWS CodePipeline—source and build stages, along with supporting processes and automation, support your team’s transition toward a continuous integration. At this level of maturity, developers need to regularly pay attention to build and test results. They need to grow and maintain a healthy unit test base as well. This, in turn, bolsters the entire team’s confidence in the CI/CD pipeline and furthers its adoption. This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 17 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html AWS CodePipeline stages 18 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Continuous delivery pipeline After the continuous integration pipeline has been implemented and supporting processes have been established, your teams can start transitioning toward the continuous delivery pipeline. This transition requires teams to automate both building and deploying applications. A continuous delivery pipeline is characterized by the presence of staging and production steps, where the production step is performed after a manual approval. In the same manner the continuous integration pipeline was built, your teams can gradually start building a continuous delivery pipeline by writing their deployment scripts. Depending on the needs of an application, some of the deployment steps can be abstracted by existing AWS services. For example, AWS CodePipeline directly integrates with AWS CodeDeploy, a service that automates code deployments to Amazon EC2 instances and instances running on-premises, AWS OpsWorks, a configuration management service that helps you operate applications using Chef, and to AWS Elastic Beanstalk, a service for deploying and scaling web applications and services. AWS has detailed documentation on how to implement and integrate AWS CodeDeploy with your infrastructure and pipeline. After your team successfully automates the deployment of the application, deployment stages can be expanded with various tests. For example, you can add other out-of-the box integrations with services like Ghost Inspector, Runscope, and others as shown in the following figure. This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html 19 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS AWS CodePipeline—code tests in deployment stages Adding Lambda actions AWS CodeStar and AWS CodePipeline support integration with AWS Lambda. This integration enables implementation of a broad set of tasks, such as creating custom resources in your environment, integrating with third-party systems (such as Slack), and performing checks on your newly deployed environment. Lambda functions can be used in CI/CD pipelines to do the following tasks: • Roll out changes to your environment by applying or updating an AWS CloudFormation template. • Create resources on demand in one stage of a pipeline using AWS CloudFormation and delete them in another stage. • Deploy application versions with zero downtime in AWS Elastic Beanstalk with a Lambda function that swaps Canonical Name record (CNAME) values. This version has been archived. • Deploy to Amazon EC2 Container Service (ECS) Docker instances. For the latest version of this document, visit: • Back up resources before building or deploying by creating an AMI snapshot. • Add integration with third-party products to your pipeline, such as posting messages to an Internet Relay Chate (IRC) client. Manual approvals https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Add an approval action to a stage in a pipeline at the point where you want the pipeline processing to stop so that someone with the required AWS Identity and Access Management (IAM) permissions can approve or reject the action. 20 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS If the action is approved, the pipeline processing resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline processing does not continue. Deploying infrastructure code changes in a CI/CD pipeline AWS CodeDeploy—manual approvals This version has been archived. For the latest version of this document, visit: AWS CodePipeline lets you select AWS CloudFormation as a deployment action in any stage of your pipeline. You can then choose the specific action you would like AWS CloudFormation to perform, such as creating or deleting stacks and creating or executing change sets. A stack is an AWS CloudFormation concept and represents a https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html group of related AWS resources. While there are many ways of provisioning Infrastructure as Code, AWS CloudFormation is a comprehensive tool recommended by AWS as a scalable, complete solution that can describe the most comprehensive set of AWS resources as code. AWS recommends using AWS CloudFormation in an AWS CodePipeline project to track infrastructure changes and tests. 21 This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS 22 CI/CD for serverless applications You can also use AWS CodeStar, AWS CodePipeline, AWS CodeBuild, and AWS CloudFormation to build CI/CD pipelines for serverless applications. Serverless applications integrate managed services such as Amazon Cognito, Amazon S3, and Amazon DynamoDB with event-driven service, and AWS Lambda to deploy applications in a manner which doesn’t require managing servers. If you are a serverless application developer, you can use the combination of AWS CodePipeline, AWS CodeBuild, and AWS CloudFormation to automate the building, testing, and deployment of serverless applications that are expressed in templates built with the AWS Serverless Application Model (SAM). For more information, refer to the AWS Lambda documentation for Automating Deployment of Lambda-based Applications. You can also create secure CI/CD pipelines that follow your organization’s best practices with AWS Serverless Application Model Pipelines (AWS SAM Pipelines). AWS SAM Pipelines are a new feature of AWS SAM CLI that give you access to benefits of CI/CD in minutes, such as accelerating deployment frequency, shortening lead time for changes, and reducing deployment errors. AWS SAM Pipelines come with a set of default pipeline templates for AWS CodeBuild/CodePipeline that follow AWS deployment best practices. For more information and to view the tutorial, refer to the blog Introducing AWS SAM Pipelines. Pipelines for multiple teams, branches, and AWS Regions For a large project, it’s not uncommon for multiple project teams to work on different components. If multiple teams use a single code repository, it can be mapped so that each team has its own branch. There should also be an integration or release branch for the final merge of the project. If a service-oriented or microservice architecture is used, each team could have its own code repository. In the first scenario, if a single pipeline is used it’s possible that one team could affect the other teams’ progress by blocking the pipeline. AWS recommends that you create specific pipelines for team branches and another release pipeline for the final product delivery. Pipeline integration with AWS CodeBuild AWS CodeBuild is designed to enable your organization to build a highly available build process with almost unlimited scale. AWS CodeBuild provides quickstart environments for a number of popular languages plus the ability to run any Docker container that you specify. Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS With the advantages of tight integration with AWS CodeCommit, AWS CodePipeline, and AWS CodeDeploy, as well as Git and CodePipeline Lambda actions, the CodeBuild tool is highly flexible. Software can be built through the inclusion of a buildspec.yml file that identifies each of the build steps, including pre- and post- build actions, or specified actions through the CodeBuild tool. You can view detailed history of each build using the CodeBuild dashboard. Events are stored as Amazon CloudWatch Logs log files. This version has been archived. CloudWatch Logs log files in AWS CodeBuild Pipeline integration with Jenkins For the latest version of this document, visit: You can use the Jenkins build tool to create delivery pipelines. These pipelines use standard jobs that define steps for implementing continuous delivery stages. However, this approach might not be optimal for larger projects because the current state of the https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html pipeline doesn’t persist between Jenkins restarts, implementing manual approval is not straightforward, and tracking the state of a complex pipeline can be complicated. Instead, AWS recommends that you implement continuous delivery with Jenkins by using the AWS Code Pipeline plugin. This plugin allows complex workflows to be described using Groovy-like domain-specific language and can be used to orchestrate 23 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS complex pipelines. The AWS Code Pipeline plugin’s functionality can be enhanced by the use of satellite plugins such as the Pipeline Stage View Plugin, which visualizes the current progress of stages defined in a pipeline, or Pipeline Multibranch Plugin, which groups builds from different branches. AWS recommends that you store your pipeline configuration in Jenkinsfile and have it checked into a source code repository. This allows for tracking changes to pipeline code and becomes even more important when working with the Pipeline Multibranch Plugin. AWS also recommends that you divide your pipeline into stages. This logically groups the pipeline steps and also enables the Pipeline Stage View Plugin to visualize the current state of the pipeline. The following figure shows a sample Jenkins pipeline, with four defined stages visualized by the Pipeline Stage View Plugin. This version has been archived. Defined stages of Jenkins pipeline visualized by the Pipeline Stage View Plugin For the latest version of this document, visit: Deployment methods You can consider multiple deployment strategies and variations for rolling out new versions of software in a continuous delivery process. This section discusses the most common deployment methods: all at once (deploy in place), rolling, immutable, and blue/green. AWS indicates which of these methods are supported by AWS CodeDeploy and AWS Elastic Beanstalk. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html The following table summarizes the characteristics of each deployment method. 24 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Table 1 - Characteristics of deployment methods Method Impact of failed deployment Deploy time Zero downtime No DNS change Rollback process Code deployed to Deploy in place Downtime ☓ ✓ Re-deploy Existing instances Rolling Single batch out of service. Any successful batches prior to failure running new application version. † ✓ ✓ Re-deploy Existing instances Rolling with additional batch (beanstalk) Immutable Minimal if first batch fails, otherwise similar to rolling. † ✓ ✓ Re-deploy New and existing instances Minimal ✓ ✓ Re-deploy New instances Traffic splitting This version has been archived. Minimal ✓ ✓ Re-route traffic and terminate new instances For the latest version of this document, visit: New instances Blue/green Minimal ✓ ☓ Switch back to old environmen t https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html New instances † Varies depending on batch size 25 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS All at once (in-place deployment) All at once (in-place deployment), is a method you can use to roll out new application code to an existing fleet of servers. This method replaces all the code in one deployment action. It requires downtime because all servers in the fleet are updated at once. There is no need to update existing DNS records. In case of a failed deployment, the only way to restore operations is to redeploy the code on all servers again. In AWS Elastic Beanstalk, this deployment is called all at once, and is available for single and load-balanced applications. In AWS CodeDeploy this deployment method is called in-place deployment with a deployment configuration of AllAtOnce. Rolling deployment With rolling deployment, the fleet is divided into portions so that all of the fleet isn’t upgraded at once. During the deployment process two software versions, new and old, are running on the same fleet. This method allows a zero-downtime update. If the deployment fails, only the updated portion of the fleet will be affected. A variation of the rolling deployment method, called canary release, involves deployment of the new software version on a very small percentage of servers at first. This way, you can observe how the software behaves in production on a few servers, while minimizing the impact of breaking changes. If there is an elevated rate of errors from a canary deployment, the software is rolled back. Otherwise, the percentage of servers with the new version is gradually increased. This version has been archived. AWS Elastic Beanstalk has followed the rolling deployment pattern with two deployment options, rolling and rolling with additional batch. These options allow the application to first scale up before taking servers out of service, preserving full capability during the deployment. AWS CodeDeploy accomplishes this pattern as a variation of an in-place deployment with patterns like OneAtATime and HalfAtATime. For the latest version of this document, visit: Immutable and blue/green deployments https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html The immutable pattern specifies a deployment of application code by starting an entirely new set of servers with a new configuration or version of application code. This pattern leverages the cloud capability that new server resources are created with simple API calls. The blue/green deployment strategy is a type of immutable deployment which also requires creation of another environment. Once the new environment is up and passed 26 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS all tests, traffic is shifted to this new deployment. Crucially the old environment, that is the “blue” environment, is kept idle in case a rollback is needed. AWS Elastic Beanstalk supports immutable and blue/green deployment patterns. AWS CodeDeploy also supports the blue/green pattern. For more information on how AWS services accomplish these immutable patterns, refer to the Blue/Green Deployments on AWS whitepaper. Database schema changes It’s common for modern software to have a database layer. Typically, a relational database is used, which stores both data and the structure of the data. It’s often necessary to modify the database in the continuous delivery process. Handling changes in a relational database requires special consideration, and it offers other challenges than the ones present when deploying application binaries. Usually, when you upgrade an application binary you stop the application, upgrade it, and then start it again. You don't really bother about the application state, which is handled outside of the application. When upgrading databases, you do need to consider state because a database contains much state but comparatively little logic and structure. The database schema before and after a change is applied should be considered different versions of the database. You could use tools such as Liquibase and Flyway to manage the versions. In general, those tools employ some variant of the following methods: • Add a table to the database where a database version is stored. This version has been archived. For the latest version of this document, visit: • Keep track of database change commands and bunch them together in versioned change sets. In the case of Liquibase, these changes are stored in XML files. Flyway employs a slightly different method where the change sets are handled as separate SQL files or occasionally as separate Java classes for more complex transitions. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html • When Liquibase is being asked to upgrade a database, it looks at the metadata table and determines which change sets to run in order to bring the database up-to-date with the latest version. 27 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS Summary of best practices The following are some best practice dos and don’ts for CI/CD. Do: • Treat your infrastructure as code. o Use version control for your infrastructure code. o Make use of bug tracking/ticketing systems. o Have peers review changes before applying them. o Establish infrastructure code patterns/designs. o Test infrastructure changes like code changes. • Put developers into integrated teams of no more than 12 self-sustaining members. • Have all developers commit code to the main trunk frequently, with no long running feature branches. • Consistently adopt a build system such as Maven or Gradle across your organization and standardize builds. • Have developers build unit tests toward 100% coverage of the code base. • Ensure that unit tests are 70% of the overall testing in duration, number, and scope. • Ensure that unit tests are up-to-date and not neglected. Unit test failures should be fixed, not bypassed. This version has been archived. • Treat your continuous delivery configuration as code. For the latest version of this document, visit: • Establish role-based security controls (that is, who can do what and when). o Monitor/track every resource possible. o Alert on services, availability, and response times. https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html o Capture, learn, and improve. o Share access with everyone on the team. o Plan metrics and monitoring into the lifecycle. 28 Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS • Keep and track standard metrics. o Number of builds. o Number of deployments. o Average time for changes to reach production. o Average time from first pipeline stage to each stage. o Number of changes reaching production. o Average build time. • Use multiple distinct pipelines for each branch and team. Don’t: • Have long-running branches with large complicated merges. • Have manual tests. • Have manual approval processes, gates, code reviews, and security reviews. Conclusion Continuous integration and continuous delivery provide an ideal scenario for your organization’s application teams. Your developers simply push code to a repository. This code will be integrated, tested, deployed, tested again, merged with infrastructure, go through security and quality reviews, and be ready to deploy with extremely high confidence. This version has been archived. For the latest version of this document, visit: When CI/CD is used, code quality is improved and software updates are delivered quickly and with high confidence that there will be no breaking changes. The impact of any release can be correlated with data from production and operations. It can be used for planning the next cycle, too—a vital DevOps practice in your organization’s cloud transformation. Further reading https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html For more information on the topics discussed in this whitepaper, refer to the following AWS whitepapers: • Overview of Deployment Options on AWS 29 This version has been archived. For the latest version of this document, visit: https://docs.aws.amazon.com/whitepapers/latest/ practicing-continuous-integration-continuous delivery/welcome.html Amazon Web Services Practicing Continuous Integration and Continuous Delivery on AWS 30 • Blue/Green Deployments on AWS • Setting up CI/CD pipeline by integrating Jenkins with AWS CodeBuild and AWS CodeDeploy • Implementing Microservices on AWS • Docker on AWS: Running Containers in the Cloud Contributors The following individuals and organizations contributed to this document: • Amrish Thakkar, Principal Solutions Architect, AWS • David Stacy, Senior Consultant - DevOps, AWS Professional Services • Asif Khan, Solutions Architect, AWS • Xiang Shen, Senior Solutions Architect, AWS Document revisions Date Description October 27, 2021 Updated content. June 1, 2017 First publication   
Implementing Microservices on AWS First Published December 1, 2016 Updated November 9, 2021 This version has been archived. For the latest version of this document, refer to https://docs.aws.amazon.com/whitepapers/latest/ microservices-on-aws/microservices-on-aws.pdf Archived Notices Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. © 2021 Amazon Web Services, Inc. or its affiliates. All rights reserved. Archived Archived Contents Introduction .......................................................................................................................... 5 Microservices architecture on AWS .................................................................................... 6 User interface ................................................................................................................... 6 Microservices.................................................................................................................... 7 Data store ......................................................................................................................... 9 Reducing operational complexity ...................................................................................... 10 API implementation ........................................................................................................ 11 Serverless microservices ............................................................................................... 12 Disaster recovery ........................................................................................................... 14 Deploying Lambda-based applications .......................................................................... 15 Distributed systems components ...................................................................................... 16 Service discovery ........................................................................................................... 16 Distributed data management ........................................................................................ 18 Configuration management............................................................................................ 21 Asynchronous communication and lightweight messaging .......................................... 21 Distributed monitoring .................................................................................................... 26 Chattiness ....................................................................................................................... 33 Auditing ........................................................................................................................... 34 Resources .......................................................................................................................... 37 Conclusion ......................................................................................................................... 38 Document Revisions.......................................................................................................... 39 Contributors ....................................................................................................................... 39 Abstract Microservices are an architectural and organizational approach to software development created to speed up deployment cycles, foster innovation and ownership, improve maintainability and scalability of software applications, and scale organizations delivering software and services by using an agile approach that helps teams work independently. With a microservices approach, software is composed of small services that communicate over well-defined application programming interfaces (APIs) that can be deployed independently. These services are owned by small autonomous teams. This agile approach is key to successfully scale your organization. Three common patterns have been observed when AWS customers build microservices: API driven, event driven, and data streaming. This whitepaper introduces all three approaches and summarizes the common characteristics of microservices, discusses the main challenges of building microservices, and describes how product teams can use Amazon Web Services (AWS) to overcome these challenges. Due to the rather involved nature of various topics discussed in this whitepaper, including data store, asynchronous communication, and service discovery, the reader is encouraged to consider specific requirements and use cases of their applications, in addition to the provided guidance, prior to making architectural choices. Archived Amazon Web Services Implementing Microservices on AWS Introduction Microservices architectures are not a completely new approach to software engineering, but rather a combination of various successful and proven concepts such as: • Agile software development • Service-oriented architectures • API-first design • Continuous integration/continuous delivery (CI/CD) In many cases, design patterns of the Twelve-Factor App are used for microservices. This whitepaper first describes different aspects of a highly scalable, fault-tolerant microservices architecture (user interface, microservices implementation, and data store) and how to build it on AWS using container technologies. It then recommends the AWS services for implementing a typical serverless microservices architecture to reduce operational complexity. Serverless is defined as an operational model by the following tenets: • No infrastructure to provision or manage • Automatically scaling by unit of consumption • Pay for value billing model • Built-in availability and fault tolerance Finally, this whitepaper covers the overall system and discusses the cross-service aspects of a microservices architecture, such as distributed monitoring and auditing, data consistency, and asynchronous communication. This whitepaper only focuses on workloads running in the AWS Cloud. It doesn’t cover hybrid scenarios or migration strategies. For more information about migration, refer to the Container Migration Methodology whitepaper. Archived 5 Amazon Web Services Implementing Microservices on AWS Microservices architecture on AWS Typical monolithic applications are built using different layers—a user interface (UI) layer, a business layer, and a persistence layer. A central idea of a microservices architecture is to split functionalities into cohesive verticals—not by technological layers, but by implementing a specific domain. The following figure depicts a reference architecture for a typical microservices application on AWS. Typical microservices application on AWS User interface Modern web applications often use JavaScript frameworks to implement a single-page application that communicates with a representational state transfer (REST) or RESTful Archived 6 Amazon Web Services Implementing Microservices on AWS API. Static web content can be served using Amazon Simple Storage Service (Amazon S3) and Amazon CloudFront. Because clients of a microservice are served from the closest edge location and get responses either from a cache or a proxy server with optimized connections to the origin, latencies can be significantly reduced. However, microservices running close to each other don’t benefit from a content delivery network. In some cases, this approach might actually add additional latency. A best practice is to implement other caching mechanisms to reduce chattiness and minimize latencies. For more information, refer to the Chattiness topic. Microservices APIs are the front door of microservices, which means that APIs serve as the entry point for applications logic behind a set of programmatic interfaces, typically a RESTful web services API. This API accepts and processes calls from clients, and might implement functionality such as traffic management, request filtering, routing, caching, authentication, and authorization. Microservices implementation AWS has integrated building blocks that support the development of microservices. Two popular approaches are using AWS Lambda and Docker containers with AWS Fargate. With AWS Lambda, you upload your code and let Lambda take care of everything required to run and scale the implementation to meet your actual demand curve with high availability. No administration of infrastructure is needed. Lambda supports several programming languages and can be invoked from other AWS services or be called directly from any web or mobile application. One of the biggest advantages of AWS Lambda is that you can move quickly: you can focus on your business logic because security and scaling are managed by AWS. Lambda’s opinionated approach drives the scalable platform. A common approach to reduce operational efforts for deployment is container-based deployment. Container technologies, like Docker, have increased in popularity in the last few years due to benefits like portability, productivity, and efficiency. The learning curve with containers can be steep and you have to think about security fixes for your Docker images and monitoring. Amazon Elastic Container Service (Amazon ECS) and Amazon Archived 7 Amazon Web Services Implementing Microservices on AWS Elastic Kubernetes Service (Amazon EKS) eliminate the need to install, operate, and scale your own cluster management infrastructure. With API calls, you can launch and stop Docker-enabled applications, query the complete state of your cluster, and access many familiar features like security groups, Load Balancing, Amazon Elastic Block Store (Amazon EBS) volumes, and AWS Identity and Access Management (IAM) roles. AWS Fargate is a serverless compute engine for containers that works with both Amazon ECS and Amazon EKS. With Fargate, you no longer have to worry about provisioning enough compute resources for your container applications. Fargate can launch tens of thousands of containers and easily scale to run your most mission-critical applications. Amazon ECS supports container placement strategies and constraints to customize how Amazon ECS places and ends tasks. A task placement constraint is a rule that is considered during task placement. You can associate attributes, which are essentially key-value pairs, to your container instances and then use a constraint to place tasks based on these attributes. For example, you can use constraints to place certain microservices based on instance type or instance capability, such as GPU-powered instances. Amazon EKS runs up-to-date versions of the open-source Kubernetes software, so you can use all the existing plugins and tooling from the Kubernetes community. Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, whether running in on-premises data centers or public clouds. Amazon EKS integrates IAM with Kubernetes, enabling you to register IAM entities with the native authentication system in Kubernetes. There is no need to manually set up credentials for authenticating with the Kubernetes control plane. The IAM integration enables you to use IAM to directly authenticate with the control plane itself and provide fine granular access to the public endpoint of your Kubernetes control plane. Docker images used in Amazon ECS and Amazon EKS can be stored in Amazon Elastic Container Registry (Amazon ECR). Amazon ECR eliminates the need to operate and scale the infrastructure required to power your container registry. Continuous integration and continuous delivery (CI/CD) are best practices and a vital part of a DevOps initiative that enables rapid software changes while maintaining system stability and security. However, this is out of scope for this whitepaper. For more Archived 8 Amazon Web Services Implementing Microservices on AWS information, refer to the Practicing Continuous Integration and Continuous Delivery on AWS whitepaper. Private links AWS PrivateLink is a highly available, scalable technology that enables you to privately connect your virtual private cloud (VPC) to supported AWS services, services hosted by other AWS accounts (VPC endpoint services), and supported AWS Marketplace partner services. You do not require an internet gateway, network address translation device, public IP address, AWS Direct Connect connection, or VPN connection to communicate with the service. Traffic between your VPC and the service does not leave the Amazon network. Private links are a great way to increase the isolation and security of microservices architecture. A microservice, for example, could be deployed in a totally separate VPC, fronted by a load balancer, and exposed to other microservices through an AWS PrivateLink endpoint. With this setup, using AWS PrivateLink, the network traffic to and from the microservice never traverses the public internet. One use case for such isolation includes regulatory compliance for services handling sensitive data such as PCI, HIPPA and EU/US Privacy Shield. Additionally, AWS PrivateLink allows connecting microservices across different accounts and Amazon VPCs, with no need for firewall rules, path definitions, or route tables; simplifying network management. Utilizing PrivateLink, software as a service (SaaS) providers, and ISVs can offer their microservices-based solutions with complete operational isolation and secure access, as well. Data store The data store is used to persist data needed by the microservices. Popular stores for session data are in-memory caches such as Memcached or Redis. AWS offers both technologies as part of the managed Amazon ElastiCache service. Putting a cache between application servers and a database is a common mechanism for reducing the read load on the database, which, in turn, may enable resources to be used to support more writes. Caches can also improve latency. Relational databases are still very popular to store structured data and business objects. AWS offers six database engines (Microsoft SQL Server, Oracle, MySQL, Archived 9 Amazon Web Services Implementing Microservices on AWS MariaDB, PostgreSQL, and Amazon Aurora) as managed services through Amazon Relational Database Service (Amazon RDS). Relational databases, however, are not designed for endless scale, which can make it difficult and time intensive to apply techniques to support a high number of queries. NoSQL databases have been designed to favor scalability, performance, and availability over the consistency of relational databases. One important element of NoSQL databases is that they typically don’t enforce a strict schema. Data is distributed over partitions that can be scaled horizontally and is retrieved using partition keys. Because individual microservices are designed to do one thing well, they typically have a simplified data model that might be well suited to NoSQL persistence. It is important to understand that NoSQL databases have different access patterns than relational databases. For example, it is not possible to join tables. If this is necessary, the logic has to be implemented in the application. You can use Amazon DynamoDB to create a database table that can store and retrieve any amount of data and serve any level of request traffic. DynamoDB delivers single-digit millisecond performance, however, there are certain use cases that require response times in microseconds. Amazon DynamoDB Accelerator (DAX) provides caching capabilities for accessing data. DynamoDB also offers an automatic scaling feature to dynamically adjust throughput capacity in response to actual traffic. However, there are cases where capacity planning is difficult or not possible because of large activity spikes of short duration in your application. For such situations, DynamoDB provides an on-demand option, which offers simple pay-per-request pricing. DynamoDB on-demand is capable of serving thousands of requests per second instantly without capacity planning. Reducing operational complexity The architecture previously described in this whitepaper is already using managed services, but Amazon Elastic Compute Cloud (Amazon EC2) instances still need to be managed. The operational efforts needed to run, maintain, and monitor microservices can be further reduced by using a fully serverless architecture. Archived 10 Amazon Web Services Implementing Microservices on AWS API implementation Architecting, deploying, monitoring, continuously improving, and maintaining an API can be a time-consuming task. Sometimes different versions of APIs need to be run to assure backward compatibility for all clients. The different stages of the development cycle (for example, development, testing, and production) further multiply operational efforts. Authorization is a critical feature for all APIs, but it is usually complex to build and involves repetitive work. When an API is published and becomes successful, the next challenge is to manage, monitor, and monetize the ecosystem of third-party developers utilizing the APIs. Other important features and challenges include throttling requests to protect the backend services, caching API responses, handling request and response transformation, and generating API definitions and documentation with tools such as Swagger. Amazon API Gateway addresses those challenges and reduces the operational complexity of creating and maintaining RESTful APIs. API Gateway allows you to create your APIs programmatically by importing Swagger definitions, using either the AWS API or the AWS Management Console. API Gateway serves as a front door to any web application running on Amazon EC2, Amazon ECS, AWS Lambda, or in any on premises environment. Basically, API Gateway allows you to run APIs without having to manage servers. The following figure illustrates how API Gateway handles API calls and interacts with other components. Requests from mobile devices, websites, or other backend services are routed to the closest CloudFront Point of Presence to minimize latency and provide optimum user experience. Archived 11 Amazon Web Services Implementing Microservices on AWS API Gateway call flow Serverless microservices “No server is easier to manage than no server.” — AWS re:Invent Getting rid of servers is a great way to eliminate operational complexity. Lambda is tightly integrated with API Gateway. The ability to make synchronous calls from API Gateway to Lambda enables the creation of fully serverless applications and is described in detail in the Amazon API Gateway Developer Guide. The following figure shows the architecture of a serverless microservice with AWS Lambda where the complete service is built out of managed services, which eliminates the architectural burden to design for scale and high availability, and eliminates the operational efforts of running and monitoring the microservice’s underlying infrastructure. Archived 12 Amazon Web Services Implementing Microservices on AWS Serverless microservice using AWS Lambda A similar implementation that is also based on serverless services is shown in the following figure. In this architecture, Docker containers are used with Fargate, so it’s not necessary to care about the underlying infrastructure. In addition to DynamoDB, Amazon Aurora Serverless is used, which is an on-demand, auto-scaling configuration for Aurora (MySQL-compatible edition), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. Archived 13 Amazon Web Services Implementing Microservices on AWS Serverless microservice using Fargate Disaster recovery As previously mentioned in the introduction of this whitepaper, typical microservices applications are implemented using the Twelve-Factor Application patterns. The Processes section states that “Twelve-factor processes are stateless and share nothing. Any data that needs to persist must be stored in a stateful backing service, typically a database.” For a typical microservices architecture, this means that the main focus for disaster recovery should be on the downstream services that maintain the state of the application. For example, these can be file systems, databases, or queues, for example. When creating a disaster recovery strategy, organizations most commonly plan for the recovery time objective and recovery point objective. Recovery time objective is the maximum acceptable delay between the interruption of service and restoration of service. This objective determines what is considered an acceptable time window when service is unavailable and is defined by the organization. Archived 14 Amazon Web Services Implementing Microservices on AWS Recovery point objective is the maximum acceptable amount of time since the last data recovery point. This objective determines what is considered an acceptable loss of data between the last recovery point and the interruption of service and is defined by the organization. For more information, refer to the Disaster Recovery of Workloads on AWS: Recovery in the Cloud whitepaper. High availability This section takes a closer look at high availability for different compute options. Amazon EKS runs Kubernetes control and data plane instances across multiple Availability Zones to ensure high availability. Amazon EKS automatically detects and replaces unhealthy control plane instances, and it provides automated version upgrades and patching for them. This control plane consists of at least two API server nodes and three etcd nodes that run across three Availability Zones within a region. Amazon EKS uses the architecture of AWS Regions to maintain high availability. Amazon ECR hosts images in a highly available and high-performance architecture, enabling you to reliably deploy images for container applications across Availability Zones. Amazon ECR works with Amazon EKS, Amazon ECS, and AWS Lambda, simplifying development to production workflow. Amazon ECS is a regional service that simplifies running containers in a highly available manner across multiple Availability Zones within an AWS Region. Amazon ECS includes multiple scheduling strategies that place containers across your clusters based on your resource needs (for example, CPU or RAM) and availability requirements. AWS Lambda runs your function in multiple Availability Zones to ensure that it is available to process events in case of a service interruption in a single zone. If you configure your function to connect to a virtual private cloud (VPC) in your account, specify subnets in multiple Availability Zones to ensure high availability. Deploying Lambda-based applications You can use AWS CloudFormation to define, deploy, and configure serverless applications. Archived 15 Amazon Web Services Implementing Microservices on AWS The AWS Serverless Application Model (AWS SAM) is a convenient way to define serverless applications. AWS SAM is natively supported by CloudFormation and defines a simplified syntax for expressing serverless resources. To deploy your application, specify the resources you need as part of your application, along with their associated permissions policies in a CloudFormation template, package your deployment artifacts, and deploy the template. Based on AWS SAM, SAM Local is an AWS Command Line Interface tool that provides an environment for you to develop, test, and analyze your serverless applications locally before uploading them to the Lambda runtime. You can use SAM Local to create a local testing environment that simulates the AWS runtime environment. Distributed systems components After looking at how AWS can solve challenges related to individual microservices, the focus moves to on cross-service challenges, such as service discovery, data consistency, asynchronous communication, and distributed monitoring and auditing. Service discovery One of the primary challenges with microservice architectures is enabling services to discover and interact with each other. The distributed characteristics of microservice architectures not only make it harder for services to communicate, but also presents other challenges, such as checking the health of those systems and announcing when new applications become available. You also must decide how and where to store meta information, such as configuration data, that can be used by applications. In this section several techniques for performing service discovery on AWS for microservices-based architectures are explored. DNS-based service discovery Amazon ECS now includes integrated service discovery that enables your containerized services to discover and connect with each other. Previously, to ensure that services were able to discover and connect with each other, you had to configure and run your own service discovery system based on Amazon Route 53, AWS Lambda, and ECS event streams, or connect every service to a load balancer. Archived 16 Amazon Web Services Implementing Microservices on AWS Amazon ECS creates and manages a registry of service names using the Route 53 Auto Naming API. Names are automatically mapped to a set of DNS records so that you can refer to a service by name in your code and write DNS queries to have the name resolve to the service’s endpoint at runtime. You can specify health check conditions in a service's task definition and Amazon ECS ensures that only healthy service endpoints are returned by a service lookup. In addition, you can also use unified service discovery for services managed by Kubernetes. To enable this integration, AWS contributed to the External DNS project, a Kubernetes incubator project. Another option is to use the capabilities of AWS Cloud Map. AWS Cloud Map extends the capabilities of the Auto Naming APIs by providing a service registry for resources, such as Internet Protocols (IPs), Uniform Resource Locators (URLs), and Amazon Resource Names (ARNs), and offering an API-based service discovery mechanism with a faster change propagation and the ability to use attributes to narrow down the set of discovered resources. Existing Route 53 Auto Naming resources are upgraded automatically to AWS Cloud Map. Third-party software A different approach to implementing service discovery is using third-party software such as HashiCorp Consul, etcd, or Netflix Eureka. All three examples are distributed, reliable key-value stores. For HashiCorp Consul, there is an AWS Quick Start that sets up a flexible, scalable AWS Cloud environment and launches HashiCorp Consul automatically into a configuration of your choice. Service meshes In an advanced microservices architecture, the actual application can be composed of hundreds, or even thousands, of services. Often the most complex part of the application is not the actual services themselves, but the communication between those services. Service meshes are an additional layer for handling interservice communication, which is responsible for monitoring and controlling traffic in microservices architectures. This enables tasks, like service discovery, to be completely handled by this layer. Typically, a service mesh is split into a data plane and a control plane. The data plane consists of a set of intelligent proxies that are deployed with the application code as a Archived 17 Amazon Web Services Implementing Microservices on AWS special sidecar proxy that intercepts all network communication between microservices. The control plane is responsible for communicating with the proxies. Service meshes are transparent, which means that application developers don’t have to be aware of this additional layer and don’t have to make changes to existing application code. AWS App Mesh is a service mesh that provides application-level networking to enable your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you complete visibility and ensuring high availability for your applications. You can use App Mesh with existing or new microservices running on Amazon EC2, Fargate, Amazon ECS, Amazon EKS, and self-managed Kubernetes on AWS. App Mesh can monitor and control communications for microservices running across clusters, orchestration systems, or VPCs as a single application without any code changes. Distributed data management Monolithic applications are typically backed by a large relational database, which defines a single data model common to all application components. In a microservices approach, such a central database would prevent the goal of building decentralized and independent components. Each microservice component should have its own data persistence layer. Distributed data management, however, raises new challenges. As a consequence of the CAP theorem, distributed microservice architectures inherently trade off consistency for performance and need to embrace eventual consistency. In a distributed system, business transactions can span multiple microservices. Because they cannot use a single ACID transaction, you can end up with partial executions. In this case, we would need some control logic to redo the already processed transactions. For this purpose, the distributed Saga pattern is commonly used. In the case of a failed business transaction, Saga orchestrates a series of compensating transactions that undo the changes that were made by the preceding transactions. AWS Step Functions make it easy to implement a Saga execution coordinator as shown in the following figure. Archived 18 Amazon Web Services Implementing Microservices on AWS Saga execution coordinator Building a centralized store of critical reference data that is curated by core data management tools and procedures provides a means for microservices to synchronize their critical data and possibly roll back state. Using AWS Lambda with scheduled Amazon CloudWatch Events you can build a simple cleanup and deduplication mechanism. It’s very common for state changes to affect more than a single microservice. In such cases, event sourcing has proven to be a useful pattern. The core idea behind event sourcing is to represent and persist every application change as an event record. Instead of persisting application state, data is stored as a stream of events. Database transaction logging and version control systems are two well-known examples for event sourcing. Event sourcing has a couple of benefits: state can be determined and reconstructed for any point in time. It naturally produces a persistent audit trail and also facilitates debugging. In the context of microservices architectures, event sourcing enables decoupling different parts of an application by using a publish and subscribe pattern, and it feeds the same event data into different data models for separate microservices. Event sourcing is frequently used in conjunction with the Command Query Responsibility Segregation (CQRS) pattern to decouple read from write workloads and optimize both for performance, scalability, and security. In traditional data management systems, commands and queries are run against the same data repository. The following figure shows how the event sourcing pattern can be implemented on AWS. Amazon Kinesis Data Streams serves as the main component of the central event store, which captures application changes as events and persists them on Archived 19 Amazon Web Services Implementing Microservices on AWS Amazon S3. The figure depicts three different microservices, composed of API Gateway, AWS Lambda, and DynamoDB. The arrows indicate the flow of the events: when Microservice 1 experiences an event state change, it publishes an event by writing a message into Kinesis Data Streams. All microservices run their own Kinesis Data Streams application in AWS Lambda which reads a copy of the message, filters it based on relevancy for the microservice, and possibly forwards it for further processing. If your function returns an error, Lambda retries the batch until processing succeeds or the data expires. To avoid stalled shards, you can configure the event source mapping to retry with a smaller batch size, limit the number of retries, or discard records that are too old. To retain discarded events, you can configure the event source mapping to send details about failed batches to an Amazon Simple Queue Service (SQS) queue or Amazon Simple Notification Service (SNS) topic. Event sourcing pattern on AWS Amazon S3 durably stores all events across all microservices and is the single source of truth when it comes to debugging, recovering application state, or auditing application changes. There are two primary reasons why records may be delivered more than one time to your Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times. Archived 20 Amazon Web Services Implementing Microservices on AWS Configuration management In a typical microservices architecture with dozens of different services, each service needs access to several downstream services and infrastructure components that expose data to the service. Examples could be message queues, databases, and other microservices. One of the key challenges is to configure each service in a consistent way to provide information about the connection to downstream services and infrastructure. In addition, the configuration should also contain information about the environment in which the service is operating, and restarting the application to use new configuration data shouldn’t be necessary. The third principle of the Twelve-Factor App patterns covers this topic: “The twelve factor app stores config in environment variables (often shortened to env vars or env).” For Amazon ECS, environment variables can be passed to the container by using the environment container definition parameter which maps to the --env option to docker run. Environment variables can be passed to your containers in bulk by using the environmentFiles container definition parameter to list one or more files containing the environment variables. The file must be hosted in Amazon S3. In AWS Lambda, the runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For Amazon EKS, you can define environment variables in the env-field of the configuration manifest of the corresponding pod. A different way to use env-variables is to use a ConfigMap. Asynchronous communication and lightweight messaging Communication in traditional, monolithic applications is straightforward—one part of the application uses method calls or an internal event distribution mechanism to communicate with the other parts. If the same application is implemented using decoupled microservices, the communication between different parts of the application must be implemented using network communication. REST-based communication The HTTP/S protocol is the most popular way to implement synchronous communication between microservices. In most cases, RESTful APIs use HTTP as a Archived 21 Amazon Web Services Implementing Microservices on AWS transport layer. The REST architectural style relies on stateless communication, uniform interfaces, and standard methods. With API Gateway, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your backend services. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud. An API object defined with the API Gateway service is a group of resources and methods. A resource is a typed object within the domain of an API and may have associated a data model or relationships to other resources. Each resource can be configured to respond to one or more methods, that is, standard HTTP verbs such as GET, POST, or PUT. REST APIs can be deployed to different stages, and versioned as well as cloned to new versions. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Asynchronous messaging and event passing Message passing is an additional pattern used to implement communication between microservices. Services communicate by exchanging messages by a queue. One major benefit of this communication style is that it’s not necessary to have a service discovery and services are loosely coupled. Synchronous systems are tightly coupled, which means a problem in a synchronous downstream dependency has immediate impact on the upstream callers. Retries from upstream callers can quickly fan-out and amplify problems. Depending on specific requirements, like protocols, AWS offers different services which help to implement this pattern. One possible implementation uses a combination of Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS). Both services work closely together. Amazon SNS enables applications to send messages to multiple subscribers through a push mechanism. By using Amazon SNS and Amazon SQS together, one message can be delivered to multiple consumers. The following figure demonstrates the integration of Amazon SNS and Amazon SQS. Archived 22 Amazon Web Services Implementing Microservices on AWS Message bus pattern on AWS When you subscribe an SQS queue to an SNS topic, you can publish a message to the topic, and Amazon SNS sends a message to the subscribed SQS queue. The message contains subject and message published to the topic along with metadata information in JSON format. Another option for building event-driven architectures with event sources spanning internal applications, third-party SaaS applications, and AWS services, at scale, is Amazon EventBridge. A fully managed event bus service, EventBridge receives events from disparate sources, identifies a target based on a routing rule, and delivers near real-time data to that target, including AWS Lambda, Amazon SNS, and Amazon Kinesis Streams, among others. An inbound event can also be customized, by input transformer, prior to delivery. To develop event-driven applications significantly faster, EventBridge schema registries collect and organize schemas, including schemas for all events generated by AWS services. Customers can also define custom schemas or use an infer schema option to discover schemas automatically. In balance, however, a potential trade-off for all these features is a relatively higher latency value for EventBridge delivery. Also, the default throughput and quotas for EventBridge may require an increase, through a support request, based on use case. A different implementation strategy is based on Amazon MQ, which can be used if existing software is using open standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket. Amazon SQS exposes a custom Archived 23 Amazon Web Services Implementing Microservices on AWS API which means, if you have an existing application that you want to migrate from—for example, an on-premises environment to AWS—code changes are necessary. With Amazon MQ this is not necessary in many cases. Amazon MQ manages the administration and maintenance of ActiveMQ, a popular open-source message broker. The underlying infrastructure is automatically provisioned for high availability and message durability to support the reliability of your applications. Orchestration and state management The distributed character of microservices makes it challenging to orchestrate workflows when multiple microservices are involved. Developers might be tempted to add orchestration code into their services directly. This should be avoided because it introduces tighter coupling and makes it harder to quickly replace individual services. You can use AWS Step Functions to build applications from individual components that each perform a discrete function. Step Functions provides a state machine that hides the complexities of service orchestration, such as error handling, serialization, and parallelization. This lets you scale and change applications quickly while avoiding additional coordination code inside services. Step Functions is a reliable way to coordinate components and step through the functions of your application. Step Functions provides a graphical console to arrange and visualize the components of your application as a series of steps. This makes it easier to build and run distributed services. Step Functions automatically starts and tracks each step and retries when there are errors, so your application executes in order and as expected. Step Functions logs the state of each step so when something goes wrong, you can diagnose and debug problems quickly. You can change and add steps without even writing code to evolve your application and innovate faster. Step Functions is part of the AWS serverless platform and supports orchestration of Lambda functions as well as applications based on compute resources, such as Amazon EC2, Amazon EKS, and Amazon ECS, and additional services like Amazon SageMaker and AWS Glue. Step Functions manages the operations and underlying infrastructure for you to help ensure that your application is available at any scale. Archived 24 Amazon Web Services Implementing Microservices on AWS To build workflows, Step Functions uses the Amazon States Language. Workflows can contain sequential or parallel steps as well as branching steps. The following figure shows an example workflow for a microservices architecture combining sequential and parallel steps. Invoking such a workflow can be done either through the Step Functions API or with API Gateway. An example of a microservices workflow invoked by Step Functions Archived 25 Amazon Web Services Implementing Microservices on AWS Distributed monitoring A microservices architecture consists of many different distributed parts that have to be monitored. You can use Amazon CloudWatch to collect and track metrics, centralize and monitor log files, set alarms, and automatically react to changes in your AWS environment. CloudWatch can monitor AWS resources such as Amazon EC2 instances, DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. Monitoring You can use CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. CloudWatch provides a reliable, scalable, and flexible monitoring solution that you can start using within minutes. You no longer need to set up, manage, and scale your own monitoring systems and infrastructure. In a microservices architecture, the capability of monitoring custom metrics using CloudWatch is an additional benefit because developers can decide which metrics should be collected for each service. In addition, dynamic scaling can be implemented based on custom metrics. In addition to Amazon Cloudwatch, you can also use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. CloudWatch Container Insights automatically collects metrics for many resources, such as CPU, memory, disk, and network and aggregate as CloudWatch metrics at the cluster, node, pod, task, and service level. Using CloudWatch Container Insights, you can gain access to CloudWatch Container Insights dashboard metrics. It also provides diagnostic information, such as container restart failures, to help you isolate issues and resolve them quickly. You can also set CloudWatch alarms on metrics that Container Insights collects. Container Insights is available for Amazon ECS, Amazon EKS, and Kubernetes platforms on Amazon EC2. Amazon ECS support includes support for Fargate. Another popular option, especially for Amazon EKS, is to use Prometheus. Prometheus is an open-source monitoring and alerting toolkit that is often used in combination with Grafana to visualize the collected metrics. Many Kubernetes components store metrics at /metrics and Prometheus can scrape these metrics at a regular interval. Archived 26 Amazon Web Services Implementing Microservices on AWS Amazon Managed Service for Prometheus (AMP) is a Prometheus-compatible monitoring service that enables you to monitor containerized applications at scale. With AMP, you can use the open-source Prometheus query language (PromQL) to monitor the performance of containerized workloads without having to manage the underlying infrastructure required to manage the ingestion, storage, and querying of operational metrics. You can collect Prometheus metrics from Amazon EKS and Amazon ECS environments, using AWS Distro for OpenTelemetry or Prometheus servers as collection agents. AMP is often used in combination with Amazon Managed Service for Grafana (AMG). AMG makes it easy to query, visualize, alert on and understand your metrics no matter where they are stored. With AMG, you can analyze your metrics, logs, and traces without having to provision servers, configure and update software, or do the heavy lifting involved in securing and scaling Grafana in production. Centralizing logs Consistent logging is critical for troubleshooting and identifying issues. Microservices enable teams to ship many more releases than ever before and encourage engineering teams to run experiments on new features in production. Understanding customer impact is crucial to gradually improving an application. By default, most AWS services centralize their log files. The primary destinations for log files on AWS are Amazon S3 and Amazon CloudWatch Logs. For applications running on Amazon EC2 instances, a daemon is available to send log files to CloudWatch Logs. Lambda functions natively send their log output to CloudWatch Logs and Amazon ECS includes support for the awslogs log driver that enables the centralization of container logs to CloudWatch Logs. For Amazon EKS, either Fluent Bit or Fluentd can forward logs from the individual instances in the cluster to a centralized logging CloudWatch Logs where they are combined for higher-level reporting using Amazon OpenSearch Service and Kibana. Because of its smaller footprint and performance advantages, Fluent Bit is recommended instead of Fluentd. The following figure illustrates the logging capabilities of some of the services. Teams are then able to search and analyze these logs using tools like Amazon OpenSearch Service and Kibana. Amazon Athena can be used to run a one-time query against centralized log files in Amazon S3. Archived 27 Amazon Web Services Implementing Microservices on AWS Logging capabilities of AWS services Distributed tracing In many cases, a set of microservices works together to handle a request. Imagine a complex system consisting of tens of microservices in which an error occurs in one of the services in the call chain. Even if every microservice is logging properly and logs are consolidated in a central system, it can be difficult to find all relevant log messages. The central idea of AWS X-Ray is the use of correlation IDs, which are unique identifiers attached to all requests and messages related to a specific event chain. The trace ID is added to HTTP requests in specific tracing headers named X-Amzn-Trace-Id when the request hits the first X-Ray integrated service (for example, Application Load Balancer or API Gateway) and included in the response. Through the X-Ray SDK, any microservice can read, but can also add or update this header. X-Ray works with Amazon EC2, Amazon ECS, AWS Lambda, and AWS Elastic Beanstalk. You can use X-Ray with applications written in Java, Node.js, and .NET that are deployed on these services. Archived 28 Amazon Web Services Implementing Microservices on AWS X-Ray service map Epsagon is fully managed SaaS that includes tracing for all AWS services, third-party APIs (through HTTP calls), and other common services such as Redis, Kafka, and Elastic. The Epsagon service includes monitoring capabilities, alerting to the most common services, and payload visibility into each and every call your code is making. AWS Distro for OpenTelemetry is a secure, production-ready, AWS-supported distribution of the OpenTelemetry project. Part of the Cloud Native Computing Foundation, AWS Distro for OpenTelemetry provides open-source APIs, libraries, and agents to collect distributed traces and metrics for application monitoring. With AWS Distro for OpenTelemetry, you can instrument your applications just one time to send correlated metrics and traces to multiple AWS and partner monitoring solutions. Use auto-instrumentation agents to collect traces without changing your code. AWS Distro for OpenTelemetry also collects metadata from your AWS resources and managed services to correlate application performance data with underlying infrastructure data, reducing the mean time to problem resolution. Use AWS Distro for OpenTelemetry to instrument your applications running on Amazon EC2, Amazon ECS, Amazon EKS on Amazon EC2, Fargate, and AWS Lambda, as well as on-premises. Archived 29 Amazon Web Services Implementing Microservices on AWS Options for log analysis on AWS Searching, analyzing, and visualizing log data is an important aspect of understanding distributed systems. Amazon CloudWatch Logs Insights enables you to explore, analyze, and visualize your logs instantly. This allows you to troubleshoot operational problems. Another option for analyzing log files is to use Amazon OpenSearch Service together with Kibana. Amazon OpenSearch Service can be used for full-text search, structured search, analytics, and all three in combination. Kibana is an open-source data visualization plugin that seamlessly integrates with the Amazon OpenSearch Service. The following figure demonstrates log analysis with Amazon OpenSearch Service and Kibana. CloudWatch Logs can be configured to stream log entries to Amazon OpenSearch Service in near real time through a CloudWatch Logs subscription. Kibana visualizes the data and exposes a convenient search interface to data stores in Amazon OpenSearch Service. This solution can be used in combination with software like ElastAlert to implement an alerting system to send SNS notifications and emails, create JIRA tickets, and so forth, if anomalies, spikes, or other patterns of interest are detected in the data. Archived 30 Amazon Web Services Implementing Microservices on AWS Log analysis with Amazon OpenSearch Service and Kibana Another option for analyzing log files is to use Amazon Redshift with Amazon QuickSight. QuickSight can be easily connected to AWS data services, including Redshift, Amazon RDS, Aurora, Amazon EMR, DynamoDB, Amazon S3, and Amazon Kinesis. CloudWatch Logs can act as a centralized store for log data, and, in addition to only storing the data, it is possible to stream log entries to Amazon Kinesis Data Firehose. The following figure depicts a scenario where log entries are streamed from different sources to Redshift using CloudWatch Logs and Kinesis Data Firehose. QuickSight uses the data stored in Redshift for analysis, reporting, and visualization. Archived 31 Amazon Web Services Implementing Microservices on AWS Log analysis with Amazon Redshift and Amazon QuickSight The following figure depicts a scenario of log analysis on Amazon S3. When the logs are stored in Amazon S3 buckets, the log data can be loaded in different AWS data services, such as Redshift or Amazon EMR, to analyze the data stored in the log stream and find anomalies. Archived 32 Amazon Web Services Implementing Microservices on AWS Log analysis on Amazon S3 Chattiness By breaking monolithic applications into small microservices, the communication overhead increases because microservices have to talk to each other. In many implementations, REST over HTTP is used because it is a lightweight communication protocol, but high message volumes can cause issues. In some cases, you might consider consolidating services that send many messages back and forth. If you find yourself in a situation where you consolidate an increased number of services just to reduce chattiness, you should review your problem domains and your domain model. Protocols Earlier in this whitepaper, in the section Asynchronous communication and lightweight messaging, different possible protocols are discussed. For microservices it is common to use protocols like HTTP. Messages exchanged by services can be encoded in different ways, such as human-readable formats like JSON or YAML, or efficient binary formats such as Avro or Protocol Buffers. Archived 33 Amazon Web Services Implementing Microservices on AWS Caching Caches are a great way to reduce latency and chattiness of microservices architectures. Several caching layers are possible, depending on the actual use case and bottlenecks. Many microservice applications running on AWS use ElastiCache to reduce the volume of calls to other microservices by caching results locally. API Gateway provides a built in caching layer to reduce the load on the backend servers. In addition, caching is also useful to reduce load from the data persistence layer. The challenge for any caching mechanism is to find the right balance between a good cache hit rate, and the timeliness and consistency of data. Auditing Another challenge to address in microservices architectures, which can potentially have hundreds of distributed services, is ensuring visibility of user actions on each service and being able to get a good overall view across all services at an organizational level. To help enforce security policies, it is important to audit both resource access and activities that lead to system changes. Changes must be tracked at the individual service level as well as across services running on the wider system. Typically, changes occur frequently in microservices architectures, which makes auditing changes even more important. This section examines the key services and features within AWS that can help you audit your microservices architecture. Audit trail AWS CloudTrail is a useful tool for tracking changes in microservices because it enables all API calls made in the AWS Cloud to be logged and sent to either CloudWatch Logs in real time, or to Amazon S3 within several minutes. All user and automated system actions become searchable and can be analyzed for unexpected behavior, company policy violations, or debugging. Information recorded includes a timestamp, user and account information, the service that was called, the service action that was requested, the IP address of the caller, as well as request parameters and response elements. CloudTrail allows the definition of multiple trails for the same account, which enables different stakeholders, such as security administrators, software developers, or IT Archived 34 Amazon Web Services Implementing Microservices on AWS auditors, to create and manage their own trail. If microservice teams have different AWS accounts, it is possible to aggregate trails into a single S3 bucket. The advantages of storing the audit trails in CloudWatch are that audit trail data is captured in real time, and it is easy to reroute information to Amazon OpenSearch Service for search and visualization. You can configure CloudTrail to log in to both Amazon S3 and CloudWatch Logs. Events and real-time actions Certain changes in systems architectures must be responded to quickly and either action taken to remediate the situation, or specific governance procedures to authorize the change must be initiated. The integration of Amazon CloudWatch Events with CloudTrail allows it to generate events for all mutating API calls across all AWS services. It is also possible to define custom events or generate events based on a fixed schedule. When an event is fired and matches a defined rule, a pre-defined group of people in your organization can be immediately notified, so that they can take the appropriate action. If the required action can be automated, the rule can automatically trigger a built in workflow or invoke a Lambda function to resolve the issue. The following figure shows an environment where CloudTrail and CloudWatch Events work together to address auditing and remediation requirements within a microservices architecture. All microservices are being tracked by CloudTrail and the audit trail is stored in an Amazon S3 bucket. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information. CloudWatch Events sit on top of CloudTrail and triggers alerts when a specific change is made to your architecture. Archived 35 Amazon Web Services Implementing Microservices on AWS Auditing and remediation Resource inventory and change management To maintain control over fast-changing infrastructure configurations in an agile development environment, having a more automated, managed approach to auditing and controlling your architecture is essential. Although CloudTrail and CloudWatch Events are important building blocks to track and respond to infrastructure changes across microservices, AWS Config rules enable a company to define security policies with specific rules to automatically detect, track, and alert you to policy violations. The next example demonstrates how it is possible to detect, inform, and automatically react to non-compliant configuration changes within your microservices architecture. A member of the development team has made a change to the API Gateway for a microservice to allow the endpoint to accept inbound HTTP traffic, rather than only allowing HTTPS requests. Because this situation has been previously identified as a security compliance concern by the organization, an AWS Config rule is already monitoring for this condition. Archived 36 Amazon Web Services Implementing Microservices on AWS The rule identifies the change as a security violation, and performs two actions: it creates a log of the detected change in an Amazon S3 bucket for auditing, and it creates an SNS notification. Amazon SNS is used for two purposes in our scenario: to send an email to a specified group to inform about the security violation, and to add a message to an SQS queue. Next, the message is picked up, and the compliant state is restored by changing the API Gateway configuration. Detecting security violations with AWS Config Resources • AWS Architecture Center • AWS Whitepapers • AWS Architecture Monthly • AWS Architecture Blog • This Is My Architecture videos • AWS Answers • AWS Documentation Archived 37 Amazon Web Services Implementing Microservices on AWS Conclusion Microservices architecture is a distributed design approach intended to overcome the limitations of traditional monolithic architectures. Microservices help to scale applications and organizations while improving cycle times. However, they also come with a couple of challenges that might add additional architectural complexity and operational burden. AWS offers a large portfolio of managed services that can help product teams build microservices architectures and minimize architectural and operational complexity. This whitepaper guided you through the relevant AWS services and how to implement typical patterns, such as service discovery or event sourcing, natively with AWS services. Archived 38 Amazon Web Services Implementing Microservices on AWS Document Revisions Date Description November 9, 2021 Integration of Amazon EventBridge, AWS OpenTelemetry, AMP, AMG, Container Insights, minor text changes. August 1, 2019 Minor text changes. June 1, 2019 Integration of Amazon EKS, AWS Fargate, Amazon MQ, AWS PrivateLink, AWS App Mesh, AWS Cloud Map September 1, 2017 Integration of AWS Step Functions, AWS X-Ray, and ECS event streams. December 1, 2016 First publication Contributors The following individuals contributed to this document: • Sascha Möllering, Solutions Architecture, AWS • Christian Müller, Solutions Architecture, AWS • Matthias Jung, Solutions Architecture, AWS • Peter Dalbhanjan, Solutions Architecture, AWS • Peter Chapman, Solutions Architecture, AWS • Christoph Kassen, Solutions Architecture, AWS Archived 39 Amazon Web Services Implementing Microservices on AWS • Umair Ishaq, Solutions Architecture, AWS • Rajiv Kumar, Solutions Architecture, AWS Archived 40 
This asset has been archived. Architecting for the Cloud AWS Best Practices October 2018 For best practices information, see the AWS Well-Architected Framework. Archived Notices This document is provided for informational purposes only. It represents AWS’s current product offerings and practices as of the date of issue of this document, which are subject to change without notice. Customers are responsible for making their own independent assessment of the information in this document and any use of AWS’s products or services, each of which is provided “as is” without warranty of any kind, whether express or implied. This document does not create any warranties, representations, contractual commitments, conditions or assurances from AWS, its affiliates, suppliers or licensors. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. © 2018, Amazon Web Services, Inc. or its affiliates. All rights reserved. Archived Contents Introduction Differences Between Traditional and Cloud Computing Environments IT Assets as Provisioned Resources Global, Available, and Scalable Capacity Higher-Level Managed Services Built-in Security Architecting for Cost Operations on AWS Design Principles Scalability Disposable Resources Instead of Fixed Servers Automation Loose Coupling Services, Not Servers Databases Managing Increasing Volumes of Data Removing Single Points of Failure Optimize for Cost Caching Security Conclusion Contributors 1 2 2 2 3 3 3 4 5 5 9 12 14 18 20 27 27 33 Archived 36 37 41 41 Further Reading 41 Document Revisions 42 Archived Abstract This whitepaper is intended for solutions architects and developers who are building solutions that will be deployed on Amazon Web Services (AWS). It provides architectural guidance and advice on technical design patterns and how they are applied in the context of cloud computing. This introduction provides the key concepts and differences when designing solutions on AWS. It includes a discussion on how to take advantage of attributes that are specific to the dynamic nature of cloud computing, such as elasticity and infrastructure automation. These patterns can provide the context for a more detailed review of choices, operational status, and implementation status as detailed in the AWS Well-Architected Framework.1 Archived Amazon Web Services – Architecting for the Cloud: AWS Best Practices Introduction Migrating applications to AWS, even without significant changes (an approach known as lift and shift), provides organizations with the benefits of a secure and cost-efficient infrastructure. However, to make the most of the elasticity and agility that are possible with cloud computing, engineers have to evolve their architectures to take advantage of AWS capabilities. For new applications, cloud-specific IT architecture patterns can help drive efficiency and scalability. Those new architectures can support anything from real-time analytics of internet-scale data to applications with unpredictable traffic from thousands of connected Internet of Things (IoT) or mobile devices. Whether you are rearchitecting the applications that currently run in your on-premises environment to run on AWS, or designing cloud-native applications, you must consider the differences between traditional environments and cloud computing environments. This includes architecture choices, scalability, resource types, automation, as well as flexible components, services, and databases. If you are new to AWS, we recommend that you review the information on the About AWS page to get a basic understanding of AWS services.2 Archived Page 1 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Differences Between Traditional and Cloud Computing Environments Cloud computing differs from a traditional, on-premises environment in many ways, including flexible, global, and scalable capacity, managed services, built-in security, options for cost optimization, and various operating models. IT Assets as Provisioned Resources In a traditional computing environment, you provision capacity based on an estimate of a theoretical maximum peak. This can result in periods where expensive resources are sitting idle or occasions of insufficient capacity. With cloud computing, you can access as much or as little capacity as you need and dynamically scale to meet actual demand, while only paying for what you use. On AWS, servers, databases, storage, and higher-level application components can be instantiated within seconds. You can treat these as temporary resources, free from the inflexibility and constraints of a fixed and finite IT infrastructure. This resets the way you approach change management, testing, reliability, and capacity planning. This change in approach encourages experimentation by introducing the ability in processes to fail fast and iterate quickly. Global, Available, and Scalable Capacity Using the global infrastructure of AWS, you can deploy your application to the AWS Region that best meets your requirements (e.g., proximity to your end users, compliance, data residency constraints, and cost).3 For global applications, you can reduce latency to end users around the world by using the Amazon CloudFront content delivery network (CDN). This also makes it much easier to operate production applications and databases across multiple data centers to achieve high availability and fault tolerance. The global infrastructure of AWS and the ability to provision capacity as needed let you think differently about your infrastructure as the demands on your applications and the breadth of your services expand. Archived Page 2 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Higher-Level Managed Services Apart from the compute resources of Amazon Elastic Compute Cloud (Amazon EC2), you also have access to a broad set of storage, database, analytics, application, and deployment services. Because these services are instantly available to developers, they reduce dependency on in-house specialized skills and allow organizations to deliver new solutions faster. AWS services that are managed can lower operational complexity and cost. They are also designed for scalability and high availability, so they can reduce risk for your implementations. Built-in Security In traditional IT environments, infrastructure security auditing can be a periodic and manual process. In contrast, the AWS Cloud provides governance capabilities that enable continuous monitoring of configuration changes to your IT resources. Security at AWS is the highest priority, which means that you benefit from data centers and network architecture that are built to meet the requirements of the most security sensitive organizations. Since AWS resources are programmable using tools and APIs, you can formalize and embed your security policy within the design of your infrastructure. With the ability to spin up temporary environments, security testing can now become part of your continuous delivery pipeline. Finally, you can leverage a variety of native AWS security and encryption features that can help you achieve higher levels of data protection and compliance. Architecting for Cost Traditional cost management of on-premises solutions is not typically tightly coupled to the provision of services. When you provision a cloud computing environment, optimizing for cost is a fundamental design tenant for architects. When selecting a solution, you should not only focus on the functional architecture and feature set but on the cost profile of the solutions you select. Archived AWS provides fine-grained billing, which enables you to track the costs associated with all aspects of your solutions. There are a range of services to help you manage budgets, alert you to costs incurred, and to help you optimize resource usage and costs. Page 3 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Operations on AWS When operating services on AWS, there are several common categories of operating models: • Applications that are migrated, maintain existing traditional operating models, leverage the ability to manage Infrastructure as Code through APIs enabling robust and repeatable build processes, improving reliability. • Solutions that are refactored leverage higher levels of automation of the operational processes as the supporting services, e.g. AWS Auto Scaling and self-healing architectures. • Solutions that are rearchitected and designed for cloud operations are typically fully automated through DevOps processes for delivery pipeline and management. Supporting these transitions does not just change the technologies used, but also cultural changes in the way that development and operational teams are managed. AWS provides tooling, processes, and best practices to support the transition of operational practices to maximize the benefits that can be leveraged from cloud computing. Archived Page 4 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Design Principles The AWS Cloud includes many design patterns and architectural options that you can apply to a wide variety of use cases. Some key design principles of the AWS Cloud include scalability, disposable resources, automation, loose coupling managed services instead of servers, and flexible data storage options. Scalability Systems that are expected to grow over time need to be built on top of a scalable architecture. Such an architecture can support growth in users, traffic, or data size with no drop-in performance. It should provide that scale in a linear manner where adding extra resources results in at least a proportional increase in ability to serve additional load. Growth should introduce economies of scale, and cost should follow the same dimension that generates business value out of that system. While cloud computing provides virtually unlimited on-demand capacity, your design needs to be able to take advantage of those resources seamlessly. There are generally two ways to scale an IT architecture: vertically and horizontally. Scaling Vertically Scaling vertically takes place through an increase in the specifications of an individual resource, such as upgrading a server with a larger hard drive or a faster CPU. With Amazon EC2, you can stop an instance and resize it to an instance type that has more RAM, CPU, I/O, or networking capabilities. This way of scaling can eventually reach a limit, and it is not always a cost-efficient or highly available approach. However, it is very easy to implement and can be sufficient for many use cases especially in the short term. Scaling Horizontally Scaling horizontally takes place through an increase in the number of resources, such as adding more hard drives to a storage array or adding more servers to support an application. This is a great way to build internet-scale applications that leverage the elasticity of cloud computing. Not all architectures are designed to distribute their Archived workload to multiple resources, so let’s examine some possible scenarios. Page 5 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Stateless Applications When users or services interact with an application they will often perform a series of interactions that form a session. A session is unique data for users that persists between requests while they use the application. A stateless application is an application that does not need knowledge of previous interactions and does not store session information. For example, an application that, given the same input, provides the same response to any end user, is a stateless application. Stateless applications can scale horizontally because any of the available compute resources (such as EC2 instances and AWS Lambda functions) can service any request. Without stored session data, you can simply add more compute resources as needed. When that capacity is no longer required, you can safely terminate those individual resources, after running tasks have been drained. Those resources do not need to be aware of the presence of their peers—all that is required is a way to distribute the workload to them. Distribute Load to Multiple Nodes To distribute the workload to multiple nodes in your environment, you can choose either a push or a pull model. With a push model, you can use Elastic Load Balancing (ELB) to distribute a workload. ELB routes incoming application requests across multiple EC2 instances. When routing traffic, a Network Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model to handle millions of requests per second. With the adoption of container-based services, you can also use an Application Load Balancer. An Application Load Balancer provides Layer 7 of the OSI model and supports content based routing of requests based on application traffic. Alternatively, you can use Amazon Route 53 to implement a DNS round robin. In this case, DNS responses return an IP address from a list of valid hosts in a round-robin fashion. While easy to implement, this approach does not always work well with the elasticity of cloud computing. This is because even if you can set low time to live (TTL) values for your DNS records, caching DNS resolvers are outside the control of Amazon Route 53 and might not always respect your settings. Instead of a load balancing solution, you can implement a pull model for asynchronous, event-driven workloads. In a pull model, tasks that need to be performed or data that needs to be processed can be stored as messages in a queue using Amazon Simple Queue Service (Amazon SQS) or as a streaming data solution Archived Page 6 Amazon Web Services – Architecting for the Cloud: AWS Best Practices such as Amazon Kinesis. Multiple compute resources can then pull and consume those messages, processing them in a distributed fashion. Stateless Components In practice, most applications maintain some kind of state information. For example, web applications need to track whether a user is signed in so that personalized content can be presented based on previous actions. An automated multi-step process also needs to track previous activity to decide what its next action should be. You can still make a portion of these architectures stateless by not storing anything that needs to persist for more than a single request in the local file system. For example, web applications can use HTTP cookies to store session information (such as shopping cart items) in the web client cache. The browser passes that information back to the server at each subsequent request so that the application does not need to store it. However, this approach has two drawbacks. First, the content of the HTTP cookies can be tampered with on the client side, so you should always treat it as untrusted data that must be validated. Second, HTTP cookies are transmitted with every request, which means that you should keep their size to a minimum to avoid unnecessary latency. Consider only storing a unique session identifier in an HTTP cookie and storing more detailed user session information on the server side. Most programming platforms provide a native session management mechanism that works this way. However, user session information is often stored on the local file system by default and results in a stateful architecture. A common solution to this problem is to store this information in a database. Amazon DynamoDB is a great choice because of its scalability, high availability, and durability characteristics. For many platforms, there are open source drop-in replacement libraries that allow you to store native sessions in Amazon DynamoDB.4 Other scenarios require storage of larger files (such as user uploads and interim results of batch processes). By placing those files in a shared storage layer such as Amazon Simple Storage Service (Amazon S3) or Amazon Elastic File System (Amazon EFS), you can avoid the introduction of stateful components. Archived Finally, a complex multi-step workflow is another example where you must track the current state of each execution. You can use AWS Step Functions to centrally store execution history and make these workloads stateless. Page 7 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Stateful Components Inevitably, there will be layers of your architecture that you won’t turn into stateless components. By definition, databases are stateful. For more information, see the Databases section. In addition, many legacy applications were designed to run on a single server by relying on local compute resources. Other use cases might require client devices to maintain a connection to a specific server for prolonged periods. For example, real-time multiplayer gaming must offer multiple players a consistent view of the game world with very low latency. This is much simpler to achieve in a non distributed implementation where participants are connected to the same server. You might still be able to scale those components horizontally by distributing the load to multiple nodes with session affinity. In this model, you bind all the transactions of a session to a specific compute resource. But, this model does have some limitations. Existing sessions do not directly benefit from the introduction of newly launched compute nodes. More importantly, session affinity cannot be guaranteed. For example, when a node is terminated or becomes unavailable, users bound to it will be disconnected and experience a loss of session-specific data, which is anything that is not stored in a shared resource such as Amazon S3, Amazon EFS, or a database. Implement Session Affinity For HTTP and HTTPS traffic, you can use the sticky sessions feature of an Application Load Balancer to bind a user’s session to a specific instance.5 With this feature, an Application Load Balancer will try to use the same server for that user for the duration of the session. Another option—if you control the code that runs on the client—is to use client-side load balancing. This adds extra complexity, but can be useful in scenarios where a load balancer does not meet your requirements. For example, you might be using a protocol that’s not supported by ELB, or you might need full control over how users are assigned to servers (e.g., in a gaming scenario, you might need to make sure that game participants are matched and connect to the same server). In this model, the clients need a way of discovering valid server endpoints to directly connect to. You can use DNS for that, or you can build a simple discovery API to provide that information to the software running on the client. In the absence of a load balancer, the health checking mechanism also needs to be implemented on the client side. You should design your client logic so that when server unavailability is detected, devices reconnect to another server with little disruption for the application. Archived Page 8 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Distributed Processing Use cases that involve the processing of very large amounts of data—anything that can’t be handled by a single compute resource in a timely manner—require a distributed processing approach. By dividing a task and its data into many small fragments of work, you can execute them in parallel across a set of compute resources. Implement Distributed Processing Offline batch jobs can be horizontally scaled by using distributed data processing engines such as AWS Batch, AWS Glue, and Apache Hadoop. On AWS, you can use Amazon EMR to run Hadoop workloads on top of a fleet of EC2 instances without the operational complexity. For real-time processing of streaming data, Amazon Kinesis partitions data in multiple shards that can then be consumed by multiple Amazon EC2 or AWS Lambda resources to achieve scalability. For more information on these types of workloads, see the Big Data Analytics Options on AWS6 and Core Tenets of IoT whitepapers.7 Disposable Resources Instead of Fixed Servers In a traditional infrastructure environment, you have to work with fixed resources because of the upfront cost and lead time of introducing new hardware. This drives practices such as manually logging in to servers to configure software or fix issues, hardcoding IP addresses, and running tests or processing jobs sequentially. When designing for AWS, you can take advantage of the dynamically provisioned nature of cloud computing. You can think of servers and other components as temporary resources. You can launch as many as you need, and use them only for as long as you need them. Another issue with fixed, long-running servers is configuration drift. Changes and software patches applied through time can result in untested and heterogeneous configurations across different environments. You can solve this problem with an immutable infrastructure pattern. With this approach, a server—once launched—is never updated. Instead, when there is a problem or need for an update, the problem server is replaced with a new server that has the latest configuration. This enables resources to always be in a consistent (and tested) state, and makes rollbacks easier to perform. This is more easily supported with stateless architectures. Archived Page 9 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Instantiating Compute Resources Whether you are deploying a new environment for testing, or increasing capacity of an existing system to cope with extra load, you do not want to manually set up new resources with their configuration and code. It is important that you make this an automated and repeatable process that avoids long lead times and is not prone to human error. There are a few methods to achieve this. Bootstrapping When you launch an AWS resource such as an EC2 instance or Amazon Relational Database Service (Amazon RDS) DB instance, you start with a default configuration. You can then execute automated bootstrapping actions, which are scripts that install software or copy data to bring that resource to a particular state. You can parameterize configuration details that vary between different environments (such as production or test) so that you can reuse the same scripts without modifications. You can set up new EC2 instances with user data scripts and cloud-init directives.8 You can use simple scripts and configuration management tools such as Chef or Puppet. In addition, with custom scripts and the AWS APIs, or with AWS CloudFormation support for AWS Lambda-backed custom resources, you can write provisioning logic that acts on almost any AWS resource.9 Golden Images Certain AWS resource types, such as EC2 instances, Amazon RDS DB instances, and Amazon Elastic Block Store (Amazon EBS) volumes, can be launched from a golden image, which is a snapshot of a particular state of that resource. When compared to the bootstrapping approach, a golden image results in faster start times and removes dependencies to configuration services or third-party repositories. This is important in auto-scaled environments where you want to be able to quickly and reliably launch additional resources as a response to demand changes. You can customize an EC2 instance and then save its configuration by creating an Amazon Machine Image (AMI).10 You can launch as many instances from the AMI as you need, and they will all include those customizations. Each time you want to change your configuration you must create a new golden image, so you must have a versioning convention to manage your golden images over time. We recommend that you use a script to create the bootstrap for the EC2 instances that you use to create Archived Page 10 Amazon Web Services – Architecting for the Cloud: AWS Best Practices your AMIs. This gives you a flexible method to test and modify those images through time. Alternatively, if you have an existing on-premises virtualized environment, you can use VM Import/Export from AWS to convert a variety of virtualization formats to an AMI. You can also find and use prebaked, shared AMIs provided either by AWS or third parties in AWS Marketplace. While golden images are most commonly used when you launch new EC2 instances, they can also be applied to resources such as Amazon RDS DB instances or Amazon EBS volumes. For example, when you launch a new test environment, you might want to prepopulate its database by instantiating it from a specific Amazon RDS snapshot, instead of importing the data from a lengthy SQL script. Containers Another option popular with developers is Docker—an open-source technology that allows you to build and deploy distributed applications inside software containers. Docker allows you to package a piece of software in a Docker image, which is a standardized unit for software development, containing everything the software needs to run: code, runtime, system tools, system libraries, etc. AWS Elastic Beanstalk, Amazon Elastic Container Service (Amazon ECS) and AWS Fargate let you deploy and manage multiple containers across a cluster of EC2 instances. You can build golden Docker images and use the ECS Container Registry to manage them. An alternative container environment is Kubernetes and Amazon Elastic Container Service for Kubernetes (Amazon EKS). With Kubernetes and Amazon EKS, you can easily deploy, manage, and scale containerized applications. Hybrid You can also use a combination of the two approaches: some parts of the configuration are captured in a golden image, while others are configured dynamically through a bootstrapping action. Items that do not change often or that introduce external dependencies will typically be part of your golden image. An example of a good candidate is your web server software that would otherwise have to be downloaded by a third-party repository each time you launch an instance. Archived Page 11 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Items that change often or differ between your various environments can be set up dynamically through bootstrapping actions. For example, if you are deploying new versions of your application frequently, creating a new AMI for each application version might be impractical. You also do not want to hard code the database hostname configuration to your AMI because that would be different between the test and production environments. User data or tags allow you to use more generic AMIs that can be modified at launch time. For example, if you run web servers for various small businesses, they can all use the same AMI and retrieve their content from an S3 bucket location that you specify in the user data at launch. AWS Elastic Beanstalk follows the hybrid model. It provides preconfigured run time environments—each initiated from its own AMI11—but allows you to run bootstrap actions through .ebextensions configuration files12, and configure environmental variables to parameterize the environment differences. For a more detailed discussion of the different ways you can manage deployments of new resources, see the Overview of Deployment Options on AWS13 and Managing Your AWS Infrastructure at Scale whitepapers.14 Infrastructure as Code Application of the principles we have discussed does not have to be limited to the individual resource level. Because AWS assets are programmable, you can apply techniques, practices, and tools from software development to make your whole infrastructure reusable, maintainable, extensible, and testable. For more information, see the Infrastructure as Code whitepaper.15 AWS CloudFormation templates give you an easy way to create and manage a collection of related AWS resources, and provision and update them in an orderly and predictable fashion. You can describe the AWS resources and any associated dependencies or runtime parameters required to run your application. Your CloudFormation templates can live with your application in your version control repository, which allows you to reuse architectures and reliably clone production environments for testing. Automation Archived In a traditional IT infrastructure, you often have to manually react to a variety of events. When deploying on AWS, there is an opportunity for automation, so that you Page 12 Amazon Web Services – Architecting for the Cloud: AWS Best Practices improve both your system’s stability and the efficiency of your organization. Consider introducing one or more of these types of automation into your application architecture to ensure more resiliency, scalability, and performance. Serverless Management and Deployment When you adopt serverless patterns, the operational focus is on the automation of the deployment pipeline. AWS manages the underlying services, scale, and availability. AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy support the automation of the deployment of these processes.16 Infrastructure Management and Deployment AWS Elastic Beanstalk: You can use this service to deploy and scale web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.17 Developers can simply upload their application code, and the service automatically handles all the details, such as resource provisioning, load balancing, auto scaling, and monitoring. Amazon EC2 auto recovery: You can create an Amazon CloudWatch alarm that monitors an EC2 instance and automatically recovers it if it becomes impaired.18 A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. However, this feature is only available for applicable instance configurations. Refer to the Amazon EC2 documentation for an up-to-date description of those preconditions. In addition, during instance recovery, the instance is migrated through an instance reboot, and any data that is in-memory is lost. AWS Systems Manager: You can automatically collect software inventory, apply OS patches, create a system image to configure Windows and Linux operating systems, and execute arbitrary commands. Provisioning these services simplifies the operating model and ensures the optimum environment configuration.19 Auto Scaling: You can maintain application availability and scale your Amazon EC2, Amazon DynamoDB, Amazon ECS, Amazon Elastic Container Service for Kubernetes (Amazon EKS) capacity up or down automatically according to the conditions you define.20 You can use Auto Scaling to help make sure that you are running the desired number of healthy EC2 instances across multiple Availability Zones. Auto Scaling can also automatically increase the number of EC2 instances during demand spikes to Archived Page 13 Amazon Web Services – Architecting for the Cloud: AWS Best Practices maintain performance and decrease capacity during less busy periods to optimize costs. Alarms and Events Amazon CloudWatch alarms: You can create a CloudWatch alarm that sends an Amazon Simple Notification Service (Amazon SNS) message when a particular metric goes beyond a specified threshold for a specified number of periods.21 Those Amazon SNS messages can automatically kick off the execution of a subscribed Lambda function, enqueue a notification message to an Amazon SQS queue, or perform a POST request to an HTTP or HTTPS endpoint. Amazon CloudWatch Events: Delivers a near real-time stream of system events that describe changes in AWS resources.22 Using simple rules, you can route each type of event to one or more targets, such as Lambda functions, Kinesis streams, and SNS topics. AWS Lambda scheduled events: You can create a Lambda function and configure AWS Lambda to execute it on a regular schedule.23 AWS WAF security automations: AWS WAF is a web application firewall that enables you to create custom, application-specific rules that block common attack patterns that can affect application availability, compromise security, or consume excessive resources. You can administer AWS WAF completely through APIs, which makes security automation easy, enabling rapid rule propagation and fast incident response.24 Loose Coupling As application complexity increases, a desirable attribute of an IT system is that it can be broken into smaller, loosely coupled components. This means that IT systems should be designed in a way that reduces interdependencies—a change or a failure in one component should not cascade to other components. Well-Defined Interfaces Archived A way to reduce interdependencies in a system is to allow the various components to interact with each other only through specific, technology-agnostic interfaces, such as RESTful APIs. In that way, technical implementation detail is hidden so that teams can Page 14 Amazon Web Services – Architecting for the Cloud: AWS Best Practices modify the underlying implementation without affecting other components. As long as those interfaces maintain backwards compatibility, deployments of difference components are decoupled. This granular design pattern is commonly referred to as a microservices architecture. Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Service Discovery Applications that are deployed as a set of smaller services depend on the ability of those services to interact with each other. Because each of those services can be running across multiple compute resources, there needs to be a way for each service to be addressed. For example, in a traditional infrastructure, if your front-end web service needs to connect with your back-end web service, you could hardcode the IP address of the compute resource where this service was running. Although this approach can still work in cloud computing, if those services are meant to be loosely coupled, they should be able to be consumed without prior knowledge of their network topology details. Apart from hiding complexity, this also allows infrastructure details to change at any time. Loose coupling is a crucial element if you want to take advantage of the elasticity of cloud computing where new resources can be launched or terminated at any point in time. In order to achieve that you will need some way of implementing service discovery. Implement Service Discovery For an Amazon EC2-hosted service, a simple way to achieve service discovery is through Elastic Load Balancing (ELB). Because each load balancer gets its own hostname, you can consume a service through a stable endpoint. This can be combined with DNS and private Amazon Route 53 zones, so that the particular load balancer’s endpoint can be abstracted and modified at any time. Archived Another option is to use a service registration and discovery method to allow retrieval of the endpoint IP addresses and port number of any service. Because service discovery becomes the glue between the components, it is important that it is highly available and reliable. If load balancers are not used, service discovery should also Page 15 Amazon Web Services – Architecting for the Cloud: AWS Best Practices allow options such as health checks. Amazon Route 53 supports auto naming to make it easier to provision instances for microservices. Auto naming lets you automatically create DNS records based on a configuration you define. Other example implementations include custom solutions using a combination of tags, a highly available database, custom scripts that call the AWS APIs, or open-source tools such as Netflix Eureka, Airbnb Synapse, or HashiCorp Consul. Asynchronous Integration Asynchronous integration is another form of loose coupling between services. This model is suitable for any interaction that does not need an immediate response and where an acknowledgement that a request has been registered will suffice. It involves one component that generates events and another that consumes them. The two components do not integrate through direct point-to-point interaction but usually through an intermediate durable storage layer, such as an SQS queue or a streaming data platform such as Amazon Kinesis, cascading Lambda events, AWS Step Functions, or Amazon Simple Workflow Service. Figure 1: Tight and loose coupling Archived This approach decouples the two components and introduces additional resiliency. So, for example, if a process that is reading messages from the queue fails, messages can still be added to the queue and processed when the system recovers. It also allows you to protect a less scalable back-end service from front-end spikes and find the right Page 16 Amazon Web Services – Architecting for the Cloud: AWS Best Practices tradeoff between cost and processing lag. For example, you can decide that you don’t need to scale your database to accommodate an occasional peak of write queries, as long as you eventually process those queries asynchronously with some delay. Finally, by removing slow operations from interactive request paths you can also improve the end-user experience. Examples of asynchronous integration include: • A front-end application inserts jobs in a queue system such as Amazon SQS. A back-end system retrieves those jobs and processes them at its own pace. • An API generates events and pushes them into Kinesis streams. A back-end application processes these events in batches to create aggregated time-series data stored in a database. • Multiple heterogeneous systems use AWS Step Functions to communicate the flow of work between them without directly interacting with each other. • Lambda functions can consume events from a variety of AWS sources, such as Amazon DynamoDB update streams and Amazon S3 event notifications. You don’t have to worry about implementing a queuing or other asynchronous integration method because Lambda handles this for you. Distributed Systems Best Practices Another way to increase loose coupling is to build applications that handle component failure in a graceful manner. You can identify ways to reduce the impact to your end users and increase your ability to make progress on your offline procedures, even in the event of some component failure. Graceful Failure in Practice A request that fails can be retried with an exponential backoff and Jitter strategy, or it can be stored in a queue for later processing.25 For front-end interfaces, it might be possible to provide alternative or cached content instead of failing completely when, for example, your database server becomes unavailable. The Amazon Route 53 DNS failover feature also gives you the ability to monitor your website and automatically route your visitors to a backup site if your primary site becomes unavailable. You can host your backup site as a static website on Amazon S3 or as a separate dynamic environment. Archived Page 17 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Services, Not Servers Developing, managing, and operating applications, especially at scale, requires a wide variety of underlying technology components. With traditional IT infrastructure, companies would have to build and operate all those components. AWS offers a broad set of compute, storage, database, analytics, application, and deployment services that help organizations move faster and lower IT costs. Architectures that do not leverage that breadth (e.g., if they use only Amazon EC2) might not be making the most of cloud computing and might be missing an opportunity to increase developer productivity and operational efficiency. Managed Services AWS managed services provide building blocks that developers can consume to power their applications. These managed services include databases, machine learning, analytics, queuing, search, email, notifications, and more. For example, with Amazon SQS you can offload the administrative burden of operating and scaling a highly available messaging cluster, while paying a low price for only what you use. Amazon SQS is also inherently scalable and reliable. The same applies to Amazon S3, which enables you to store as much data as you want and access it when you need it, without having to think about capacity, hard disk configurations, replication, and other related issues. Other examples of managed services that power your applications include:26 • Amazon CloudFront for content delivery • ELB for load balancing • Amazon DynamoDB for NoSQL databases • Amazon CloudSearch for search workloads • Amazon Elastic Transcoder for video encoding • Amazon Simple Email Service (Amazon SES) for sending and receiving emails27 Archived Page 18 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Serverless Architectures Serverless architectures can reduce the operational complexity of running applications. It is possible to build both event-driven and synchronous services for mobile, web, analytics, CDN business logic, and IoT without managing any server infrastructure. These architectures can reduce costs because you don’t have to manage or pay for underutilized servers, or provision redundant infrastructure to implement high availability. For example, you can upload your code to the AWS Lambda compute service, and the service can run the code on your behalf using AWS infrastructure. With AWS Lambda, you are charged for every 100ms your code executes and the number of times your code is triggered. By using Amazon API Gateway, you can develop virtually infinitely scalable synchronous APIs powered by AWS Lambda. When combined with Amazon S3 for serving static content assets, this pattern can deliver a complete web application. For more details on this type of architecture, see the AWS Serverless Multi-Tier Architectures whitepaper.28 When it comes to mobile and web apps, you can use Amazon Cognito so that you don’t have to manage a back-end solution to handle user authentication, network state, storage, and sync. Amazon Cognito generates unique identifiers for your users. Those identifiers can be referenced in your access policies to enable or restrict access to other AWS resources on a per-user basis. Amazon Cognito provides temporary AWS credentials to your users, allowing the mobile application running on the device to interact directly with AWS Identity and Access Management (IAM)-protected AWS services. For example, using IAM you can restrict access to a folder in an S3 bucket to a particular end user. For IoT applications, organizations have traditionally had to provision, operate, scale, and maintain their own servers as device gateways to handle the communication between connected devices and their services. AWS IoT provides a fully managed device gateway that scales automatically with your usage without any operational overhead. Serverless architectures have also made it possible to run responsive services at edge locations. AWS Lambda@Edge lets you run Lambda functions at Amazon CloudFront edge locations in response to CloudFront events. This enables patterns for low-latency Archived Page 19 Amazon Web Services – Architecting for the Cloud: AWS Best Practices solutions and the introduction of functionality without changing underlying applications.29 For data analytics, managing queries on large volumes of data typically requires you to manage a complex infrastructure. Amazon Athena is an interactive query service that makes it easy for you to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Databases With traditional IT infrastructure, organizations are often limited to the database and storage technologies they can use. There can be constraints based on licensing costs and the ability to support diverse database engines. On AWS, these constraints are removed by managed database services that offer enterprise performance at open source cost. As a result, it is not uncommon for applications to run on top of a polyglot data layer choosing the right technology for each workload. Choose the Right Database Technology for Each Workload These questions can help you make decisions about which solutions to include in your architecture: • Is this a read-heavy, write-heavy, or balanced workload? How many reads and writes per second are you going to need? How will those values change if the number of users increases? • How much data will you need to store and for how long? How quickly will this grow? Is there an upper limit in the near future? What is the size of each object (average, min, max)? How will these objects be accessed? • What are the requirements in terms of durability of data? Is this data store going to be your “source of truth?” • What are your latency requirements? How many concurrent users do you need to support? Archived • What is your data model and how are you going to query the data? Are your queries relational in nature (e.g., JOINs between multiple tables)? Could you denormalize your schema to create flatter data structures that are easier to scale? Page 20 Amazon Web Services – Architecting for the Cloud: AWS Best Practices • What kind of functionality do you require? Do you need strong integrity controls, or are you looking for more flexibility (e.g., schema-less data stores)? Do you require sophisticated reporting or search capabilities? Are your developers more familiar with relational databases than NoSQL? • What are the associated database technology license costs? Do these costs consider application development investment, storage, and usage costs over time? Does the licensing model support projected growth? Could you use cloud-native database engines such as Amazon Aurora to get the simplicity and cost-effectiveness of open-source databases? Relational Databases Relational databases (also known as RDBS or SQL databases) normalize data into well defined tabular structures known as tables, which consist of rows and columns. They provide a powerful query language, flexible indexing capabilities, strong integrity controls, and the ability to combine data from multiple tables in a fast and efficient manner. Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud with support for many familiar database engines. Scalability Relational databases can scale vertically by upgrading to a larger Amazon RDS DB instance or adding more and faster storage. In addition, consider using Amazon Aurora, which is a database engine designed to deliver much higher throughput compared to standard MySQL running on the same hardware. For read-heavy applications, you can also horizontally scale beyond the capacity constraints of a single DB instance by creating one or more read replicas. Read replicas are separate database instances that are replicated asynchronously. As a result, they are subject to replication lag and might be missing some of the latest transactions. Application designers need to consider which queries have tolerance to slightly stale data. Those queries can be executed on a read replica, while the remainder should run on the primary node. Read replicas can also not accept any write queries. Archived Relational database workloads that need to scale their write capacity beyond the constraints of a single DB instance require a different approach called data partitioning or sharding. With this model, data is split across multiple database schemas each Page 21 Amazon Web Services – Architecting for the Cloud: AWS Best Practices running in its own autonomous primary DB instance. Although Amazon RDS removes the operational overhead of running those instances, sharding introduces some complexity to the application. The application’s data access layer needs to be modified to have awareness of how data is split so that it can direct queries to the right instance. In addition, schema changes must be performed across multiple database schemas, so it is worth investing some effort to automate this process. High Availability For any production relational database, we recommend using the Amazon RDS Multi AZ deployment feature, which creates a synchronously replicated standby instance in a different Availability Zone. In case of failure of the primary node, Amazon RDS performs an automatic failover to the standby without the need for manual administrative intervention. When a failover is performed, there is a short period during which the primary node is not accessible. Resilient applications can be designed for Graceful Failure by offering reduced functionality, such as read-only mode by using read replicas. Amazon Aurora provides multi-master capability to enable reads and writes to be scaled across Availability Zones and also supports cross-Region replication. Anti-Patterns If your application primarily indexes and queries data with no need for joins or complex transactions—especially if you expect a write throughput beyond the constraints of a single instance—consider a NoSQL database instead. If you have large binary files (audio, video, and image), it will be more efficient to store the actual files in Amazon S3 and only hold the metadata for the files in your database. For more detailed relational database best practices, see the Amazon RDS documentation.30 Archived Page 22 Amazon Web Services – Architecting for the Cloud: AWS Best Practices NoSQL Databases NoSQL databases trade some of the query and transaction capabilities of relational databases for a more flexible data model that seamlessly scales horizontally. NoSQL databases use a variety of data models, including graphs, key-value pairs, and JSON documents, and are widely recognized for ease of development, scalable performance, high availability, and resilience. Amazon DynamoDB is a fast and flexible NoSQL database service for applications that need consistent, single-digit, millisecond latency at any scale.31 It is a fully managed cloud database and supports both document and key-value store models. Scalability NoSQL database engines will typically perform data partitioning and replication to scale both the reads and the writes in a horizontal fashion. They do this transparently, and don’t need the data partitioning logic implemented in the data access layer of your application. Amazon DynamoDB in particular manages table partitioning automatically, adding new partitions as your table grows in size or as read-provisioned and write-provisioned capacity changes. Amazon DynamoDB Accelerator (DAX) is a managed, highly available, in-memory cache for DynamoDB to leverage significant performance improvements.32 To learn about how you can make the most of Amazon DynamoDB scalability when you design your application, see Best Practices for DynamoDB.33 High Availability Amazon DynamoDB synchronously replicates data across three facilities in an AWS Region, which provides fault tolerance in the event of a server failure or Availability Zone disruption. Amazon DynamoDB also supports global tables to provide a fully managed, multi-Region, multi-master database that provides fast, local, read-and write performance for massively scaled global applications. Global Tables are replicated across your selected AWS Regions. Anti-Patterns Archived If your schema cannot be denormalized, and your application requires joins or complex transactions, consider a relational database instead. If you have large binary files (audio, video, and image), consider storing the files in Amazon S3 and storing the metadata for the files in your database. Page 23 Amazon Web Services – Architecting for the Cloud: AWS Best Practices For guidance on migrating from a relational database to DynamoDB, or on evaluating which workloads to migrate, see the Best Practices for Migrating from RDBMS to DynamoDB whitepaper.34 Data Warehouse A data warehouse is a specialized type of relational database, which is optimized for analysis and reporting of large amounts of data. It can be used to combine transactional data from disparate sources (such as user behavior in a web application, data from your finance and billing system, or customer relationship management or CRM) to make them available for analysis and decision-making. Traditionally, setting up, running, and scaling a data warehouse has been complicated and expensive. On AWS, you can leverage Amazon Redshift, a managed data warehouse service that is designed to operate at less than a tenth the cost of traditional solutions. Scalability Amazon Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing (MPP), columnar data storage, and targeted data compression encoding schemes. It is particularly suited to analytic and reporting workloads against very large data sets. The Amazon Redshift MPP architecture enables you to increase performance by increasing the number of nodes in your data warehouse cluster. Amazon Redshift Spectrum enables Amazon Redshift SQL queries against exabytes of data in Amazon S3, which extends the analytic capabilities of Amazon Redshift beyond data stored on local disks in the data warehouse to unstructured data, without the need to load or transform data. High Availability Amazon Redshift has multiple features that enhance the reliability of your data warehouse cluster. We recommend that you deploy production workloads in multi node clusters, so that data that is written to a node is automatically replicated to other nodes within the cluster. Data is also continuously backed up to Amazon S3. Amazon Redshift continuously monitors the health of the cluster and automatically re replicates data from failed drives and replaces nodes as necessary. For more information, see the Amazon Redshift FAQ.35 Archived Page 24 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Anti-Patterns Because Amazon Redshift is an SQL-based relational database management system (RDBMS), it is compatible with other RDBMS applications and business intelligence tools. Although Amazon Redshift provides the functionality of a typical RDBMS, including online transaction processing (OLTP) functions, it is not designed for these workloads. If you expect a high concurrency workload that generally involves reading and writing all of the columns for a small number of records at a time, you should instead consider using Amazon RDS or Amazon DynamoDB. Search Search is often confused with query. A query is a formal database query, which is addressed in formal terms to a specific data set. Search enables datasets to be queried that are not precisely structured. For this reason, applications that require sophisticated search functionality will typically outgrow the capabilities of relational or NoSQL databases. A search service can be used to index and search both structured and free text format and can support functionality that is not available in other databases, such as customizable result ranking, faceting for filtering, synonyms, and stemming. On AWS, you can choose between Amazon CloudSearch and Amazon Elasticsearch Service (Amazon ES). Amazon CloudSearch is a managed service that requires little configuration and will scale automatically. Amazon ES offers an open-source API and gives you more control over the configuration details. Amazon ES has also evolved to become more than just a search solution. It is often used as an analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analytics. Archived Page 25 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Scalability Both Amazon CloudSearch and Amazon ES use data partitioning and replication to scale horizontally. The difference is that with Amazon CloudSearch, you don’t need to worry about how many partitions and replicas you need because the service automatically handles that. High Availability Both Amazon CloudSearch and Amazon ES include features that store data redundantly across Availability Zones. For details, see the Amazon CloudSearch36 and Amazon ES documentation.37 Graph Databases A graph database uses graph structures for queries. A graph is defined as consisting of edges (relationships), which directly relate to nodes (data entities) in the store. The relationships enable data in the store to be linked together directly, which allows for the fast retrieval of complex hierarchical structures in relational systems. For this reason, graph databases are purposely built to store and navigate relationships and are typically used in use cases like social networking, recommendation engines, and fraud detection, in which you need to be able to create relationships between data and quickly query these relationships. Amazon Neptune is a fully-managed graph database service. For more information, see the Amazon Neptune FAQ. Scalability Amazon Neptune is a purpose-built, high-performance graph database optimized for processing graph queries. High Availability Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure, with support for encryption at rest and in transit.38 Archived Page 26 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Managing Increasing Volumes of Data Traditional data storage and analytics tools can no longer provide the agility and flexibility required to deliver relevant business insights. That’s why many organizations are shifting to a data lake architecture. A data lake is an architectural approach that allows you to store massive amounts of data in a central location so that it's readily available to be categorized, processed, analyzed, and consumed by diverse groups within your organization. Since data can be stored as-is, you do not have to convert it to a predefined schema, and you no longer need to know what questions to ask about your data beforehand. This enables you to select the correct technology to meet your specific analytical requirements. For more information, see the Building a Data Lake with Amazon Web Services whitepaper.39 Removing Single Points of Failure Production systems typically come with defined or implicit objectives for uptime. A system is highly available when it can withstand the failure of an individual component or multiple components, such as hard disks, servers, and network links. To help you create a system with high availability, you can think about ways to automate recovery and reduce disruption at every layer of your architecture. For more information about high availability design patterns, refer to the Building Fault Tolerant Applications whitepaper.40 Introducing Redundancy Single points of failure can be removed by introducing redundancy, which means you have multiple resources for the same task. Redundancy can be implemented in either standby or active mode. In standby redundancy, when a resource fails, functionality is recovered on a secondary resource with the failover process. The failover typically requires some time before it completes, and during this period the resource remains unavailable. The secondary resource can either be launched automatically only when needed (to reduce cost), or it can already be running idle (to accelerate failover and minimize disruption). Standby redundancy is often used for stateful components such as relational databases. Archived Page 27 Amazon Web Services – Architecting for the Cloud: AWS Best Practices In active redundancy, requests are distributed to multiple redundant compute resources. When one of them fails, the rest can simply absorb a larger share of the workload. Compared to standby redundancy, active redundancy can achieve better usage and affect a smaller population when there is a failure. Detect Failure You should aim to build as much automation as possible in both detecting and reacting to failure. You can use services such as ELB and Amazon Route 53 to configure health checks and mask failure by routing traffic to healthy endpoints. In addition, you can replace unhealthy nodes automatically using Auto Scaling or by using the Amazon EC2 auto-recovery feature or services such as AWS Elastic Beanstalk.41 It won’t be possible to predict every possible failure scenario on day one. Make sure you collect enough logs and metrics to understand normal system behavior. After you understand that, you will be able to set up alarms for manual intervention or automated response. Design Good Health Checks Configuring the right health checks for your application helps determine your ability to respond correctly and promptly to a variety of failure scenarios. Specifying the wrong health check can actually reduce your application’s availability. In a typical three-tier application, you configure health checks on ELB. Design your health checks with the objective of reliably assessing the health of the back-end nodes. A simple TCP health check won’t detect if the instance itself is healthy but the web server process has crashed. Instead, you should assess whether the web server can return an HTTP 200 response for some simple request. At this layer, it might not be a good idea to configure a deep health check, which is a test that depends on other layers of your application to be successful, because false positives can result. For example, if your health check also assesses whether the instance can connect to a back-end database, you risk marking all of your web servers as unhealthy when that database node becomes shortly unavailable. A layered approach is often the best. A deep health check might be appropriate to implement at the Amazon Route 53 level. By running a more holistic check that determines if that environment is able to actually provide the required functionality, you can configure Amazon Route 53 to failover to a static version of your website until your database is up and running again. Archived Page 28 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Durable Data Storage Your application and your users will create and maintain a variety of data. It is crucial that your architecture protects both data availability and integrity. Data replication is the technique that introduces redundant copies of data. It can help horizontally scale read capacity, but it also increases data durability and availability. Replication can occur in a few different modes. Synchronous replication only acknowledges a transaction after it has been durably stored in both the primary location and its replicas. It is ideal for protecting the integrity of data from the event of a failure of the primary node. Synchronous replication can also scale read capacity for queries that require the most up-to-date data (strong consistency). The drawback of synchronous replication is that the primary node is coupled to the replicas. A transaction can’t be acknowledged before all replicas have performed the write. This can compromise performance and availability, especially in topologies that run across unreliable or high-latency network connections. For the same reason, it is not recommended to maintain many synchronous replicas. Regardless of the durability of your solution, this is no replacement for backups. Synchronous replication redundantly stores all updates to your data—even those that are results of software bugs or human error. However, particularly for objects stored on Amazon S3, you can use versioning to preserve, retrieve, and restore any of their versions.42 With versioning, you can recover from both unintended user actions and application failures. Asynchronous replication decouples the primary node from its replicas at the expense of introducing replication lag. This means that changes on the primary node are not immediately reflected on its replicas. Asynchronous replicas are used to horizontally scale the system’s read capacity for queries that can tolerate that replication lag. It can also be used to increase data durability when some loss of recent transactions can be tolerated during a failover. For example, you can maintain an asynchronous replica of a database in a separate AWS Region as a disaster recovery solution. Quorum-based replication combines synchronous and asynchronous replication to overcome the challenges of large-scale distributed database systems. Archived Replication to multiple nodes can be managed by defining a minimum number of nodes that must participate in a successful write operation. A detailed discussion of Page 29 Amazon Web Services – Architecting for the Cloud: AWS Best Practices distributed data stores is beyond the scope of this document. For more information about distributed data stores and the core set of principles for an ultra-scalable and highly reliable database system, see the Amazon Dynamo whitepaper.43 It is important to understand where each technology you are using fits in these data storage models. Their behavior during various failover or backup/restore scenarios should align to your recovery point objective (RPO) and your recovery time objective (RTO). You must ascertain how much data you expect to lose and how quickly you need to resume operations. For example, the Redis engine for Amazon ElastiCache supports replication with automatic failover, but the Redis engine’s replication is asynchronous. During a failover, it is highly likely that some recent transactions will be lost. However, Amazon RDS, with its Multi-AZ feature, is designed to provide synchronous replication to keep data on the standby node up-to-date with the primary. Automated Multi-Data Center Resilience Business-critical applications also need protection against disruption scenarios that affect more than just a single disk, server, or rack. In a traditional infrastructure, you typically have a disaster recovery plan to allow failover to a distant second data center in the event of a major disruption in the primary one. Because of the long distance between the two data centers, latency makes it impractical to maintain synchronous cross-data center copies of the data. As a result, a failover will most certainly lead to data loss or a very costly data recovery process. This makes failover a risky and not always sufficiently tested procedure. Nevertheless, this is a model that provides excellent protection against a low probability but huge impact risk, such as a natural catastrophe that brings down your whole infrastructure for a long time. For guidance on how to implement this approach on AWS, see the AWS Disaster Recovery whitepaper.44 A shorter interruption in a data center is a more likely scenario. For short disruptions in which the duration of the failure isn’t predicted to be long, the choice to perform a failover is a difficult one and is generally avoided. On AWS, it is possible to adopt a simpler, more efficient protection from this type of failure. Each AWS Region contains multiple distinct locations, or Availability Zones. Each Availability Zone is engineered to be independent from failures in other Availability Zones. An Availability Zone is a data center, and in some cases, an Availability Zone consists of multiple data centers. Archived Availability Zones within a Region provide inexpensive, low-latency network Page 30 Amazon Web Services – Architecting for the Cloud: AWS Best Practices connectivity to other zones in the same Region. This allows you to replicate your data across data centers in a synchronous manner so that failover can be automated and be transparent for your users. It is also possible to implement active redundancy. For example, a fleet of application servers can be distributed across multiple Availability Zones and be attached to ELB. When the EC2 instances of a particular Availability Zone fail their health checks, ELB stops sending traffic to those nodes. In addition, AWS Auto Scaling ensures that the correct number of EC2 instances are available to run your application, launching and terminating instances based on demand and defined by your scaling policies. If your application requires no short-term performance degradation because of an Availability Zone failure, your architecture should be statically stable, which means it does not require a change in the behavior of your workload to tolerate failures. In this scenario, your architecture should provision excess capacity to withstand the loss of one Availability Zone. 45 Many of the higher-level services on AWS are inherently designed according to the multiple Availability Zone (Multi-AZ) principle. For example, Amazon RDS provides high availability and automatic failover support for DB instances using Multi-AZ deployments, while with Amazon S3 and Amazon DynamoDB your data is redundantly stored across multiple facilities. Fault Isolation and Traditional Horizontal Scaling Though the active redundancy pattern is great for balancing traffic and handling instance or Availability Zone disruptions, it is not sufficient if there is something harmful about the requests themselves. For example, there could be scenarios where every instance is affected. If a particular request happens to trigger a bug that causes the system to fail over, then the caller may trigger a cascading failure by repeatedly trying the same request against all instances. Shuffle Sharding One fault-isolating improvement you can make to traditional horizontal scaling is called sharding. Similar to the technique traditionally used with data storage systems, instead of spreading traffic from all customers across every node, you can group the instances into shards. For example, if you have eight instances for your service, you might create four shards of two instances each (two instances for some redundancy within each shard) and distribute each customer to a specific shard. In this way, you Archived Page 31 Amazon Web Services – Architecting for the Cloud: AWS Best Practices are able to reduce the impact on customers in direct proportion to the number of shards you have. However, some customers will still be affected, so the key is to make the client fault tolerant. If the client can try every endpoint in a set of sharded resources until one succeeds, you get a dramatic improvement. This technique is known as shuffle sharding. For more information about this technique see the Shuffle Sharding: Massive and Magical Fault Isolation blog post.46 Archived Page 32 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Optimize for Cost When you move your existing architectures into the cloud, you can reduce capital expenses and drive savings as a result of the AWS economies of scale. By iterating and using more AWS capabilities, you can realize further opportunity to create cost optimized cloud architectures. For more information about how to optimize for cost with AWS cloud computing, see the Cost Optimization with AWS whitepaper.47 Right Sizing AWS offers a broad range of resource types and configurations for many use cases. For example, services such as Amazon EC2, Amazon RDS, Amazon Redshift, and Amazon ES offer many instance types. In some cases, you should select the cheapest type that suits your workload’s requirements. In other cases, using fewer instances of a larger instance type might result in lower total cost or better performance. You should benchmark your application environment and select the right instance type depending on how your workload uses CPU, RAM, network, storage size, and I/O. Similarly, you can reduce cost by selecting the right storage solution for your needs. For example, Amazon S3 offers a variety of storage classes, including Standard, Reduced Redundancy, and Standard-Infrequent Access. Other services, such as Amazon EC2, Amazon RDS, and Amazon ES, support different EBS volume types (magnetic, general purpose SSD, provisioned IOPS SSD) that you should evaluate. Over time, you can continue to reduce cost with continuous monitoring and tagging. Just like application development, cost optimization is an iterative process. Because, your application and its usage will evolve over time, and because AWS iterates frequently and regularly releases new options, it is important to continuously evaluate your solution. AWS provides tools to help you identify those cost-saving opportunities and keep your resources right-sized.48 To make those tools’ outcomes easy to interpret, you should define and implement a tagging policy for your AWS resources. You can make tagging a part of your build process and automate it with AWS management tools such as AWS Elastic Beanstalk and AWS OpsWorks. You can also use the managed rules provided by AWS Config to assess whether specific tags are applied to your resources or not. Archived Page 33 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Elasticity Another way you can save money with AWS is by taking advantage of the platform’s elasticity. Plan to implement Auto Scaling for as many Amazon EC2 workloads as possible, so that you horizontally scale up when needed and scale down and automatically reduce your spending when you don’t need that capacity anymore. In addition, you can automate turning off non-production workloads when not in use.49 Ultimately, consider which compute workloads you could implement on AWS Lambda so that you never pay for idle or redundant resources. Where possible, replace Amazon EC2 workloads with AWS managed services that either don’t require you to make any capacity decisions (such as ELB, Amazon CloudFront, Amazon SQS, Amazon Kinesis Firehose, AWS Lambda, Amazon SES, Amazon CloudSearch, or Amazon EFS) or enable you to easily modify capacity as and when need (such as Amazon DynamoDB, Amazon RDS, or Amazon ES). Take Advantage of the Variety of Purchasing Options Amazon EC2 On-Demand instance pricing gives you maximum flexibility with no long term commitments. Two other EC2 instances that can help you reduce spending are Reserved Instances and Spot instances. Reserved Instances Amazon EC2 Reserved Instances allow you to reserve Amazon EC2 computing capacity in exchange for a significantly discounted hourly rate compared to On-Demand instance pricing. This is ideal for applications with predictable minimum capacity requirements. You can take advantage of tools such as AWS Trusted Advisor or Amazon EC2 usage reports to identify the compute resources that you use most often and that you should consider reserving. Depending on your Reserved Instance purchases, the discounts will be reflected in the monthly bill. There is technically no difference between an On-Demand EC2 instance and a Reserved Instance. The difference lies in the way you pay for instances that you reserve. Reserved capacity options exist for other services as well (e.g., Amazon Redshift, Amazon RDS, Amazon DynamoDB, and Amazon CloudFront). Archived Tip: You should not commit to Reserved Instance purchases before you have sufficiently benchmarked your application in production. After you Page 34 Amazon Web Services – Architecting for the Cloud: AWS Best Practices have purchased reserved capacity, you can use the Reserved Instance utilization reports to make sure you are still making the most of your reserved capacity. Spot Instances For less steady workloads, consider using Spot instances. Amazon EC2 Spot instances allow you to use spare Amazon EC2 computing capacity. Since Spot instances are often available at a discount compared to On-Demand pricing, you can significantly reduce the cost of running your applications. Spot instances enable you to request unused EC2 instances, which can lower your Amazon EC2 costs significantly. The hourly price for a Spot instance (of each instance type in each Availability Zone) is set by Amazon EC2, and adjusted gradually based on the long-term supply of, and demand for, Spot instances. Your Spot instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. As a result, Spot instances are great for workloads that can tolerate interruption. You can, however, also use Spot instances when you require more predictable availability. For example, you can combine Reserved, On-Demand, and Spot instances to combine a predictable minimum capacity with opportunistic access to additional compute resources, depending on the Spot market price. This is a great, cost-effective way to improve throughput or application performance. Archived Page 35 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Caching Caching is a technique that stores previously calculated data for future use. This technique is used to improve application performance and increase the cost efficiency of an implementation. It can be applied at multiple layers of an IT architecture. Application Data Caching Applications can be designed so that they store and retrieve information from fast, managed, in-memory caches. Cached information might include the results of I/O intensive database queries, or the outcome of computationally intensive processing. When the result set is not found in the cache, the application can calculate it, or retrieve it from a database or expensive, slowly mutating third-party content, and store it in the cache for subsequent requests. However, when a result set is found in the cache, the application can use that result directly, which improves latency for end users and reduces load on back-end systems. Your application can control how long each cached item remains valid. In some cases, even a few seconds of caching for very popular objects can result in a dramatic decrease on the load for your database. Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. It supports two open-source, in-memory caching engines: Memcached and Redis. For more details on how to select the right engine for your workload, as well as a description of common ElastiCache design patterns, see the Performance at Scale with Amazon ElastiCache whitepaper.50 Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers performance improvements from milliseconds to microseconds, for high throughput. DAX adds in-memory acceleration to your DynamoDB tables without requiring you to manage cache invalidation, data population, or cluster management. Edge Caching Copies of static content (images, CSS files, or streaming pre-recorded video) and dynamic content (responsive HTML, live video) can be cached at an Amazon CloudFront edge location, which is a CDN with multiple points of presence around the world. Edge caching allows content to be served by infrastructure that is closer to Archived Page 36 Amazon Web Services – Architecting for the Cloud: AWS Best Practices viewers, which lowers latency and gives you the high, sustained data transfer rates necessary to deliver large popular objects to end users at scale. Requests for your content are intelligently routed to Amazon S3 or your origin servers. If the origin is running on AWS, requests are transferred over optimized network paths for a more reliable and consistent experience. You can use Amazon CloudFront to deliver your entire website, including non-cachable content. In this case, the benefit is that Amazon CloudFront reuses existing connections between the Amazon CloudFront edge and the origin server, which reduces connection setup latency for each origin request. Other connection optimizations are also applied to avoid internet bottlenecks and fully use available bandwidth between the edge location and the viewer. This means that Amazon CloudFront can expedite the delivery of your dynamic content and provide your viewers with a consistent and reliable, yet personalized, experience when navigating your web application. Amazon CloudFront also applies the same performance benefits to upload requests as those applied to the requests for downloading dynamic content. Security Most of the security tools and techniques that you might already be familiar with in a traditional IT infrastructure can be used in the cloud. At the same time, AWS allows you to improve your security in a variety of ways. AWS is a platform that allows you to formalize the design of security controls in the platform itself. It simplifies system use for administrators and your IT department, and makes your environment much easier to audit in a continuous manner. For a detailed view on how you can achieve a high level of security governance, see the Security at Scale: Governance in AWS51 and the AWS Security Best Practices whitepapers.52 Use AWS Features for Defense in Depth AWS provides many features that can help you build architectures that feature defense in depth methods. Starting at the network level, you can build a VPC topology that isolates parts of the infrastructure through the use of subnets, security groups, and routing controls. Services like AWS WAF, a web application firewall, can help protect your web applications from SQL injection and other vulnerabilities in your application code. For access control, you can use IAM to define a granular set of policies and assign them to users, groups, and AWS resources. Finally, the AWS Cloud offers many options to protect your data, whether it is in transit or at rest with Archived Page 37 Amazon Web Services – Architecting for the Cloud: AWS Best Practices encryption.53 For more information about all of the available AWS security features, see the AWS Cloud Security page on the AWS website.54 Share Security Responsibility with AWS AWS operates under a shared security responsibility model: AWS is responsible for the security of the underlying cloud infrastructure and you are responsible for securing the workloads you deploy in AWS. This helps you to reduce the scope of your responsibility and focus on your core competencies through the use of AWS managed services. For example, when you use services such as Amazon RDS and Amazon ElastiCache, security patches are applied automatically to your configuration settings. This not only reduces operational overhead for your team, but it could also reduce your exposure to vulnerabilities. Reduce Privileged Access When your servers are programmable resources, you get many security benefits. The ability to change your servers whenever you need to enables you to eliminate the need for guest operating system access to production environments. If an instance experiences an issue, you can automatically or manually terminate and replace it. However, before you replace instances, you should collect and centrally store log data that can help you recreate issues in your development environment and deploy them as fixes through your continuous deployment process. This approach ensures that the log data assist with troubleshooting and raise awareness of security events. This is particularly important in an elastic compute environment where servers are temporary. You can use Amazon CloudWatch Logs to collect this information. Where you don’t have direct access, you can implement services such as AWS Systems Manager 55 to take a unified view and automate actions on groups of resources. You can integrate these requests with your ticketing system, so that access requests are tracked and dynamically handled only after approval. Another common security risk is the use of stored, long term credentials or service accounts. In a traditional environment, service accounts are often assigned long-term credentials that are stored in a configuration file. On AWS, you can instead use IAM roles to grant permissions to applications running on EC2 instances through the use of short-term credentials, which are automatically distributed and rotated. For mobile applications, you can use Amazon Cognito to allow client devices to access AWS resources through temporary tokens with fine-grained permissions. As an AWS Archived Page 38 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Management Console user, you can similarly provide federated access through temporary tokens instead of creating IAM users in your AWS account. Then, when an employee leaves your organization and is removed from your organization’s identity directory, that employee also automatically loses access to your AWS accounts. Security as Code Traditional security frameworks, regulations, and organizational policies define security requirements related to items such as firewall rules, network access controls, internal/external subnets, and operating system hardening. You can implement these in an AWS environment as well, but you now have the opportunity to capture them all in a template that defines a Golden Environment. This template is used by AWS CloudFormation and deploys your resources in alignment with your security policy. You can reuse security best practices among multiple projects, as a part of your continuous integration pipeline. You can perform security testing as part of your release cycle, and automatically discover application gaps and drift from your security policy. Additionally, for greater control and security, AWS CloudFormation templates can be imported as products into AWS Service Catalog.56 This allows you to centrally manage your resources to support consistent governance, security, and compliance requirements, while enabling your users to quickly deploy only the approved IT services they need. You apply IAM permissions to control who can view and modify your products, and you define constraints to restrict the ways that specific AWS resources can be deployed for a product. Real-Time Auditing Testing and auditing your environment is key to moving fast while staying safe. Traditional approaches that involve periodic (and often manual or sample-based) checks are not sufficient, especially in agile environments where change is constant. On AWS, you can implement continuous monitoring and automation of controls to minimize exposure to security risks. Services such as AWS Config, Amazon Inspector, and AWS Trusted Advisor continually monitor for compliance or vulnerabilities, giving you a clear overview of which IT resources are in compliance, and which are not. With AWS Config rules you also know if a resource was out of compliance even for a brief period of time, making both point-in-time and period-in-time audits very effective. You Archived Page 39 Amazon Web Services – Architecting for the Cloud: AWS Best Practices can implement extensive logging for your applications (using Amazon CloudWatch Logs) and for the actual AWS API calls by enabling AWS CloudTrail.57 AWS CloudTrail is a web service that records API calls to supported AWS services in your AWS account and delivers a log file to your S3 bucket. Log data can then be stored in an immutable manner and automatically processed to either send a notification or take an action on your behalf, protecting your organization from non compliance. You can use AWS Lambda, Amazon EMR, Amazon ES, Amazon Athena, or third-party tools from AWS Marketplace to scan log data to detect events such as unused permissions, privileged account overuse, key usage, anomalous logins, policy violations, and system abuse. Archived Page 40 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Conclusion When you design your architecture in the AWS Cloud, it is important to consider the important principles and design patterns available in AWS, including how to select the right database for your application, and how to architect applications that can scale horizontally and with high availability. Because each implementation is unique, you must evaluate how to apply this guidance to your implementation. The topic of cloud computing architectures is broad and continuously evolving. You can stay up-to-date with the latest changes and additions to the AWS cloud offerings with the material available on the AWS website58 and the AWS training and certification offerings.59 Contributors These individuals contributed to this document: • Andreas Chatzakis, Manager, AWS Solutions Architecture • Paul Armstrong, Principal Solutions Architect, AWS Further Reading For more architecture examples, see the AWS Architecture Center.60 For applications already running on AWS, we recommend you review the AWS Well Architected Framework whitepaper, which provides a structured evaluation model.61 For information to help you validate your operational readiness, see the comprehensive AWS Operational Checklist.62 Archived Page 41 Amazon Web Services – Architecting for the Cloud: AWS Best Practices Document Revisions Date Description October 2018 Document review and update June 2013 First publication Notes 1http://d0.awsstatic.com/whitepapers/architecture/AWS_Well Architected_Framework.pdf 2https://aws.amazon.com/about-aws/ 3https://aws.amazon.com/about-aws/global-infrastructure/ 4For example, see the PHP Amazon DynamoDB session handler (http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/dynamodb-session handler.html) and the Tomcat Amazon DynamoDB session handler (http://docs.aws.amazon.com/AWSSdkDocsJava/latest//DeveloperGuide/java-dg tomcat-session-manager.html) 5https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer target-groups.html#sticky-sessions 6https://d0.awsstatic.com/whitepapers/Big_Data_Analytics_Options_on_AWS.pdf 7https://d1.awsstatic.com/whitepapers/core-tenets-of-iot1.pdf 8http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance metadata.html 9http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template custom-resources-lambda.html 10 http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html 11 http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html Archived Page 42 Amazon Web Services – Architecting for the Cloud: AWS Best Practices 12 http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html 13 https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf 14 https://d0.awsstatic.com/whitepapers/managing-your-aws-infrastructure-at scale.pdf 15 https://d0.awsstatic.com/whitepapers/DevOps/infrastructure-as-code.pdf 16 https://docs.aws.amazon.com/lambda/latest/dg/automating-deployment.html 17 https://aws.amazon.com/elasticbeanstalk/ 18 http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html 19 https://aws.amazon.com/ec2/systems-manager/ 20 https://aws.amazon.com/autoscaling/ 21 22 http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/AlarmTha tSendsEmail.html http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCl oudWatchEvents.html 23 http://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html 24 https://aws.amazon.com/answers/security/aws-waf-security-automations/ 25 https://www.awsarchitectureblog.com/2015/03/backoff.html 26 http://aws.amazon.com/products/ 28 https://d0.awsstatic.com/whitepapers/AWS_Serverless_Multi Tier_Architectures.pdf 29 http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html 30 http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/ CHAP_BestPractices.html Archived Page 43 Amazon Web Services – Architecting for the Cloud: AWS Best Practices 31 https://aws.amazon.com/nosql/ 32 https://aws.amazon.com/dynamodb/dax/ 33 http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BestPractice s.html 34 https://d0.awsstatic.com/whitepapers/migration-best-practices-rdbms-to dynamodb.pdf 35 https://aws.amazon.com/redshift/faqs/ 36 https://aws.amazon.com/documentation/cloudsearch/ 37 https://aws.amazon.com/documentation/elasticsearch-service/ 38 https://aws.amazon.com/neptune/ 39 https://d0.awsstatic.com/whitepapers/Storage/data-lake-on-aws.pdf 40 https://d0.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf 41 http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html 42 http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html 43 http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html 44 https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf 45 https://aws.amazon.com/architecture/well-architected/ 46 http://www.awsarchitectureblog.com/2014/04/shuffle-sharding.html 47 https://d0.awsstatic.com/whitepapers/Cost_Optimization_with_AWS.pdf 48 http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ monitoring costs.html 49 http://docs.aws.amazon.com/AmazonCloudWatch/latest/Developer Guide/UsingAlarmActions.html Archived Page 44 Amazon Web Services – Architecting for the Cloud: AWS Best Practices 50 https://d0.awsstatic.com/whitepapers/performance-at-scale-with-amazon elasticache.pdf 51 https://d0.awsstatic.com/whitepapers/compliance/AWS_Security _at_Scale_Governance_in_AWS_Whitepaper.pdf 52 https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf 53 https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf 54 http://aws.amazon.com/security 55 https://aws.amazon.com/systems-manager/ 56 https://aws.amazon.com/servicecatalog/ 57 https://d0.awsstatic.com/whitepapers/compliance/AWS_Security _at_Scale_Logging_in_AWS_Whitepaper.pdf 58 https://aws.amazon.com/ 59 https://aws.amazon.com/training/ 60 https://aws.amazon.com/architecture 61 http://d0.awsstatic.com/whitepapers/architecture/AWS_Well Architected_Framework.pdf 62 https://d0.awsstatic.com/whitepapers/aws-operational-checklists.pdf Archived Page 45  
Amazon Web Services: Overview of Security Processes March 2020 This paper has been archived. For the latest technical content on Security and Compliance, see https://aws.amazon.com/ architecture/security-identity compliance/ Archived Notices Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. © 2020 Amazon Web Services, Inc. or its affiliates. All rights reserved. Archived Archived Contents Introduction .......................................................................................................................... 1 Shared Security Responsibility Model ................................................................................ 1 AWS Security Responsibilities ......................................................................................... 2 Customer Security Responsibilities ................................................................................. 2 AWS Global Infrastructure Security .................................................................................... 3 AWS Compliance Program .............................................................................................. 3 Physical and Environmental Security .............................................................................. 4 Business Continuity Management ................................................................................... 6 Network Security .............................................................................................................. 7 AWS Access ................................................................................................................... 11 Secure Design Principles ............................................................................................... 12 Change Management..................................................................................................... 12 AWS Account Security Features ................................................................................... 14 Individual User Accounts ............................................................................................... 19 Secure HTTPS Access Points ....................................................................................... 19 Security Logs .................................................................................................................. 20 AWS Trusted Advisor Security Checks ......................................................................... 20 AWS Config Security Checks ........................................................................................ 21 AWS Service-Specific Security ......................................................................................... 21 Compute Services .......................................................................................................... 21 Networking Services ...................................................................................................... 28 Storage Services ............................................................................................................ 43 Database Services ......................................................................................................... 55 Application Services ....................................................................................................... 66 Analytics Services .......................................................................................................... 73 Deployment and Management Services ....................................................................... 77 Archived Mobile Services .............................................................................................................. 82 Applications .................................................................................................................... 85 Document Revisions.......................................................................................................... 88 Abstract This document is intended to answer questions, such as How does AWS help me ensure that my data is secure? Specifically, this paper describes AWS physical and operational security processes for the network and server infrastructure under the management of AWS. Archived Amazon Web Services Amazon Web Services: Overview of Security Processes Introduction Amazon Web Services (AWS) delivers a scalable cloud computing platform with high availability and dependability, providing the tools that enable customers to run a wide range of applications. Helping to protect the confidentiality, integrity, and availability of our customers’ systems and data is of the utmost importance to AWS, as is maintaining customer trust and confidence. Shared Security Responsibility Model Before covering the details of how AWS secures its resources, it is important to understand how security in the cloud is slightly different than security in your on premises data centers. When you move computer systems and data to the cloud, security responsibilities become shared between you and your cloud service provider. In this case, AWS is responsible for securing the underlying infrastructure that supports the cloud, and you’re responsible for anything you put on the cloud or connect to the cloud. This shared security responsibility model can reduce your operational burden in many ways, and in some cases may even improve your default security posture without additional action on your part. Figure 1: AWS shared security responsibility model Archived The amount of security configuration work you have to do varies depending on which services you select and how sensitive your data is. However, there are certain security Page 1 Amazon Web Services Amazon Web Services: Overview of Security Processes features—such as individual user accounts and credentials, SSL/TLS for data transmissions, and user activity logging—that you should configure no matter which AWS service you use. For more information about these security features, see the AWS Account Security Features section. AWS Security Responsibilities Amazon Web Services is responsible for protecting the global infrastructure that runs all of the services offered in the AWS Cloud. This infrastructure comprises the hardware, software, networking, and facilities that run AWS services. Protecting this infrastructure is the number one priority of AWS. Although, you can’t visit our data centers or offices to see this protection firsthand, we provide several reports from third-party auditors who have verified our compliance with a variety of computer security standards and regulations. For more information, visit AWS Compliance. Note that in addition to protecting this global infrastructure, AWS is responsible for the security configuration of its products that are considered managed services. Examples of these types of services include Amazon DynamoDB, Amazon RDS, Amazon Redshift, Amazon EMR, Amazon WorkSpaces, and several other services. These services provide the scalability and flexibility of cloud-based resources with the additional benefit of being managed. For these services, AWS handles basic security tasks like guest operating system (OS) and database patching, firewall configuration, and disaster recovery. For most of these managed services, all you have to do is configure logical access controls for the resources and protect your account credentials. A few of them may require additional tasks, such as setting up database user accounts, but overall the security configuration work is performed by the service. Customer Security Responsibilities With the AWS cloud, you can provision virtual servers, storage, databases, and desktops in minutes instead of weeks. You can also use cloud-based analytics and workflow tools to process your data as you need it, and then store it in your own data centers or in the cloud. The AWS services that you use determine how much configuration work you have to perform as part of your security responsibilities. AWS products that fall into the well-understood category of Infrastructure-as-a-Service (IaaS)—such as Amazon EC2, Amazon VPC, and Amazon S3—are completely under your control and require you to perform all of the necessary security configuration and management tasks. For example, for EC2 instances, you’re responsible for management of the guest OS (including updates and security patches), any application Archived Page 2 Amazon Web Services Amazon Web Services: Overview of Security Processes software or utilities you install on the instances, and the configuration of the AWS provided firewall (called a security group) on each instance. These are basically the same security tasks that you’re used to performing no matter where your servers are located. AWS managed services like Amazon RDS or Amazon Redshift provide all of the resources you need to perform a specific task—but without the configuration work that can come with them. With managed services, you don’t have to worry about launching and maintaining instances, patching the guest OS or database, or replicating databases—AWS handles that for you. But as with all services, you should protect your AWS Account credentials and set up individual user accounts with Amazon Identity and Access Management (IAM) so that each of your users has their own credentials and you can implement segregation of duties. We also recommend using multi-factor authentication (MFA) with each account, requiring the use of SSL/TLS to communicate with your AWS resources, and setting up API/user activity logging with AWS CloudTrail. For more information about additional measures you can take, refer to the AWS Security Best Practices whitepaper and recommended reading on the AWS Security Learning webpage. AWS Global Infrastructure Security AWS operates the global cloud infrastructure that you use to provision a variety of basic computing resources such as processing and storage. The AWS global infrastructure includes the facilities, network, hardware, and operational software (e.g., host OS, virtualization software, etc.) that support the provisioning and use of these resources. The AWS global infrastructure is designed and managed according to security best practices as well as a variety of security compliance standards. As an AWS customer, you can be assured that you’re building web architectures on top of some of the most secure computing infrastructure in the world. AWS Compliance Program AWS Compliance enables customers to understand the robust controls in place at AWS to maintain security and data protection in the cloud. As systems are built on top of AWS cloud infrastructure, compliance responsibilities are shared. By tying together governance-focused, audit friendly service features with applicable compliance or audit standards, AWS Compliance enablers build on traditional programs; helping customers to establish and operate in an AWS security control environment. The IT infrastructure Archived Page 3 Amazon Web Services Amazon Web Services: Overview of Security Processes that AWS provides to its customers is designed and managed in alignment with security best practices and a variety of IT security standards, including: • SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70) • SOC 2 • SOC 3 • FISMA, DIACAP, and FedRAMP • DOD CSM Levels 1-5 • PCI DSS Level 1 • ISO 9001 / ISO 27001 / ISO 27017 / ISO 27018 • ITAR • FIPS 140-2 • MTCS Level 3 • HITRUST In addition, the flexibility and control that the AWS platform provides allows customers to deploy solutions that meet several industry-specific standards, including: • Criminal Justice Information Services (CJIS) • Cloud Security Alliance (CSA) • Family Educational Rights and Privacy Act (FERPA) • Health Insurance Portability and Accountability Act (HIPAA) • Motion Picture Association of America (MPAA) AWS provides a wide range of information regarding its IT control environment to customers through white papers, reports, certifications, accreditations, and other third party attestations. For more information, see AWS Compliance. Physical and Environmental Security Archived AWS data centers are state of the art, utilizing innovative architectural and engineering approaches. Amazon has many years of experience in designing, constructing, and operating large-scale data centers. This experience has been applied to the AWS platform and infrastructure. AWS data centers are housed in facilities that are not Page 4 Amazon Web Services Amazon Web Services: Overview of Security Processes branded as AWS facilities. Physical access is strictly controlled both at the perimeter and at building ingress points by professional security staff utilizing video surveillance, intrusion detection systems, and other electronic means. Authorized staff must pass two-factor authentication a minimum of two times to access data center floors. All visitors are required to present identification and are signed in and continually escorted by authorized staff. AWS only provides data center access and information to employees and contractors who have a legitimate business need for such privileges. When an employee no longer has a business need for these privileges, his or her access is immediately revoked, even if they continue to be an employee of Amazon or Amazon Web Services. All physical access to data centers by AWS employees is logged and audited routinely. Fire Detection and Suppression Automatic fire detection and suppression equipment has been installed to reduce risk. The fire detection system utilizes smoke detection sensors in all data center environments, mechanical and electrical infrastructure spaces, chiller rooms and generator equipment rooms. These areas are protected by either wet-pipe, double interlocked pre-action, or gaseous sprinkler systems. Power The data center electrical power systems are designed to be fully redundant and maintainable without impact to operations, 24 hours a day, and seven days a week. Uninterruptible Power Supply (UPS) units provide back-up power in the event of an electrical failure for critical and essential loads in the facility. Data centers use generators to provide back-up power for the entire facility. Climate and Temperature Climate control is required to maintain a constant operating temperature for servers and other hardware, which prevents overheating and reduces the possibility of service outages. Data centers are conditioned to maintain atmospheric conditions at optimal levels. Personnel and systems monitor and control temperature and humidity at appropriate levels. Archived Page 5 Amazon Web Services Amazon Web Services: Overview of Security Processes Management AWS monitors electrical, mechanical, and life support systems and equipment so that any issues are immediately identified. Preventative maintenance is performed to maintain the continued operability of equipment. Storage Device Decommissioning When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process that is designed to prevent customer data from being exposed to unauthorized individuals. AWS uses the techniques detailed in NIST 800-88 (“Guidelines for Media Sanitization”) as part of the decommissioning process. Business Continuity Management Amazon’s infrastructure has a high level of availability and provides customers the features to deploy a resilient IT architecture. AWS has designed its systems to tolerate system or hardware failures with minimal customer impact. Data center Business Continuity Management at AWS is under the direction of the Amazon Infrastructure Group. Availability Data centers are built in clusters in various global regions. All data centers are online and serving customers; no data center is “cold.” In case of failure, automated processes move customer data traffic away from the affected area. Core applications are deployed in an N+1 configuration, so that in the event of a data center failure, there is sufficient capacity to enable traffic to be load- balanced to the remaining sites. AWS provides you with the flexibility to place instances and store data within multiple geographic regions as well as across multiple availability zones within each region. Each availability zone is designed as an independent failure zone. This means that availability zones are physically separated within a typical metropolitan region and are located in lower risk flood plains (specific flood zone categorization varies by Region). In addition to discrete uninterruptable power supply (UPS) and onsite backup generation facilities, they are each fed via different grids from independent utilities to further reduce single points of failure. Availability zones are all redundantly connected to multiple tier-1 transit providers. Archived You should architect your AWS usage to take advantage of multiple regions and availability zones. Distributing applications across multiple availability zones provides Page 6 Amazon Web Services Amazon Web Services: Overview of Security Processes the ability to remain resilient in the face of most failure modes, including natural disasters or system failures. Incident Response The Amazon Incident Management team employs industry-standard diagnostic procedures to drive resolution during business-impacting events. Staff operators provide 24x7x365 coverage to detect incidents and to manage the impact and resolution. Company-Wide Executive Review Amazon’s Internal Audit group has recently reviewed the AWS services resiliency plans, which are also periodically reviewed by members of the Senior Executive management team and the Audit Committee of the Board of Directors. Communication AWS has implemented various methods of internal communication at a global level to help employees understand their individual roles and responsibilities and to communicate significant events in a timely manner. These methods include orientation and training programs for newly hired employees; regular management meetings for updates on business performance and other matters; and electronics means such as video conferencing, electronic mail messages, and the posting of information via the Amazon intranet. AWS has also implemented various methods of external communication to support its customer base and the community. Mechanisms are in place to allow the customer support team to be notified of operational issues that impact the customer experience. A Service Health Dashboard is available and maintained by the customer support team to alert customers to any issues that may be of broad impact. The AWS Cloud Security Center is available to provide you with security and compliance details about AWS. You can also subscribe to AWS Support offerings that include direct communication with the customer support team and proactive alerts to any customer impacting issues. Network Security The AWS network has been architected to permit you to select the level of security and resiliency appropriate for your workload. To enable you to build geographically dispersed, fault-tolerant web architectures with cloud resources, AWS has implemented a world-class network infrastructure that is carefully monitored and managed. Archived Page 7 Amazon Web Services Amazon Web Services: Overview of Security Processes Secure Network Architecture Network devices, including firewall and other boundary devices, are in place to monitor and control communications at the external boundary of the network and at key internal boundaries within the network. These boundary devices employ rule sets, access control lists (ACL), and configurations to enforce the flow of information to specific information system services. ACLs, or traffic flow policies, are established on each managed interface, which manage and enforce the flow of traffic. ACL policies are approved by Amazon Information Security. These policies are automatically pushed using AWS’s ACL- Manage tool, to help ensure these managed interfaces enforce the most up-to-date ACLs. Secure Access Points AWS has strategically placed a limited number of access points to the cloud to allow for a more comprehensive monitoring of inbound and outbound communications and network traffic. These customer access points are called API endpoints, and they allow secure HTTP access (HTTPS), which allows you to establish a secure communication session with your storage or compute instances within AWS. To support customers with FIPS cryptographic requirements, the SSL-terminating load balancers in AWS GovCloud (US) are FIPS 140-2-compliant. In addition, AWS has implemented network devices that are dedicated to managing interfacing communications with Internet service providers (ISPs). AWS employs a redundant connection to more than one communication service at each Internet-facing edge of the AWS network. These connections each have dedicated network devices. Transmission Protection You can connect to an AWS access point via HTTP or HTTPS using Secure Sockets Layer (SSL), a cryptographic protocol that is designed to protect against eavesdropping, tampering, and message forgery. For customers who require additional layers of network security, AWS offers the Amazon Virtual Private Cloud (VPC), which provides a private subnet within the AWS cloud, and the ability to use an IPsec Virtual Private Network (VPN) device to provide an encrypted tunnel between the Amazon VPC and your data center. For more information about VPC configuration options, see the Amazon Virtual Private Cloud (Amazon VPC) Security section. Archived Page 8 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon Corporate Segregation Logically, the AWS Production network is segregated from the Amazon Corporate network by means of a complex set of network security / segregation devices. AWS developers and administrators on the corporate network who need to access AWS cloud components in order to maintain them must explicitly request access through the AWS ticketing system. All requests are reviewed and approved by the applicable service owner. Approved AWS personnel then connect to the AWS network through a bastion host that restricts access to network devices and other cloud components, logging all activity for security review. Access to bastion hosts require SSH public- key authentication for all user accounts on the host. For more information on AWS developer and administrator logical access, see AWS Access below. Fault-Tolerant Design Amazon’s infrastructure has a high level of availability and provides you with the capability to deploy a resilient IT architecture. AWS has designed its systems to tolerate system or hardware failures with minimal customer impact. Data centers are built in clusters in various global regions. All data centers are online and serving customers; no data center is “cold.” In case of failure, automated processes move customer data traffic away from the affected area. Core applications are deployed in an N+1 configuration, so that in the event of a data center failure, there is sufficient capacity to enable traffic to be load-balanced to the remaining sites. AWS provides you with the flexibility to place instances and store data within multiple geographic regions as well as across multiple availability zones within each region. Each availability zone is designed as an independent failure zone. This means that availability zones are physically separated within a typical metropolitan region and are located in lower risk flood plains (specific flood zone categorization varies by region). In addition to utilizing discrete uninterruptable power supply (UPS) and onsite backup generators, they are each fed via different grids from independent utilities to further reduce single points of failure. Availability zones are all redundantly connected to multiple tier-1 transit providers. You should architect your AWS usage to take advantage of multiple regions and availability zones. Distributing applications across multiple availability zones provides the ability to remain resilient in the face of most failure scenarios, including natural disasters or system failures. However, you should be aware of location-dependent Archived Page 9 Amazon Web Services Amazon Web Services: Overview of Security Processes privacy and compliance requirements, such as the EU Data Privacy Directive. Data is not replicated between regions unless proactively done so by the customer, thus allowing customers with these types of data placement and privacy requirements the ability to establish compliant environments. It should be noted that all communications between regions is across public internet infrastructure; therefore, appropriate encryption methods should be used to protect sensitive data. Data centers are built in clusters in various global regions, including: US East (Northern Virginia), US West (Oregon), US West (Northern California), AWS GovCloud (US) (Oregon), EU (Frankfurt), EU (Ireland), Asia Pacific (Seoul) Asia Pacific (Singapore), Asia Pacific (Tokyo), Asia Pacific (Sydney), China (Beijing), and South America (Sao Paulo). For a complete list of AWS Regions, see the AWS Global Infrastructure page. AWS GovCloud (US) is an isolated AWS Region designed to allow US government agencies and customers to move workloads into the cloud by helping them meet certain regulatory and compliance requirements. The AWS GovCloud (US) framework allows US government agencies and their contractors to comply with U.S. International Traffic in Arms Regulations (ITAR) regulations as well as the Federal Risk and Authorization Management Program (FedRAMP) requirements. AWS GovCloud (US) has received an Agency Authorization to Operate (ATO) from the US Department of Health and Human Services (HHS) utilizing a FedRAMP accredited Third Party Assessment Organization (3PAO) for several AWS services. The AWS GovCloud (US) Region provides the same fault-tolerant design as other regions, with two Availability Zones. In addition, the AWS GovCloud (US) region is a mandatory AWS Virtual Private Cloud (VPC) service by default to create an isolated portion of the AWS cloud and launch Amazon EC2 instances that have private (RFC 1918) addresses. For more information, see AWS GovCloud (US). Network Monitoring and Protection AWS uses a wide variety of automated monitoring systems to provide a high level of service performance and availability. AWS monitoring tools are designed to detect unusual or unauthorized activities and conditions at ingress and egress communication points. These tools monitor server and network usage, port scanning activities, application usage, and unauthorized intrusion attempts. The tools have the ability to set custom performance metrics thresholds for unusual activity. Archived Systems within AWS are extensively instrumented to monitor key operational metrics. Alarms are configured to automatically notify operations and management personnel when early warning thresholds are crossed on key operational metrics. An on-call Page 10 Amazon Web Services Amazon Web Services: Overview of Security Processes schedule is used so personnel are always available to respond to operational issues. This includes a pager system so alarms are quickly and reliably communicated to operations personnel. Documentation is maintained to aid and inform operations personnel in handling incidents or issues. If the resolution of an issue requires collaboration, a conferencing system is used which supports communication and logging capabilities. Trained call leaders facilitate communication and progress during the handling of operational issues that require collaboration. Post-mortems are convened after any significant operational issue, regardless of external impact, and Cause of Error (COE) documents are drafted so the root cause is captured and preventative actions are taken in the future. Implementation of the preventative measures is tracked during weekly operations meetings. AWS Access The AWS Production network is segregated from the Amazon Corporate network and requires a separate set of credentials for logical access. The Amazon Corporate network relies on user IDs, passwords, and Kerberos, whereas the AWS Production network requires SSH public-key authentication through a bastion host. AWS developers and administrators on the Amazon Corporate network who need to access AWS cloud components must explicitly request access through the AWS access management system. All requests are reviewed and approved by the appropriate owner or manager. Account Review and Audit Accounts are reviewed every 90 days; explicit re-approval is required or access to the resource is automatically revoked. Access is also automatically revoked when an employee’s record is terminated in Amazon’s Human Resources system. Windows and UNIX accounts are disabled and Amazon’s permission management system removes the user from all systems. Requests for changes in access are captured in the Amazon permissions management tool audit log. When changes in an employee’s job function occur, continued access must be explicitly approved to the resource or it will be automatically revoked. Archived Page 11 Amazon Web Services Amazon Web Services: Overview of Security Processes Background Checks AWS has established formal policies and procedures to delineate the minimum standards for logical access to AWS platform and infrastructure hosts. AWS conducts criminal background checks, as permitted by law, as part of pre- employment screening practices for employees and commensurate with the employee’s position and level of access. The policies also identify functional responsibilities for the administration of logical access and security. Credentials Policy AWS Security has established a credentials policy with required configurations and expiration intervals. Passwords must be complex and are forced to be changed every 90 days. Secure Design Principles The AWS development process follows secure software development best practices, which include formal design reviews by the AWS Security Team, threat modeling, and completion of a risk assessment. Static code analysis tools are run as a part of the standard build process, and all deployed software undergoes recurring penetration testing performed by carefully selected industry experts. Our security risk assessment reviews begin during the design phase and the engagement lasts through launch to ongoing operations. Change Management Routine, emergency, and configuration changes to existing AWS infrastructure are authorized, logged, tested, approved, and documented in accordance with industry norms for similar systems. Updates to the AWS infrastructure are done to minimize any impact on the customer and their use of the services. AWS will communicate with customers, either via email, or through the AWS Service Health Dashboard when service use is likely to be adversely affected. Software AWS applies a systematic approach to managing change so that changes to customer impacting services are thoroughly reviewed, tested, approved, and well-communicated. The AWS change management process is designed to avoid unintended service disruptions and to maintain the integrity of service to the customer. Changes deployed into production environments are: Archived Page 12 Amazon Web Services Amazon Web Services: Overview of Security Processes • Reviewed – Peer reviews of the technical aspects of a change are required. • Tested – Changes being applied are tested to help ensure they will behave as expected and not adversely impact performance. • Approved – All changes must be authorized in order to provide appropriate oversight and understanding of business impact. Changes are typically pushed into production in a phased deployment starting with lowest impact areas. Deployments are tested on a single system and closely monitored so impacts can be evaluated. Service owners have a number of configurable metrics that measure the health of the service’s upstream dependencies. These metrics are closely monitored with thresholds and alarming in place. Rollback procedures are documented in the Change Management (CM) ticket. When possible, changes are scheduled during regular change windows. Emergency changes to production systems that require deviations from standard change management procedures are associated with an incident and are logged and approved as appropriate. Periodically, AWS performs self-audits of changes to key services to monitor quality, maintain high standards, and facilitate continuous improvement of the change management process. Any exceptions are analyzed to determine the root cause, and appropriate actions are taken to bring the change into compliance or roll back the change if necessary. Actions are then taken to address and remediate the process or people issue. Infrastructure Amazon’s Corporate Applications team develops and manages software to automate IT processes for UNIX/Linux hosts in the areas of third-party software delivery, internally developed software, and configuration management. The Infrastructure team maintains and operates a UNIX/Linux configuration management framework to address hardware scalability, availability, auditing, and security management. By centrally managing hosts through the use of automated processes that manage change, Amazon is able to achieve its goals of high availability, repeatability, scalability, security, and disaster recovery. Systems and network engineers monitor the status of these automated tools on a continuous basis, reviewing reports to respond to hosts that fail to obtain or update their configuration and software. Archived Page 13 Amazon Web Services Amazon Web Services: Overview of Security Processes Internally developed configuration management software is installed when new hardware is provisioned. These tools are run on all UNIX hosts to validate that they are configured and that software is installed in compliance with standards determined by the role assigned to the host. This configuration management software also helps to regularly update packages that are already installed on the host. Only approved personnel enabled through the permissions service may log in to the central configuration management servers. AWS Account Security Features AWS provides a variety of tools and features that you can use to keep your AWS Account and resources safe from unauthorized use. This includes credentials for access control, HTTPS endpoints for encrypted data transmission, the creation of separate IAM user accounts, user activity logging for security monitoring, and Trusted Advisor security checks. You can take advantage of all of these security tools no matter which AWS services you select. AWS Credentials To help ensure that only authorized users and processes access your AWS Account and resources, AWS uses several types of credentials for authentication. These include passwords, cryptographic keys, digital signatures, and certificates. We also provide the option of requiring multi-factor authentication (MFA) to log into your AWS Account or IAM user accounts. The following table highlights the various AWS credentials and their uses. Table 1: Credential types and uses Credential Type Use Description Passwords AWS root account or IAM user account login to the AWS Management Console A string of characters used to log into your AWS account or IAM account. AWS passwords must be a minimum of 6 characters and may be up to 128 characters. Multi-Factor Authentication (MFA) AWS root account or IAM user account login to the AWS Management Console Archived A six-digit single-use code that is required in addition to your password to log in to your AWS Account or IAM user account. Page 14 Amazon Web Services Amazon Web Services: Overview of Security Processes Credential Type Use Description Access Keys Digitally signed requests to AWS APIs (using the AWS SDK, CLI, or REST/Query APIs) Includes an access key ID and a secret access key. You use access keys to digitally sign programmatic requests that you make to AWS. Key Pairs SSH login to EC2 instances CloudFront signed URLs A key pair is required to connect to an EC2 instance launched from a public AMI. The supported lengths are 1024, 2048, and 4096. If you connect using SSH while using the EC2 Instance Connect API, the supported lengths are 2048 and 4096. You can have a key pair generated automatically for you when you launch the instance or you can upload your own. X.509 Certificates Digitally signed SOAP requests to AWS APIs SSL server certificates for HTTPS X.509 certificates are only used to sign SOAP-based requests (currently used only with Amazon S3). You can have AWS create an X.509 certificate and private key that you can download, or you can upload your own certificate by using the Security Credentials page. You can download a Credential Report for your account at any time from the Security Credentials page. This report lists all of your account’s users and the status of their credentials—whether they use a password, whether their password expires and must be changed regularly, the last time they changed their password, the last time they rotated their access keys, and whether they have MFA enabled. For security reasons, if your credentials have been lost or forgotten, you cannot recover them or re-download them. However, you can create new credentials and then disable or delete the old set of credentials. In fact, AWS recommends that you change (rotate) your access keys and certificates on a regular basis. To help you do this without potential impact to your application’s availability, AWS supports multiple concurrent access keys and certificates. With this feature, you can rotate keys and certificates into and out of operation on a regular basis without any downtime to your application. This can help to mitigate risk from lost or Archived Page 15 Amazon Web Services Amazon Web Services: Overview of Security Processes compromised access keys or certificates. The AWS IAM API enables you to rotate the access keys of your AWS Account as well as for IAM user accounts. Passwords Passwords are required to access your AWS Account, individual IAM user accounts, AWS Discussion Forums, and the AWS Support Center. You specify the password when you first create the account, and you can change it at any time by going to the Security Credentials page. AWS passwords can be up to 128 characters long and contain special characters, so we encourage you to create a strong password that cannot be easily guessed. You can set a password policy for your IAM user accounts to ensure that strong passwords are used and that they are changed often. A password policy is a set of rules that define the type of password an IAM user can set. For more information about password policies, see Managing Passwords for IAM Users. AWS Multi-Factor Authentication (MFA) AWS Multi-Factor Authentication (MFA) is an additional layer of security for accessing AWS services. When you enable this optional feature, you must provide a six-digit single-use code in addition to your standard user name and password credentials before access is granted to your AWS Account settings or AWS services and resources. You get this single-use code from an authentication device that you keep in your physical possession. This is called multi-factor authentication because more than one authentication factor is checked before access is granted: a password (something you know) and the precise code from your authentication device (something you have). You can enable MFA devices for your AWS Account as well as for the users you have created under your AWS Account with AWS IAM. In addition, you add MFA protection for access across AWS Accounts, for when you want to allow a user you’ve created under one AWS Account to use an IAM role to access resources under another AWS Account. You can require the user to use MFA before assuming the role as an additional layer of security. AWS MFA supports the use of both hardware tokens and virtual MFA devices. Virtual MFA devices use the same protocols as the physical MFA devices, but can run on any mobile hardware device, including a smartphone. A virtual MFA device uses a software application that generates six-digit authentication codes that are compatible with the Time-Based One-Time Password (TOTP) standard, as described in RFC 6238. Most virtual MFA applications allow you to host more than one virtual MFA device, which makes them more convenient than hardware MFA devices. However, you should be Archived Page 16 Amazon Web Services Amazon Web Services: Overview of Security Processes aware that because a virtual MFA might be run on a less secure device such as a smartphone, a virtual MFA might not provide the same level of security as a hardware MFA device. You can also enforce MFA authentication for AWS service APIs in order to provide an extra layer of protection over powerful or privileged actions such as terminating Amazon EC2 instances or reading sensitive data stored in Amazon S3. You do this by adding an MFA-authentication requirement to an IAM access policy. You can attach these access policies to IAM users, IAM groups, or resources that support Access Control Lists (ACLs) like Amazon S3 buckets, SQS queues, and SNS topics. It is easy to obtain hardware tokens from a participating third-party provider or virtual MFA applications from an AppStore and to set it up for use via the AWS website. More information is available at AWS Multi-Factor Authentication (MFA). Access Keys AWS requires that all API requests be signed—that is, they must include a digital signature that AWS can use to verify the identity of the requestor. You calculate the digital signature using a cryptographic hash function. The input to the hash function in this case includes the text of your request and your secret access key. If you use any of the AWS SDKs to generate requests, the digital signature calculation is done for you; otherwise, you can have your application calculate it and include it in your REST or Query requests by following the directions in Making Requests Using the AWS SDKs. Not only does the signing process help protect message integrity by preventing tampering with the request while it is in transit, it also helps protect against potential replay attacks. A request must reach AWS within 15 minutes of the time stamp in the request. Otherwise, AWS denies the request. The most recent version of the digital signature calculation process is Signature Version 4, which calculates the signature using the HMAC-SHA256 protocol. Version 4 provides an additional measure of protection over previous versions by requiring that you sign the message using a key that is derived from your secret access key rather than using the secret access key itself. In addition, you derive the signing key based on credential scope, which facilitates cryptographic isolation of the signing key. Archived Because access keys can be misused if they fall into the wrong hands, we encourage you to save them in a safe place and not embed them in your code. For customers with large fleets of elastically scaling EC2 instances, the use of IAM roles can be a more secure and convenient way to manage the distribution of access keys. IAM roles Page 17 Amazon Web Services Amazon Web Services: Overview of Security Processes provide temporary credentials, which not only get automatically loaded to the target instance, but are also automatically rotated multiple times a day. Key Pairs Amazon EC2 instances created from a public AMI use a public/private key pair rather than a password for signing in via Secure Shell (SSH).The public key is embedded in your instance, and you use the private key to sign in securely without a password. After you create your own AMIs, you can choose other mechanisms to securely log in to your new instances. You can have a key pair generated automatically for you when you launch the instance or you can upload your own. Save the private key in a safe place on your system, and record the location where you saved it. For Amazon CloudFront, you use key pairs to create signed URLs for private content, such as when you want to distribute restricted content that someone paid for. You create Amazon CloudFront key pairs by using the Security Credentials page. CloudFront key pairs can be created only by the root account and cannot be created by IAM users. X.509 Certificates X.509 certificates are used to sign SOAP-based requests. X.509 certificates contain a public key and additional metadata (like an expiration date that AWS verifies when you upload the certificate), and is associated with a private key. When you create a request, you create a digital signature with your private key and then include that signature in the request, along with your certificate. AWS verifies that you're the sender by decrypting the signature with the public key that is in your certificate. AWS also verifies that the certificate you sent matches the certificate that you uploaded to AWS. For your AWS Account, you can have AWS create an X.509 certificate and private key that you can download, or you can upload your own certificate by using the Security Credentials page. For IAM users, you must create the X.509 certificate (signing certificate) by using third-party software. In contrast with root account credentials, AWS cannot create an X.509 certificate for IAM users. After you create the certificate, you attach it to an IAM user by using IAM. Archived In addition to SOAP requests, X.509 certificates are used as SSL/TLS server certificates for customers who want to use HTTPS to encrypt their transmissions. To use them for HTTPS, you can use an open-source tool like OpenSSL to create a unique Page 18 Amazon Web Services Amazon Web Services: Overview of Security Processes private key. You’ll need the private key to create the Certificate Signing Request (CSR) that you submit to a certificate authority (CA) to obtain the server certificate. You’ll then use the AWS CLI to upload the certificate, private key, and certificate chain to IAM. You’ll also need an X.509 certificate to create a customized Linux AMI for EC2 instances. The certificate is only required to create an instance-backed AMI (as opposed to an EBS-backed AMI). You can have AWS create an X.509 certificate and private key that you can download, or you can upload your own certificate by using the Security Credentials page. Individual User Accounts AWS provides a centralized mechanism called AWS Identity and Access Management (IAM) for creating and managing individual users within your AWS Account. A user can be any individual, system, or application that interacts with AWS resources, either programmatically or through the AWS Management Console or AWS Command Line Interface (CLI). Each user has a unique name within the AWS Account, and a unique set of security credentials not shared with other users. AWS IAM eliminates the need to share passwords or keys, and enables you to minimize the use of your AWS Account credentials. With IAM, you define policies that control which AWS services your users can access and what they can do with them. You can grant users only the minimum permissions they need to perform their jobs. See the AWS Identity and Access Management (AWS IAM) section for more information. Secure HTTPS Access Points For greater communication security when accessing AWS resources, you should use HTTPS instead of HTTP for data transmissions. HTTPS uses the SSL/TLS protocol, which uses public-key cryptography to prevent eavesdropping, tampering, and forgery. All AWS services provide secure customer access points (also called API endpoints) that allow you to establish secure HTTPS communication sessions. Several services also now offer more advanced cipher suites that use the Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) protocol. ECDHE allows SSL/TLS clients to provide Perfect Forward Secrecy, which uses session keys that are ephemeral and not stored anywhere. This helps prevent the decoding of captured data by unauthorized third parties, even if the secret long-term key itself is compromised. Archived Page 19 Amazon Web Services Amazon Web Services: Overview of Security Processes Security Logs As important as credentials and encrypted endpoints are for preventing security problems, logs are just as crucial for understanding events after a problem has occurred. And to be effective as a security tool, a log must include not just a list of what happened and when, but also identify the source. To help you with your after-the-fact investigations and near-real time intrusion detection, AWS CloudTrail provides a log of events within your account. For each event, you can see what service was accessed, what action was performed, and who made the request. CloudTrail captures API calls, as well as other things such as console sign-in events. Once you have enabled CloudTrail, event logs are delivered about every 5 minutes. You can configure CloudTrail so that it aggregates log files from multiple regions and/or accounts into a single Amazon S3 bucket. By default, a single trail will record and deliver events in all current and future regions. In addition to S3, you can send events to CloudWatch Logs, for custom metrics and alarming, or you can upload the logs to your favorite log management and analysis solutions to perform security analysis and detect user behavior patterns. For rapid response, you can create CloudWatch Events rules to take timely action to specific events. By default, log files are stored securely in Amazon S3, but you can also archive them to Amazon S3 Glacier to help meet audit and compliance requirements. In addition to CloudTrail’s user activity logs, you can use the Amazon CloudWatch Logs feature to collect and monitor system, application, and custom log files from your EC2 instances and other sources in near-real time. For example, you can monitor your web server's log files for invalid user messages to detect unauthorized login attempts to your guest OS. AWS Trusted Advisor Security Checks The AWS Trusted Advisor customer support service not only monitors for cloud performance and resiliency, but also cloud security. Trusted Advisor inspects your AWS environment and makes recommendations when opportunities may exist to save money, improve system performance, or close security gaps. It provides alerts on several of the most common security misconfigurations that can occur, including leaving certain ports open that make you vulnerable to hacking and unauthorized access, neglecting to create IAM accounts for your internal users, allowing public access to Amazon S3 buckets, not turning on user activity logging (AWS CloudTrail), or not using MFA on your root AWS Account. You also have the option for a Security contact at your Archived Page 20 Amazon Web Services Amazon Web Services: Overview of Security Processes organization to automatically receive a weekly email with an updated status of your Trusted Advisor security checks. The AWS Trusted Advisor service provides four checks at no additional charge to all users, including three important security checks: specific ports unrestricted, IAM use, and MFA on root account. When you sign up for Business- or Enterprise-level AWS Support, you receive full access to all Trusted Advisor checks. AWS Config Security Checks AWS Config is a continuous monitoring and assessment service that records changes to the configuration of your AWS resources. You can view the current and historic configurations of a resource and use this information to troubleshoot outages, conduct security attack analysis, and much more. You can view the configuration at any point in time and use that information to re-configure your resources and bring them into a steady state during an outage situation. Using AWS Config Rules, you can run continuous assessment checks on your resources to verify that they comply with your own security policies, industry best practices, and compliance regimes such as PCI/HIPAA. For example, AWS Config provides a managed AWS Config Rules to ensure that encryption is turned on for all EBS volumes in your account. You can also write a custom AWS Config Rule to essentially “codify” your own corporate security policies. AWS Config alerts you in real time when a resource is misconfigured, or when a resource violates a particular security policy. AWS Service-Specific Security Not only is security built into every layer of the AWS infrastructure, but also into each of the services available on that infrastructure. AWS services are architected to work efficiently and securely with all AWS networks and platforms. Each service provides extensive security features to enable you to protect sensitive data and applications. Compute Services Amazon Web Services provides a variety of cloud-based computing services that include a wide selection of compute instances that can scale up and down automatically to meet the needs of your application or enterprise. Archived Page 21 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon Elastic Compute Cloud (Amazon EC2) Security Amazon Elastic Compute Cloud (Amazon EC2) is a key component in Amazon’s Infrastructure-as-a-Service (IaaS), providing resizable computing capacity using server instances in AWS’s data centers. Amazon EC2 is designed to make web- scale computing easier by enabling you to obtain and configure capacity with minimal friction. You create and launch instances, which are collections of platform hardware and software. Multiple Levels of Security Security within Amazon EC2 is provided on multiple levels: the operating system (OS) of the host platform, the virtual instance OS or guest OS, a firewall, and signed API calls. Each of these items builds on the capabilities of the others. The goal is to prevent data contained within Amazon EC2 from being intercepted by unauthorized systems or users and to provide Amazon EC2 instances themselves that are as secure as possible without sacrificing the flexibility in configuration that customers demand. Hypervisor Amazon EC2 currently utilizes a highly customized version of the Xen hypervisor, taking advantage of paravirtualization (in the case of Linux guests). Because para-virtualized guests rely on the hypervisor to provide support for operations that normally require privileged access, the guest OS has no elevated access to the CPU. The CPU provides four separate privilege modes: 0-3, called rings. Ring 0 is the most privileged and 3 the least. The host OS executes in Ring 0. However, rather than executing in Ring 0 as most operating systems do, the guest OS runs in a lesser-privileged Ring 1 and applications in the least privileged Ring 3. This explicit virtualization of the physical resources leads to a clear separation between guest and hypervisor, resulting in additional security separation between the two. Traditionally, hypervisors protect the physical hardware and bios, virtualize the CPU, storage, networking, and provide a rich set of management capabilities. With the Nitro System, we are able to break apart those functions, offload them to dedicated hardware and software, and reduce costs by delivering all of the resources of a server to your instances. The Nitro Hypervisor provides consistent performance and increased compute and memory resources for EC2 virtualized instances by removing host system software components. It allows AWS to offer larger instance sizes (like c5.18xlarge) that provide practically all of the resources from the server to customers. Previously, C3 and C4 instances each eliminated software components by moving VPC and EBS functionality Archived Page 22 Amazon Web Services Amazon Web Services: Overview of Security Processes to hardware designed and built by AWS. This hardware enables the Nitro Hypervisor to be very small and uninvolved in data processing tasks for networking and storage. Nevertheless, as AWS expands its global cloud infrastructure, Amazon EC2’s use of its Xen-based hypervisor will also continue to grow. Xen will remain a core component of EC2 instances for the foreseeable future. Instance Isolation Different instances running on the same physical machine are isolated from each other via the Xen hypervisor. Amazon is active in the Xen community, which provides awareness of the latest developments. In addition, the AWS firewall resides within the hypervisor layer, between the physical network interface and the instance's virtual interface. All packets must pass through this layer, thus an instance’s neighbors have no more access to that instance than any other host on the Internet and can be treated as if they are on separate physical hosts. The physical RAM is separated using similar mechanisms. Customer instances have no access to raw disk devices, but instead are presented with virtualized disks. The AWS proprietary disk virtualization layer automatically resets every block of storage used by the customer, so that one customer’s data is never unintentionally exposed to another. In addition, memory allocated to guests is scrubbed (set to zero) by the hypervisor when it is unallocated to a guest. The memory is not returned to the pool of free memory available for new allocations until the memory scrubbing is complete. AWS recommends customers further protect their data using appropriate means. One common solution is to run an encrypted file system on top of the virtualized disk device. Archived Page 23 Amazon Web Services Amazon Web Services: Overview of Security Processes Figure 2: Amazon EC2 multiple layers of security Host Operating System: Administrators with a business need to access the management plane are required to use multi- factor authentication to gain access to purpose-built administration hosts. These administrative hosts are systems that are specifically designed, built, configured, and hardened to protect the management plane of the cloud. All such access is logged and audited. When an employee no longer has a business need to access the management plane, the privileges and access to these hosts and relevant systems can be revoked. Guest Operating System: Virtual instances are completely controlled by you, the customer. You have full root access or administrative control over accounts, services, and applications. AWS does not have any access rights to your instances or the guest OS. AWS recommends a base set of security best practices to include disabling password-only access to your guests, and utilizing some form of multi-factor authentication to gain access to your instances (or at a minimum certificate-based SSH Version 2 access). Additionally, you should employ a privilege escalation mechanism with logging on a per-user basis. For example, if the guest OS is Linux, after hardening your instance you should utilize certificate- based SSHv2 to access the virtual instance, disable remote root login, use command-line logging, and use ‘sudo’ for privilege escalation. You should generate your own key pairs in order to guarantee that they are unique, and not shared with other customers or with AWS. Archived Page 24 Amazon Web Services Amazon Web Services: Overview of Security Processes AWS also supports the use of the Secure Shell (SSH) network protocol to enable you to log in securely to your UNIX/Linux EC2 instances. Authentication for SSH used with AWS is via a public/private key pair to reduce the risk of unauthorized access to your instance. You can also connect remotely to your Windows instances using Remote Desktop Protocol (RDP) by utilizing an RDP certificate generated for your instance. You also control the updating and patching of your guest OS, including security updates. Amazon-provided Windows and Linux-based AMIs are updated regularly with the latest patches, so if you do not need to preserve data or customizations on your running Amazon AMI instances, you can simply relaunch new instances with the latest updated AMI. In addition, updates are provided for the Amazon Linux AMI via the Amazon Linux yum repositories. Firewall: Amazon EC2 provides a complete firewall solution; this mandatory inbound firewall is configured in a default deny-all mode and Amazon EC2 customers must explicitly open the ports needed to allow inbound traffic. The traffic may be restricted by protocol, by service port, as well as by source IP address (individual IP or Classless Inter-Domain Routing (CIDR) block). The firewall can be configured in groups permitting different classes of instances to have different rules. Consider, for example, the case of a traditional three-tiered web application. The group for the web servers would have port 80 (HTTP) and/or port 443 (HTTPS) open to the Internet. The group for the application servers would have port 8000 (application specific) accessible only to the web server group. The group for the database servers would have port 3306 (MySQL) open only to the application server group. All three groups would permit administrative access on port 22 (SSH), but only from the customer’s corporate network. Highly secure applications can be deployed using this expressive mechanism. See the following figure. Archived Page 25 Amazon Web Services Amazon Web Services: Overview of Security Processes Figure 3: Amazon EC2 security group firewall The firewall isn’t controlled through the guest OS; rather it requires your X.509 certificate and key to authorize changes, thus adding an extra layer of security. AWS supports the ability to grant granular access to different administrative functions on the instances and the firewall, therefore enabling you to implement additional security through separation of duties. The level of security afforded by the firewall is a function of which ports you open, and for what duration and purpose. The default state is to deny all incoming traffic, and you should plan carefully what you will open when building and securing your applications. Well- informed traffic management and security design are still required on a per- instance basis. AWS further encourages you to apply additional per- instance filters with host-based firewalls such as IPtables or the Windows Firewall and VPNs. This can restrict both inbound and outbound traffic. API Access: API calls to launch and terminate instances, change firewall parameters, and perform other functions are all signed by your Amazon Secret Access Key, which could be either the AWS Accounts Secret Access Key or the Secret Access key of a user created with AWS IAM. Without access to your Secret Access Key, Amazon EC2 API calls cannot be made on your behalf. In addition, API calls can be encrypted with SSL to maintain confidentiality. Amazon recommends always using SSL-protected API endpoints. Archived Permissions: AWS IAM also enables you to further control what APIs a user has permissions to call. Page 26 Amazon Web Services Amazon Web Services: Overview of Security Processes Elastic Block Storage (Amazon EBS) Security Amazon Elastic Block Storage (Amazon EBS) allows you to create storage volumes from 1 GB to 16 TB that can be mounted as devices by Amazon EC2 instances. Storage volumes behave like raw, unformatted block devices, with user supplied device names and a block device interface. You can create a file system on top of Amazon EBS volumes, or use them in any other way you would use a block device (like a hard drive). Amazon EBS volume access is restricted to the AWS Account that created the volume, and to the users under the AWS Account created with AWS IAM if the user has been granted access to the EBS operations, thus denying all other AWS Accounts and users the permission to view or access the volume. Data stored in Amazon EBS volumes is redundantly stored in multiple physical locations as part of normal operation of those services and at no additional charge. However, Amazon EBS replication is stored within the same availability zone, not across multiple zones; therefore, it is highly recommended that you conduct regular snapshots to Amazon S3 for long-term data durability. For customers who have architected complex transactional databases using EBS, it is recommended that backups to Amazon S3 be performed through the database management system so that distributed transactions and logs can be checkpointed. AWS does not perform backups of data that are maintained on virtual disks attached to running instances on Amazon EC2. You can make Amazon EBS volume snapshots publicly available to other AWS Accounts to use as the basis for creating your own volumes. Sharing Amazon EBS volume snapshots does not provide other AWS Accounts with the permission to alter or delete the original snapshot, as that right is explicitly reserved for the AWS Account that created the volume. An EBS snapshot is a block-level view of an entire EBS volume. Note that data that is not visible through the file system on the volume, such as files that have been deleted, may be present in the EBS snapshot. If you want to create shared snapshots, you should do so carefully. If a volume has held sensitive data or has had files deleted from it, a new EBS volume should be created. The data to be contained in the shared snapshot should be copied to the new volume, and the snapshot created from the new volume. Amazon EBS volumes are presented to you as raw unformatted block devices that have been wiped prior to being made available for use. Wiping occurs immediately before reuse so that you can be assured that the wipe process completed. If you have procedures requiring that all data be wiped via a specific method, such as those detailed in NIST 800-88 (“Guidelines for Media Sanitization”), you have the ability to do Archived Page 27 Amazon Web Services Amazon Web Services: Overview of Security Processes so on Amazon EBS. You should conduct a specialized wipe procedure prior to deleting the volume for compliance with your established requirements. Encryption of sensitive data is generally a good security practice, and AWS provides the ability to encrypt EBS volumes and their snapshots with AES-256. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage. In order to be able to do this efficiently and with low latency, the EBS encryption feature is only available on EC2's more powerful instance types (e.g., M3, C3, R3, G2). Auto Scaling Security Auto Scaling allows you to automatically scale your Amazon EC2 capacity up or down according to conditions you define, so that the number of Amazon EC2 instances you are using scales up seamlessly during demand spikes to maintain performance, and scales down automatically during demand lulls to minimize costs. Like all AWS services, Auto Scaling requires that every request made to its control API be authenticated so only authenticated users can access and manage Auto Scaling. Requests are signed with an HMAC-SHA1 signature calculated from the request and the user’s private key. However, getting credentials out to new EC2 instances launched with Auto Scaling can be challenging for large or elastically scaling fleets. To simplify this process, you can use roles within IAM, so that any new instances launched with a role will be given credentials automatically. When you launch an EC2 instance with an IAM role, temporary AWS security credentials with permissions specified by the role are securely provisioned to the instance and are made available to your application via the Amazon EC2 Instance Metadata Service. The Metadata Service makes new temporary security credentials available prior to the expiration of the current active credentials, so that valid credentials are always available on the instance. In addition, the temporary security credentials are automatically rotated multiple times per day, providing enhanced security. You can further control access to Auto Scaling by creating users under your AWS Account using AWS IAM, and controlling what Auto Scaling APIs these users have permission to call. For more information about using roles when launching instances, see Identity and Access Management for Amazon EC2. Networking Services Archived Amazon Web Services provides a range of networking services that enable you to create a logically isolated network that you define, establish a private network Page 28 Amazon Web Services Amazon Web Services: Overview of Security Processes connection to the AWS cloud, use a highly available and scalable DNS service and deliver content to your end users with low latency at high data transfer speeds with a content delivery web service. Elastic Load Balancing Security Elastic Load Balancing is used to manage traffic on a fleet of Amazon EC2 instances, distributing traffic to instances across all availability zones within a region. Elastic Load Balancing has all the advantages of an on-premises load balancer, plus several security benefits: • Takes over the encryption and decryption work from the Amazon EC2 instances and manages it centrally on the load balancer • Offers clients a single point of contact, and can also serve as the first line of defense against attacks on your network • When used in an Amazon VPC, supports creation and management of security groups associated with your Elastic Load Balancing to provide additional networking and security options • Supports end-to-end traffic encryption using TLS (previously SSL) on those networks that use secure HTTP (HTTPS) connections. When TLS is used, the TLS server certificate used to terminate client connections can be managed centrally on the load balancer, rather than on every individual instance. HTTPS/TLS uses a long-term secret key to generate a short-term session key to be used between the server and the browser to create the ciphered (encrypted) message. Elastic Load Balancing configures your load balancer with a pre-defined cipher set that is used for TLS negotiation when a connection is established between a client and your load balancer. The pre-defined cipher set provides compatibility with a broad range of clients and uses strong cryptographic algorithms. However, some customers may have requirements for allowing only specific ciphers and protocols (such as PCI, SOX, etc.) from clients to ensure that standards are met. In these cases, Elastic Load Balancing provides options for selecting different configurations for TLS protocols and ciphers. You can choose to enable or disable the ciphers depending on your specific requirements. Archived To help ensure the use of newer and stronger cipher suites when establishing a secure connection, you can configure the load balancer to have the final say in the cipher suite selection during the client-server negotiation. When the Server Order Preference option is selected, the load balancer selects a cipher suite based on the server’s prioritization Page 29 Amazon Web Services Amazon Web Services: Overview of Security Processes of cipher suites rather than the client’s. This gives you more control over the level of security that clients use to connect to your load balancer. For even greater communication privacy, Elastic Load Balancing allows the use of Perfect Forward Secrecy, which uses session keys that are ephemeral and not stored anywhere. This prevents the decoding of captured data, even if the secret long-term key itself is compromised. Elastic Load Balancing allows you to identify the originating IP address of a client connecting to your servers, whether you’re using HTTPS or TCP load balancing. Typically, client connection information, such as IP address and port, is lost when requests are proxied through a load balancer. This is because the load balancer sends requests to the server on behalf of the client, making your load balancer appear as though it is the requesting client. Having the originating client IP address is useful if you need more information about visitors to your applications in order to gather connection statistics, analyze traffic logs, or manage whitelists of IP addresses. Elastic Load Balancing access logs contain information about each HTTP and TCP request processed by your load balancer. This includes the IP address and port of the requesting client, the backend IP address of the instance that processed the request, the size of the request and response, and the actual request line from the client (for example, GET http://www.example.com: 80/HTTP/1.1). All requests sent to the load balancer are logged, including requests that never made it to backend instances. Amazon Virtual Private Cloud (Amazon VPC) Security Normally, each Amazon EC2 instance that you launch is randomly assigned a public IP address in the Amazon EC2 address space. Amazon VPC enables you to create an isolated portion of the AWS cloud and launch Amazon EC2 instances that have private (RFC 1918) addresses in the range of your choice (e.g., 10.0.0.0/16). You can define subnets within your VPC, grouping similar kinds of instances based on IP address range, and then set up routing and security to control the flow of traffic in and out of the instances and subnets. AWS offers a variety of VPC architecture templates with configurations that provide varying levels of public access: Archived • VPC with a single public subnet only. Your instances run in a private, isolated section of the AWS cloud with direct access to the Internet. Network ACLs and security groups can be used to provide strict control over inbound and outbound network traffic to your instances. Page 30 Amazon Web Services Amazon Web Services: Overview of Security Processes • VPC with public and private subnets. In addition to containing a public subnet, this configuration adds a private subnet whose instances are not addressable from the Internet. Instances in the private subnet can establish outbound connections to the Internet via the public subnet using Network Address Translation (NAT). • VPC with public and private subnets and hardware VPN access. This configuration adds an IPsec VPN connection between your Amazon VPC and your data center, effectively extending your data center to the cloud while also providing direct access to the Internet for public subnet instances in your Amazon VPC. In this configuration, customers add a VPN appliance on their corporate data center side. • VPC with private subnet only and hardware VPN access. Your instances run in a private, isolated section of the AWS cloud with a private subnet whose instances are not addressable from the Internet. You can connect this private subnet to your corporate data center via an IPsec VPN tunnel. You can also connect two VPCs using a private IP address, which allows instances in the two VPCs to communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single region. Security features within Amazon VPC include security groups, network ACLs, routing tables, and external gateways. Each of these items is complementary to providing a secure, isolated network that can be extended through selective enabling of direct Internet access or private connectivity to another network. Amazon EC2 instances running within an Amazon VPC inherit all of the benefits described below related to the guest OS and protection against packet sniffing. Note, however, that you must create VPC security groups specifically for your Amazon VPC; any Amazon EC2 security groups you have created will not work inside your Amazon VPC. Also, Amazon VPC security groups have additional capabilities that Amazon EC2 security groups do not have, such as being able to change the security group after the instance is launched and being able to specify any protocol with a standard protocol number (as opposed to just TCP, UDP, or ICMP). Archived Each Amazon VPC is a distinct, isolated network within the cloud; network traffic within each Amazon VPC is isolated from all other Amazon VPCs. At creation time, you select an IP address range for each Amazon VPC. You may create and attach an Internet Page 31 Amazon Web Services Amazon Web Services: Overview of Security Processes gateway, virtual private gateway, or both to establish external connectivity, subject to the controls below. API Access: Calls to create and delete Amazon VPCs, change routing, security group, and network ACL parameters, and perform other functions are all signed by your Amazon Secret Access Key, which could be either the AWS Account’s Secret Access Key or the Secret Access key of a user created with AWS IAM. Without access to your Secret Access Key, Amazon VPC API calls cannot be made on your behalf. In addition, API calls can be encrypted with SSL to maintain confidentiality. Amazon recommends always using SSL-protected API endpoints. AWS IAM also enables a customer to further control what APIs a newly created user has permissions to call. Subnets and Route Tables: You create one or more subnets within each Amazon VPC; each instance launched in the Amazon VPC is connected to one subnet. Traditional Layer 2 security attacks, including MAC spoofing and ARP spoofing, are blocked. Each subnet in an Amazon VPC is associated with a routing table, and all network traffic leaving the subnet is processed by the routing table to determine the destination. Firewall (Security Groups): Like Amazon EC2, Amazon VPC supports a complete firewall solution enabling filtering on both ingress and egress traffic from an instance. The default group enables inbound communication from other members of the same group and outbound communication to any destination. Traffic can be restricted by any IP protocol, by service port, as well as source/destination IP address (individual IP or Classless Inter-Domain Routing (CIDR) block). The firewall isn’t controlled through the guest OS; rather, it can be modified only through the invocation of Amazon VPC APIs. AWS supports the ability to grant granular access to different administrative functions on the instances and the firewall, therefore enabling you to implement additional security through separation of duties. The level of security afforded by the firewall is a function of which ports you open, and for what duration and purpose. Well-informed traffic management and security design are still required on a per-instance basis. AWS further encourages you to apply additional per-instance filters with host-based firewalls such as IP tables or the Windows Firewall. Archived Page 32 Amazon Web Services Amazon Web Services: Overview of Security Processes Figure 4: Amazon VPC network architecture Network Access Control Lists: To add a further layer of security within Amazon VPC, you can configure network ACLs. These are stateless traffic filters that apply to all traffic inbound or outbound from a subnet within Amazon VPC. These ACLs can contain ordered rules to allow or deny traffic based upon IP protocol, by service port, as well as source/destination IP address. Like security groups, network ACLs are managed through Amazon VPC APIs, adding an additional layer of protection and enabling additional security through separation of duties. The diagram below depicts how the security controls above inter-relate to enable flexible network topologies while providing complete control over network traffic flows. Archived Page 33 Amazon Web Services Amazon Web Services: Overview of Security Processes Figure 5: Flexible network topologies Virtual Private Gateway: A virtual private gateway enables private connectivity between the Amazon VPC and another network. Network traffic within each virtual private gateway is isolated from network traffic within all other virtual private gateways. You can establish VPN connections to the virtual private gateway from gateway devices at your premises. Each connection is secured by a pre-shared key in conjunction with the IP address of the customer gateway device. Internet Gateway: An Internet gateway may be attached to an Amazon VPC to enable direct connectivity to Amazon S3, other AWS services, and the Internet. Each instance desiring this access must either have an Elastic IP associated with it or route traffic through a NAT instance. Additionally, network routes are configured (see above) to direct traffic to the Internet gateway. AWS provides reference NAT AMIs that you can extend to perform network logging, deep packet inspection, application-layer filtering, or other security controls. Archived This access can only be modified through the invocation of Amazon VPC APIs. AWS supports the ability to grant granular access to different administrative functions on the instances and the Internet gateway, therefore enabling you to implement additional security through separation of duties. You can use a network address translation (NAT) Page 34 Amazon Web Services Amazon Web Services: Overview of Security Processes gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. Dedicated Instances: Within a VPC, you can launch Amazon EC2 instances that are physically isolated at the host hardware level (i.e., they will run on single-tenant hardware). An Amazon VPC can be created with ‘dedicated’ tenancy, so that all instances launched into the Amazon VPC use this feature. Alternatively, an Amazon VPC may be created with ‘default’ tenancy, but you can specify dedicated tenancy for particular instances launched into it. Elastic Network Interfaces: Each Amazon EC2 instance has a default network interface that is assigned a private IP address on your Amazon VPC network. You can create and attach an additional network interface, known as an elastic network interface, to any Amazon EC2 instance in your Amazon VPC for a total of two network interfaces per instance. Attaching more than one network interface to an instance is useful when you want to create a management network, use network and security appliances in your Amazon VPC, or create dual-homed instances with workloads/roles on distinct subnets. A network interface's attributes, including the private IP address, elastic IP addresses, and MAC address, follows the network interface as it is attached or detached from an instance and reattached to another instance. For more information about Amazon VPC, see Amazon Virtual Private Cloud. Additional Network Access Control with EC2-VPC If you launch instances in a Region where you did not have instances before AWS launched the new EC2-VPC feature (also called Default VPC), all instances are automatically provisioned in a ready-to-use default VPC. You can choose to create additional VPCs, or you can create VPCs for instances in regions where you already had instances before we launched EC2-VPC. If you create a VPC later, using regular VPC, you specify a CIDR block, create subnets, enter the routing and security for those subnets, and provision an Internet gateway or NAT instance if you want one of your subnets to be able to reach the Internet. When you launch EC2 instances into an EC2-VPC, most of this work is automatically performed for you. When you launch an instance into a default VPC using EC2-VPC, we do the following to set it up for you: • Create a default subnet in each Availability Zone Archived Page 35 Amazon Web Services Amazon Web Services: Overview of Security Processes • Create an internet gateway and connect it to your default VPC • Create a main route table for your default VPC with a rule that sends all traffic destined for the Internet to the Internet gateway • Create a default security group and associate it with your default VPC • Create a default network access control list (ACL) and associate it with your default VPC • Associate the default DHCP options set for your AWS account with your default VPC In addition to the default VPC having its own private IP range, EC2 instances launched in a default VPC can also receive a public IP. The following table summarizes the differences between instances launched into EC2 Classic, instances launched into a default VPC, and instances launched into a non default VPC. Table 2: Differences between different EC2 instances Characteristic EC2-Classic EC2-VPC (Default VPC) Regular VPC IP address by default, unless you specify otherwise during launch. Unless you specify otherwise during launch. Private IP address Your instance receives a private IP address from the EC2-Classic range each time it's started. Your instance receives a static private IP address from the address range of your default VPC. Your instance receives a static private IP address from the address range of your VPC. Multiple private IP addresses We select a single IP address for your instance. Multiple IP addresses are not supported. You can assign multiple private IP addresses to your instance. You can assign multiple private IP addresses to your instance. Archived Page 36 Amazon Web Services Amazon Web Services: Overview of Security Processes Characteristic EC2-Classic EC2-VPC (Default VPC) Regular VPC Elastic IP address An EIP is disassociated from your instance when you stop it. An EIP remains associated with your instance when you stop it. An EIP remains associated with your instance when you stop it. DNS hostnames DNS hostnames are enabled by default. DNS hostnames are enabled by default. DNS hostnames are disabled by default. Security group A security group can reference security groups that belong to other AWS accounts. A security group can reference security groups for your VPC only. A security group can reference security groups for your VPC only. Security group association You must terminate your instance to change its security group. You can change the security group of your running instance. You can change the security group of your running instance. Security group rules You can add rules for inbound traffic only. You can add rules for inbound and outbound traffic. You can add rules for inbound and outbound traffic. Tenancy Your instance runs on shared hardware; you cannot run an instance on single- tenant hardware. You can run your instance on shared hardware or single- tenant hardware. You can run your instance on shared hardware or single- tenant hardware. Archived Page 37 Amazon Web Services Amazon Web Services: Overview of Security Processes Note: Security groups for instances in EC2-Classic are slightly different than security groups for instances in EC2-VPC. For example, you can add rules for inbound traffic for EC2-Classic, but you can add rules for both inbound and outbound traffic to EC2-VPC. In EC2-Classic, you can’t change the security groups assigned to an instance after it’s launched, but in EC2-VPC, you can change security groups assigned to an instance after it’s launched. In addition, you can't use the security groups that you've created for use with EC2-Classic with instances in your VPC. You must create security groups specifically for use with instances in your VPC. The rules you create for use with a security group for a VPC can't reference a security group for EC2-Classic, and vice versa. Amazon Route 53 Security Amazon Route 53 is a highly available and scalable Domain Name System (DNS) service that answers DNS queries, translating domain names into IP addresses so computers can communicate with each other. Route 53 can be used to connect user requests to infrastructure running in AWS – such as an Amazon EC2 instance or an Amazon S3 bucket – or to infrastructure outside of AWS. Amazon Route 53 lets you manage the IP addresses (records) listed for your domain names and it answers requests (queries) to translate specific domain names into their corresponding IP addresses. Queries for your domain are automatically routed to a nearby DNS server using anycast in order to provide the lowest latency possible. Route 53 makes it possible for you to manage traffic globally through a variety of routing types, including Latency Based Routing (LBR), Geo DNS, and Weighted Round- Robin (WRR) —all of which can be combined with DNS Failover in order to help create a variety of low- latency, fault-tolerant architectures. The failover algorithms implemented by Amazon Route 53 are designed not only to route traffic to endpoints that are healthy, but also to help avoid making disaster scenarios worse due to misconfigured health checks and applications, endpoint overloads, and partition failures. Route 53 also offers Domain Name Registration – you can purchase and manage domain names such as example.com and Route 53 will automatically configure default DNS settings for your domains. You can buy, manage, and transfer (both in and out) domains from a wide selection of generic and country- specific top-level domains (TLDs). During the registration process, you have the option to enable privacy protection for your domain. This option will hide most of your personal information from the public Whois database in order to help thwart scraping and spamming. Archived Page 38 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon Route 53 is built using AWS’s highly available and reliable infrastructure. The distributed nature of the AWS DNS servers helps ensure a consistent ability to route your end users to your application. Route 53 also helps ensure the availability of your website by providing health checks and DNS failover capabilities. You can easily configure Route 53 to check the health of your website on a regular basis (even secure web sites that are available only over SSL), and to switch to a backup site if the primary one is unresponsive. Like all AWS Services, Amazon Route 53 requires that every request made to its control API be authenticated so only authenticated users can access and manage Route 53. API requests are signed with an HMAC-SHA1 or HMAC- SHA256 signature calculated from the request and the user’s AWS Secret Access key. Additionally, the Amazon Route 53 control API is only accessible via SSL- encrypted endpoints. It supports both IPv4 and IPv6 routing. You can control access to Amazon Route 53 DNS management functions by creating users under your AWS Account using AWS IAM, and controlling which Route 53 operations these users have permission to perform. Amazon CloudFront Security Amazon CloudFront gives customers an easy way to distribute content to end users with low latency and high data transfer speeds. It delivers dynamic, static, and streaming content using a global network of edge locations. Requests for customers’ objects are automatically routed to the nearest edge location, so content is delivered with the best possible performance. Amazon CloudFront is optimized to work with other AWS services, like Amazon S3, Amazon EC2, Elastic Load Balancing, and Amazon Route 53. It also works seamlessly with any non-AWS origin server that stores the original, definitive versions of your files. Amazon CloudFront requires every request made to its control API be authenticated so only authorized users can create, modify, or delete their own Amazon CloudFront distributions. Requests are signed with an HMAC-SHA1 signature calculated from the request and the user’s private key. Additionally, the Amazon CloudFront control API is only accessible via SSL-enabled endpoints. Archived There is no guarantee of durability of data held in Amazon CloudFront edge locations. The service may from time to time remove objects from edge locations if those objects are not requested frequently. Durability is provided by Amazon S3, which works as the origin server for Amazon CloudFront holding the original, definitive copies of objects delivered by Amazon CloudFront. Page 39 Amazon Web Services Amazon Web Services: Overview of Security Processes If you want control over who is able to download content from Amazon CloudFront, you can enable the service’s private content feature. This feature has two components: the first controls how content is delivered from the Amazon CloudFront edge location to viewers on the Internet. The second controls how the Amazon CloudFront edge locations access objects in Amazon S3. CloudFront also supports Geo Restriction, which restricts access to your content based on the geographic location of your viewers. To control access to the original copies of your objects in Amazon S3, Amazon CloudFront allows you to create one or more “Origin Access Identities” and associate these with your distributions. When an Origin Access Identity is associated with an Amazon CloudFront distribution, the distribution will use that identity to retrieve objects from Amazon S3. You can then use Amazon S3’s ACL feature, which limits access to that Origin Access Identity so the original copy of the object is not publicly readable. To control who is able to download objects from Amazon CloudFront edge locations, the service uses a signed-URL verification system. To use this system, you first create a public-private key pair, and upload the public key to your account via the AWS Management Console. Second, you configure your Amazon CloudFront distribution to indicate which accounts you would authorize to sign requests – you can indicate up to five AWS Accounts you trust to sign requests. Third, as you receive requests you will create policy documents indicating the conditions under which you want Amazon CloudFront to serve your content. These policy documents can specify the name of the object that is requested, the date and time of the request, and the source IP (or CIDR range) of the client making the request. You then calculate the SHA1 hash of your policy document and sign this using your private key. Finally, you include both the encoded policy document and the signature as query string parameters when you reference your objects. When Amazon CloudFront receives a request, it will decode the signature using your public key. Amazon CloudFront only serves requests that have a valid policy document and matching signature. Note: Private content is an optional feature that must be enabled when you set up your CloudFront distribution. Content delivered without this feature enabled will be publicly readable. Amazon CloudFront provides the option to transfer content over an encrypted connection (HTTPS). By default, CloudFront accepts requests over both HTTP and HTTPS protocols. However, you can also configure CloudFront to require HTTPS for all requests or have CloudFront redirect HTTP requests to HTTPS. You can even Archived Page 40 Amazon Web Services Amazon Web Services: Overview of Security Processes configure CloudFront distributions to allow HTTP for some objects but require HTTPS for other objects. Figure 6: Amazon CloudFront encrypted transmission You can configure one or more CloudFront origins to require CloudFront fetch objects from your origin using the protocol that the viewer used to request the objects. For example, when you use this CloudFront setting and the viewer uses HTTPS to request an object from CloudFront, CloudFront also uses HTTPS to forward the request to your origin. Amazon CloudFront uses the SSLv3 or TLSv1 protocols and a selection of cipher suites that includes the Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) protocol on connections to both viewers and the origin. ECDHE allows SSL/TLS clients to provide Perfect Forward Secrecy, which uses session keys that are ephemeral and not stored anywhere. This helps prevent the decoding of captured data by unauthorized third parties, even if the secret long- term key itself is compromised. Note: If you're using your own server as your origin, and you want to use HTTPS both between viewers and CloudFront and between CloudFront and your origin, you must install a valid SSL certificate on the HTTP server that is signed by a third-party certificate authority, for example, VeriSign or DigiCert. By default, you can deliver content to viewers over HTTPS by using your CloudFront distribution domain name in your URLs; for example, https://dxxxxx.cloudfront.net/image.jpg. If you want to deliver your content over HTTPS using your own domain name and your own SSL certificate, you can use SNI Custom SSL or Dedicated IP Custom SSL. With Server Name Identification (SNI) Custom SSL, CloudFront relies on the SNI extension of the TLS protocol, which is supported by most modern web browsers. However, some users may not be able to access your content Archived Page 41 Amazon Web Services Amazon Web Services: Overview of Security Processes because some older browsers do not support SNI. (For a list of supported browsers, visit CloudFront FAQs.) With Dedicated IP Custom SSL, CloudFront dedicates IP addresses to your SSL certificate at each CloudFront edge location so that CloudFront can associate the incoming requests with the proper SSL certificate. Amazon CloudFront access logs contain a comprehensive set of information about requests for content, including the object requested, the date and time of the request, the edge location serving the request, the client IP address, the referrer, and the user agent. To enable access logs, just specify the name of the Amazon S3 bucket to store the logs in when you configure your Amazon CloudFront distribution. AWS Direct Connect Security With AWS Direct Connect, you can provision a direct link between your internal network and an AWS region using a high-throughput, dedicated connection. Doing this may help reduce your network costs, improve throughput, or provide a more consistent network experience. With this dedicated connection in place, you can then create virtual interfaces directly to the AWS Cloud (for example, to Amazon EC2 and Amazon S3) and Amazon VPC. With Direct Connect, you bypass internet service providers in your network path. You can procure rack space within the facility housing the AWS Direct Connect location and deploy your equipment nearby. Once deployed, you can connect this equipment to AWS Direct Connect using a cross-connect. Each AWS Direct Connect location enables connectivity to the geographically nearest AWS region as well as access to other US regions. For example, you can provision a single connection to any AWS Direct Connect location in the US and use it to access public AWS services in all US Regions and AWS GovCloud (US). Using industry standard 802.1q VLANs, the dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon VPC using private IP space, while maintaining network separation between the public and private environments. Amazon Direct Connect requires the use of the Border Gateway Protocol (BGP) with an Autonomous System Number (ASN). To create a virtual interface, you use an MD5 cryptographic key for message authorization. MD5 creates a keyed hash using your Archived Page 42 Amazon Web Services Amazon Web Services: Overview of Security Processes secret key. You can have AWS automatically generate a BGP MD5 key or you can provide your own. Storage Services Amazon Web Services provides low-cost data storage with high durability and availability. AWS offers storage choices for backup, archiving, and disaster recovery, as well as block and object storage. Amazon Simple Storage Service (Amazon S3) Security Amazon Simple Storage Service (Amazon S3) allows you to upload and retrieve data at any time, from anywhere on the web. Amazon S3 stores data as objects within buckets. An object can be any kind of file: a text file, a photo, a video, etc. When you add a file to Amazon S3, you have the option of including metadata with the file and setting permissions to control access to the file. For each bucket, you can control access to the bucket (who can create, delete, and list objects in the bucket), view access logs for the bucket and its objects, and choose the geographical region where Amazon S3 will store the bucket and its contents. Data Access Access to data stored in Amazon S3 is restricted by default; only bucket and object owners have access to the Amazon S3 resources they create (note that a bucket/object owner is the AWS Account owner, not the user who created the bucket/object). There are multiple ways to control access to buckets and objects: • Identity and Access Management (IAM) Policies. AWS IAM enables organizations with many employees to create and manage multiple users under a single AWS Account. IAM policies are attached to the users, enabling centralized control of permissions for users under your AWS Account to access buckets or objects. With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources. • Access Control Lists (ACLs). Within Amazon S3, you can use ACLs to give read or write access on buckets or objects to groups of users. With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. Archived Page 43 Amazon Web Services Amazon Web Services: Overview of Security Processes • Bucket Policies. Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources. Table 3: Types of access control Type of Access Control AWS Account Level Control User Level Control IAM Policies No Yes ACLs Yes No Bucket Policies Yes Yes You can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requester’s IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys. For more information about action-specific policy keys available within Amazon S3, see the Amazon Simple Storage Service Developer Guide. Amazon S3 also gives developers the option to use query string authentication, which allows them to share Amazon S3 objects through URLs that are valid for a predefined period of time. Query string authentication is useful for giving HTTP or browser access to resources that would normally require authentication. The signature in the query string secures the request. Data Transfer For maximum security, you can securely upload/download data to Amazon S3 via the SSL encrypted endpoints. The encrypted endpoints are accessible from both the Internet and from within Amazon EC2, so that data is transferred securely both within AWS and to and from sources outside of AWS. Data Storage Archived Amazon S3 provides multiple options for protecting data at rest. For customers who prefer to manage their own encryption, they can use a client encryption library like the Amazon S3 Encryption Client to encrypt data before uploading to Amazon S3. Page 44 Amazon Web Services Amazon Web Services: Overview of Security Processes Alternatively, you can use Amazon S3 Server-Side Encryption (SSE) if you prefer to have Amazon S3 manage the encryption process for you. Data is encrypted with a key generated by AWS or with a key you supply, depending on your requirements. With Amazon S3 SSE, you can encrypt data on upload simply by adding an additional request header when writing the object. Decryption happens automatically when data is retrieved. Note: Metadata, which you can include with your object, is not encrypted. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata. Amazon S3 SSE uses one of the strongest block ciphers available – 256-bit Advanced Encryption Standard (AES-256). With Amazon S3 SSE, every protected object is encrypted with a unique encryption key. This object key itself is then encrypted with a regularly rotated master key. Amazon S3 SSE provides additional security by storing the encrypted data and encryption keys in different hosts. Amazon S3 SSE also makes it possible for you to enforce encryption requirements. For example, you can create and apply bucket policies that require that only encrypted data can be uploaded to your buckets. For long-term storage, you can automatically archive the contents of your Amazon S3 buckets to AWS’s archival service called Amazon S3 Glacier. You can have data transferred at specific intervals to Amazon S3 Glacier by creating lifecycle rules in Amazon S3 that describe which objects you want to be archived to Amazon S3 Glacier and when. As part of your data management strategy, you can also specify how long Amazon S3 should wait after the objects are put into Amazon S3 to delete them. When an object is deleted from Amazon S3, removal of the mapping from the public name to the object starts immediately, and is generally processed across the distributed system within several seconds. Once the mapping is removed, there is no remote access to the deleted object. The underlying storage area is then reclaimed for use by the system. Data Durability and Reliability Amazon S3 is designed to provide 99.999999999% durability and 99.99% availability of objects over a given year. Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 region. To help provide durability, Amazon S3 PUT and COPY operations synchronously store customer data across multiple facilities before returning SUCCESS. Once stored, Amazon S3 helps maintain the durability of Archived Page 45 Amazon Web Services Amazon Web Services: Overview of Security Processes the objects by quickly detecting and repairing any lost redundancy. Amazon S3 also regularly verifies the integrity of data stored using checksums. If corruption is detected, it is repaired using redundant data. In addition, Amazon S3 calculates checksums on all network traffic to detect corruption of data packets when storing or retrieving data. Amazon S3 provides further protection via Versioning. You can use Versioning to preserve, retrieve, and restore every version of every object stored in an Amazon S3 bucket. With Versioning, you can easily recover from both unintended user actions and application failures. By default, requests will retrieve the most recently written version. Older versions of an object can be retrieved by specifying a version in the request. You can further protect versions using Amazon S3 Versioning's MFA Delete feature. Once enabled for an Amazon S3 bucket, each version deletion request must include the six digit code and serial number from your multi-factor authentication device. Access Logs An Amazon S3 bucket can be configured to log access to the bucket and objects within it. The access log contains details about each access request including request type, the requested resource, the requestor’s IP, and the time and date of the request. When logging is enabled for a bucket, log records are periodically aggregated into log files and delivered to the specified Amazon S3 bucket. Cross-Origin Resource Sharing (CORS) AWS customers who use Amazon S3 to host static web pages or store objects used by other web pages can load content securely by configuring an Amazon S3 bucket to explicitly enable cross-origin requests. Modern browsers use the Same Origin policy to block JavaScript or HTML5 from allowing requests to load content from another site or domain as a way to help ensure that malicious content is not loaded from a less reputable source (such as during cross-site scripting attacks). With the Cross-Origin Resource Sharing (CORS) policy enabled, assets such as web fonts and images stored in an Amazon S3 bucket can be safely referenced by external web pages, style sheets, and HTML5 applications. Amazon S3 Glacier Security Like Amazon S3, the Amazon S3 Glacier service provides low-cost, secure, and durable storage. But where Amazon S3 is designed for rapid retrieval, Amazon S3 Glacier is meant to be used as an archival service for data that is not accessed often and for which retrieval times of several hours are suitable. Archived Page 46 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon S3 Glacier stores files as archives within vaults. Archives can be any data such as a photo, video, or document, and can contain one or several files. You can store an unlimited number of archives in a single vault and can create up to 1,000 vaults per region. Each archive can contain up to 40 TB of data. Data Upload To transfer data into Amazon S3 Glacier vaults, you can upload an archive in a single upload operation or a multipart operation. In a single upload operation, you can upload archives up to 4 GB in size. However, customers can achieve better results using the Multipart Upload API to upload archives greater than 100 MB. Using the Multipart Upload API allows you to upload large archives, up to about 40,000 GB. The Multipart Upload API call is designed to improve the upload experience for larger archives; it enables the parts to be uploaded independently, in any order, and in parallel. If a multipart upload fails, you only need to upload the failed part again and not the entire archive. When you upload data to Amazon S3 Glacier, you must compute and supply a tree hash. Amazon S3 Glacier checks the hash against the data to help ensure that it has not been altered en route. A tree hash is generated by computing a hash for each megabyte-sized segment of the data, and then combining the hashes in tree fashion to represent ever-growing adjacent segments of the data. As an alternate to using the Multipart Upload feature, customers with very large uploads to Amazon S3 Glacier may consider using the AWS Snowball service instead to transfer the data. AWS Snowball facilitates moving large amounts of data into AWS using portable storage devices for transport. AWS transfers your data directly off of storage devices using Amazon’s high-speed internal network, bypassing the Internet. You can also set up Amazon S3 to transfer data at specific intervals to Amazon S3 Glacier. You can create lifecycle rules in Amazon S3 that describe which objects you want to be archived to Amazon S3 Glacier and when. You can also specify how long Amazon S3 should wait after the objects are put into Amazon S3 to delete them. To achieve even greater security, you can securely upload/download data to Amazon S3 Glacier via the SSL-encrypted endpoints. The encrypted endpoints are accessible from both the Internet and from within Amazon EC2, so that data is transferred securely both within AWS and to and from sources outside of AWS. Archived Page 47 Amazon Web Services Amazon Web Services: Overview of Security Processes Data Retrieval Retrieving archives from Amazon S3 Glacier requires the initiation of a retrieval job, which is generally completed in 3 to 5 hours. You can then access the data via HTTP GET requests. The data will remain available to you for 24 hours. You can retrieve an entire archive or several files from an archive. If you want to retrieve only a subset of an archive, you can use one retrieval request to specify the range of the archive that contains the files you are interested or you can initiate multiple retrieval requests, each with a range for one or more files. You can also limit the number of vault inventory items retrieved by filtering on an archive creation date range or by setting a maximum items limit. Whichever method you choose, when you retrieve portions of your archive, you can use the supplied checksum to help ensure the integrity of the files provided that the range that is retrieved is aligned with the tree hash of the overall archive. Data Storage Amazon S3 Glacier automatically encrypts the data using AES-256 and stores it durably in an immutable form. Amazon S3 Glacier is designed to provide average annual durability of 99.999999999% for an archive. It stores each archive in multiple facilities and multiple devices. Unlike traditional systems which can require laborious data verification and manual repair, Amazon S3 Glacier performs regular, systematic data integrity checks and is built to be automatically self-healing. When an object is deleted from Amazon S3 Glacier, removal of the mapping from the public name to the object starts immediately, and is generally processed across the distributed system within several seconds. Once the mapping is removed, there is no remote access to the deleted object. The underlying storage area is then reclaimed for use by the system. Data Access Only your account can access your data in Amazon S3 Glacier. To control access to your data in Amazon S3 Glacier, you can use AWS IAM to specify which users within your account have rights to operations on a given vault. AWS Storage Gateway Security Archived The AWS Storage Gateway service connects your on-premises software appliance with cloud-based storage to provide seamless and secure integration between your IT environment and the AWS storage infrastructure. The service enables you to securely Page 48 Amazon Web Services Amazon Web Services: Overview of Security Processes upload data to AWS’ scalable, reliable, and secure Amazon S3 storage service for cost effective backup and rapid disaster recovery. AWS Storage Gateway transparently backs up data off-site to Amazon S3 in the form of Amazon EBS snapshots. Amazon S3 redundantly stores these snapshots on multiple devices across multiple facilities, detecting and repairing any lost redundancy. The Amazon EBS snapshot provides a point-in-time backup that can be restored on premises or used to instantiate new Amazon EBS volumes. Data is stored within a single region that you specify. AWS Storage Gateway offers three options: • Gateway-Stored Volumes (where the cloud is backup). In this option, your volume data is stored locally and then pushed to Amazon S3, where it is stored in redundant, encrypted form, and made available in the form of Amazon Elastic Block Storage (Amazon EBS) snapshots. When you use this model, the on premises storage is primary, delivering low-latency access to your entire dataset, and the cloud storage is the backup. • Gateway-Cached Volumes (where the cloud is primary). In this option, your volume data is stored encrypted in Amazon S3, visible within your enterprise's network via an iSCSI interface. Recently accessed data is cached on- premises for low-latency local access. When you use this model, the cloud storage is primary, but you get low- latency access to your active working set in the cached volumes on premises. • Gateway-Virtual Tape Library (VTL). In this option, you can configure a Gateway-VTL with up to 10 virtual tape drives per gateway, 1 media changer and up to 1500 virtual tape cartridges. Each virtual tape drive responds to the SCSI command set, so your existing on-premises backup applications (either disk-to tape or disk-to-disk-to- tape) will work without modification. No matter which option you choose, data is asynchronously transferred from your on premises storage hardware to AWS over SSL. The data is stored encrypted in Amazon S3 using Advanced Encryption Standard (AES) 256, a symmetric- key encryption standard using 256-bit encryption keys. The AWS Storage Gateway only uploads data that has changed, minimizing the amount of data sent over the Internet. Archived The AWS Storage Gateway runs as a virtual machine (VM) that you deploy on a host in your data center running VMware ESXi Hypervisor v 4.1 or v 5 or Microsoft Hyper-V (you download the VMware software during the setup process). You can also run within EC2 using a gateway AMI. During the installation and configuration process, you can Page 49 Amazon Web Services Amazon Web Services: Overview of Security Processes create up to 12 stored volumes, 20 Cached volumes, or 1500 virtual tape cartridges per gateway. Once installed, each gateway will automatically download, install, and deploy updates and patches. This activity takes place during a maintenance window that you can set on a per-gateway basis. The iSCSI protocol supports authentication between targets and initiators via CHAP (Challenge-Handshake Authentication Protocol). CHAP provides protection against man-in-the-middle and playback attacks by periodically verifying the identity of an iSCSI initiator as authenticated to access a storage volume target. To set up CHAP, you must configure it in both the AWS Storage Gateway console and in the iSCSI initiator software you use to connect to the target. After you deploy the AWS Storage Gateway VM, you must activate the gateway using the AWS Storage Gateway console. The activation process associates your gateway with your AWS Account. Once you establish this connection, you can manage almost all aspects of your gateway from the console. In the activation process, you specify the IP address of your gateway, name your gateway, identify the AWS region in which you want your snapshot backups stored, and specify the gateway time zone. AWS Snowball Security AWS Snowball is a simple, secure method for physically transferring large amounts of data to Amazon S3, EBS, or Amazon S3 Glacier storage. This service is typically used by customers who have over 100 GB of data and/or slow connection speeds that would result in very slow transfer rates over the Internet. With AWS Snowball, you prepare a portable storage device that you ship to a secure AWS facility. AWS transfers the data directly off of the storage device using Amazon’s high-speed internal network, thus bypassing the Internet. Conversely, data can also be exported from AWS to a portable storage device. Like all other AWS services, the AWS Snowball service requires that you securely identify and authenticate your storage device. In this case, you will submit a job request to AWS that includes your Amazon S3 bucket, Amazon EBS region, AWS Access Key ID, and return shipping address. You then receive a unique identifier for the job, a digital signature for authenticating your device, and an AWS address to ship the storage device to. For Amazon S3, you place the signature file on the root directory of your device. For Amazon EBS, you tape the signature barcode to the exterior of the device. The signature file is used only for authentication and is not uploaded to Amazon S3 or EBS. Archived Page 50 Amazon Web Services Amazon Web Services: Overview of Security Processes For transfers to Amazon S3, you specify the specific buckets to which the data should be loaded and ensure that the account doing the loading has write permission for the buckets. You should also specify the access control list to be applied to each object loaded to Amazon S3. For transfers to EBS, you specify the target region for the EBS import operation. If the storage device is less than or equal to the maximum volume size of 1 TB, its contents are loaded directly into an Amazon EBS snapshot. If the storage device’s capacity exceeds 1 TB, a device image is stored within the specified S3 log bucket. You can then create a RAID of Amazon EBS volumes using software such as Logical Volume Manager, and copy the image from S3 to this new volume. For added protection, you can encrypt the data on your device before you ship it to AWS. For Amazon S3 data, you can use a PIN-code device with hardware encryption or TrueCrypt software to encrypt your data before sending it to AWS. For EBS and Amazon S3 Glacier data, you can use any encryption method you choose, including a PIN-code device. AWS will decrypt your Amazon S3 data before importing using the PIN code and/or TrueCrypt password you supply in your import manifest. AWS uses your PIN to access a PIN-code device, but does not decrypt software-encrypted data for import to Amazon EBS or Amazon S3 Glacier. The following table summarizes your encryption options for each type of import/export job. Table 4: Encryption options for import/export jobs Import to Amazon S3 Source Target Result • Files on a device file system • Encrypt data using PIN code device and/or TrueCrypt before shipping device • Objects in an existing Amazon S3 bucket • One object for each file. • AWS decrypts the data before performing the import Export from Amazon S3 • AWS erases your device after every import job prior to shipping Source Target Result Archived Page 51 Archived Amazon Web Services Amazon Web Services: Overview of Security Processes Page 52 • Objects in one or more Amazon S3 buckets • Provide a PIN code and/or password that AWS will use to encrypt your data • Files on your storage device • AWS formats your device • AWS copies your data to an encrypted file container on your device • One file for each object • AWS encrypts your data prior to shipping • Use PIN-code device and/or TrueCrypt to decrypt the files Import to Amazon S3 Glacier Source Target Result • Entire device • Encrypt the data using the encryption method of your choice before shipping • One archive in an existing Amazon S3 Glacier vault • AWS does not decrypt your device • Device image stored as a single archive • AWS erases your device after every import job prior to shipping Import to Amazon EBS (Device Capacity < 1 TB) Source Target Result • Entire device • Encrypt the data using the encryption method of your choice before shipping • One Amazon EBS snapshot • AWS does not decrypt your device • Device image is stored as a single snapshot • If the device was encrypted, the image is encrypted • AWS erases your device after every import job prior to shipping Import to Amazon EBS (Device Capacity > 1 TB) Source Target Result • Entire device • Encrypt the data using the encryption method of your choice before shipping • Multiple objects in an existing Amazon S3 bucket • AWS does not decrypt your device • Device image chunked into series of 1 TB snapshots stored as objects in Amazon S3 bucket specified in manifest file • If the device was encrypted, the image is encrypted • AWS erases your device after every import job prior to shipping Amazon Web Services Amazon Web Services: Overview of Security Processes After the import is complete, AWS Snowball will erase the contents of your storage device to safeguard the data during return shipment. AWS overwrites all writable blocks on the storage device with zeroes. You will need to repartition and format the device after the wipe. If AWS is unable to erase the data on the device, it will be scheduled for destruction and our support team will contact you using the email address specified in the manifest file you ship with the device. When shipping a device internationally, the customs option and certain required subfields are required in the manifest file sent to AWS. AWS Snowball uses these values to validate the inbound shipment and prepare the outbound customs paperwork. Two of these options are whether the data on the device is encrypted or not and the encryption software’s classification. When shipping encrypted data to or from the United States, the encryption software must be classified as 5D992 under the United States Export Administration Regulations. Amazon Elastic File System Security Amazon Elastic File System (Amazon EFS) provides simple, scalable file storage for use with Amazon EC2 instances in the AWS Cloud. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files. Amazon EFS file systems are distributed across an unconstrained number of storage servers, enabling file systems to grow elastically to petabyte- scale and allowing massively parallel access from Amazon EC2 instances to your data. Data Access With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance, and then read and write data from to and from your file system. You can mount an Amazon EFS file system on EC2 instances in your VPC, through the Network File System versions 4.0 and 4.1 (NFSv4) protocol. To access your Amazon EFS file system in a VPC, you create one or more mount targets in the VPC. A mount target provides an IP address for an NFSv4 endpoint. You can then mount an Amazon EFS file system to this end point using its DNS name, which will resolve to the IP address of the EFS mount target in the same Availability Zone as your EC2 instance. You can create one mount target in each Availability Zone in a region. If there are multiple subnets in an Availability Zone in your VPC, you create a mount target in one of the subnets, and all EC2 instances in that Availability Zone share that mount target. You Archived Page 53 Amazon Web Services Amazon Web Services: Overview of Security Processes can also mount an EFS file system on a host in an on-premises datacenter using AWS Direct Connect. When using Amazon EFS, you specify Amazon EC2 security groups for your EC2 instances and security groups for the EFS mount targets associated with the file system. Security groups act as a firewall, and the rules you add define the traffic flow. You can authorize inbound/outbound access to your EFS file system by adding rules that allow your EC2 instance to connect to your Amazon EFS file system via the mount target using the NFS port. After mounting the file system via the mount target, you use it like any other POSIX compliant file system. Files and directories in an EFS file system support standard Unix style read/write/execute permissions based on the user and group ID asserted by the mounting NFSv4.1 client. For information about NFS- level permissions and related considerations, see Working with Users, Groups, and Permissions at the Network File System (NFS) Level. All Amazon EFS file systems are owned by an AWS Account. You can use IAM policies to grant permissions to other users so that they can perform administrative operations on your file systems, including deleting a file system or modifying a mount target’s security groups. For more information about EFS permissions, see Overview of Managing Access Permissions to Your Amazon EFS Resources. Data Durability and Reliability Amazon EFS is designed to be highly durable and highly available. All data and metadata is stored across multiple Availability Zones, and all service components are designed to be highly available. EFS provides strong consistency by synchronously replicating data across Availability Zones, with read-after-write semantics for most file operations. Amazon EFS incorporates checksums for all metadata and data throughout the service. Using a file system checking process (FSCK), EFS continuously validates a file system's metadata and data integrity. Data Sanitization Amazon EFS is designed so that when you delete data from a file system, that data will never be served again. If your procedures require that all data be wiped via a specific method, such as those detailed in DoD 5220.22-M (“National Industrial Security Program Operating Manual “) or NIST 800-88 (“Guidelines for Media Sanitization”), we recommend that you conduct a specialized wipe procedure prior to deleting the file system. Archived Page 54 Amazon Web Services Amazon Web Services: Overview of Security Processes Database Services Amazon Web Services provides a number of database solutions for developers and businesses—from managed relational and NoSQL database services, to in- memory caching as a service and petabyte-scale data-warehouse service. Amazon DynamoDB Security Amazon DynamoDB is a managed NoSQL database service that provides fast and predictable performance with seamless scalability. Amazon DynamoDB enables you to offload the administrative burdens of operating and scaling distributed databases to AWS, so you don’t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. You can create a database table that can store and retrieve any amount of data, and serve any level of request traffic. DynamoDB automatically spreads the data and traffic for the table over a sufficient number of servers to handle the request capacity you specified and the amount of data stored, while maintaining consistent, fast performance. All data items are stored on Solid State Drives (SSDs) and are automatically replicated across multiple availability zones in a region to provide built-in high availability and data durability. You can set up automatic backups using a special template in AWS Data Pipeline that was created just for copying DynamoDB tables. You can choose full or incremental backups to a table in the same region or a different region. You can use the copy for disaster recovery (DR) in the event that an error in your code damages the original table, or to federate DynamoDB data across regions to support a multi-region application. To control who can use the DynamoDB resources and API, you set up permissions in AWS IAM. In addition to controlling access at the resource-level with IAM, you can also control access at the database level—you can create database-level permissions that allow or deny access to items (rows) and attributes (columns) based on the needs of your application. These database level permissions are called fine-grained access controls, and you create them using an IAM policy that specifies under what circumstances a user or application can access a DynamoDB table. The IAM policy can restrict access to individual items in a table, access to the attributes in those items, or both at the same time. Archived Page 55 Amazon Web Services Amazon Web Services: Overview of Security Processes Figure 7: Database-level permissions You can optionally use web identity federation to control access by application users who are authenticated by Login with Amazon, Facebook, or Google. Web identity federation removes the need for creating individual IAM users; instead, users can sign in to an identity provider and then obtain temporary security credentials from AWS Security Token Service (AWS STS). AWS STS returns temporary AWS credentials to the application and allows it to access the specific DynamoDB table. In addition to requiring database and user permissions, each request to the DynamoDB service must contain a valid HMAC-SHA256 signature, or the request is rejected. The AWS SDKs automatically sign your requests; however, if you want to write your own HTTP POST requests, you must provide the signature in the header of your request to Amazon DynamoDB. To calculate the signature, you must request temporary security credentials from the AWS Security Token Service. Use the temporary security credentials to sign your requests to Amazon DynamoDB. Amazon DynamoDB is accessible via TLS/SSL-encrypted endpoints. Amazon Relational Database Service (Amazon RDS) Security Amazon RDS allows you to quickly create a relational database (DB) instance and flexibly scale the associated compute resources and storage capacity to meet application demand. Amazon RDS manages the database instance on your behalf by performing backups, handling failover, and maintaining the database software. Currently, Amazon RDS is available for MySQL, Oracle, Microsoft SQL Server, and PostgreSQL database engines. Amazon RDS has multiple features that enhance reliability for critical production databases, including DB security groups, permissions, SSL connections, automated backups, DB snapshots, and multi-AZ deployments. DB instances can also be deployed in an Amazon VPC for additional network isolation. Archived Page 56 Amazon Web Services Amazon Web Services: Overview of Security Processes Access Control When you first create a DB Instance within Amazon RDS, you will create a master user account, which is used only within the context of Amazon RDS to control access to your DB Instance(s). The master user account is a native database user account that allows you to log on to your DB Instance with all database privileges. You can specify the master user name and password you want associated with each DB Instance when you create the DB Instance. Once you have created your DB Instance, you can connect to the database using the master user credentials. Subsequently, you can create additional user accounts so that you can restrict who can access your DB Instance. You can control Amazon RDS DB Instance access via DB Security Groups, which are similar to Amazon EC2 Security Groups but not interchangeable. DB Security Groups act like a firewall controlling network access to your DB Instance. Database Security Groups default to a “deny all” access mode and customers must specifically authorize network ingress. There are two ways of doing this: authorizing a network IP range or authorizing an existing Amazon EC2 Security Group. DB Security Groups only allow access to the database server port (all others are blocked) and can be updated without restarting the Amazon RDS DB Instance, which allows a customer seamless control of their database access. Using AWS IAM, you can further control access to your RDS DB instances. AWS IAM enables you to control what RDS operations each individual AWS IAM user has permission to call. Network Isolation For additional network access control, you can run your DB Instances in an Amazon VPC. Amazon VPC enables you to isolate your DB Instances by specifying the IP range you wish to use, and connect to your existing IT infrastructure through industry-standard encrypted IPsec VPN. Running Amazon RDS in a VPC enables you to have a DB instance within a private subnet. You can also set up a virtual private gateway that extends your corporate network into your VPC, and allows access to the RDS DB instance in that VPC. Refer to the Amazon VPC User Guide for more details. For Multi-AZ deployments, defining a subnet for all availability zones in a region will allow Amazon RDS to create a new standby in another availability zone should the need arise. You can create DB Subnet Groups, which are collections of subnets that you may want to designate for your RDS DB Instances in a VPC. Each DB Subnet Group should have at least one subnet for every availability zone in a given region. In this case, when you create a DB Instance in a VPC, you select a DB Subnet Group; Amazon RDS then uses that DB Subnet Group and your preferred availability zone to select a subnet and Archived Page 57 Amazon Web Services Amazon Web Services: Overview of Security Processes an IP address within that subnet. Amazon RDS creates and associates an Elastic Network Interface to your DB Instance with that IP address. DB Instances deployed within an Amazon VPC can be accessed from the Internet or from Amazon EC2 Instances outside the VPC via VPN or bastion hosts that you can launch in your public subnet. To use a bastion host, you will need to set up a public subnet with an EC2 instance that acts as an SSH Bastion. This public subnet must have an Internet gateway and routing rules that allow traffic to be directed via the SSH host, which must then forward requests to the private IP address of your Amazon RDS DB instance. DB Security Groups can be used to help secure DB Instances within an Amazon VPC. In addition, network traffic entering and exiting each subnet can be allowed or denied via network ACLs. All network traffic entering or exiting your Amazon VPC via your IPsec VPN connection can be inspected by your on- premises security infrastructure, including network firewalls and intrusion detection systems. Encryption You can encrypt connections between your application and your DB Instance using SSL. For MySQL and SQL Server, RDS creates an SSL certificate and installs the certificate on the DB instance when the instance is provisioned. For MySQL, you launch the mysql client using the --ssl_ca parameter to reference the public key in order to encrypt connections. For SQL Server, download the public key and import the certificate into your Windows operating system. Oracle RDS uses Oracle native network encryption with a DB instance. You simply add the native network encryption option to an option group and associate that option group with the DB instance. Once an encrypted connection is established, data transferred between the DB Instance and your application will be encrypted during transfer. You can also require your DB instance to only accept encrypted connections. Amazon RDS supports Transparent Data Encryption (TDE) for SQL Server (SQL Server Enterprise Edition) and Oracle (part of the Oracle Advanced Security option available in Oracle Enterprise Edition). The TDE feature automatically encrypts data before it is written to storage and automatically decrypts data when it is read from storage. Note: SSL support within Amazon RDS is for encrypting the connection between your application and your DB Instance; it should not be relied on for authenticating the DB Instance itself. Archived Page 58 Amazon Web Services Amazon Web Services: Overview of Security Processes While SSL offers security benefits, be aware that SSL encryption is a compute intensive operation and will increase the latency of your database connection. To learn how SSL works with SQL Server, you can read more in the Amazon Relational Database Service User Guide. Automated Backups and DB Snapshots Amazon RDS provides two different methods for backing up and restoring your DB Instance(s): automated backups and database snapshots (DB Snapshots). Turned on by default, the automated backup feature of Amazon RDS enables point-in time recovery for your DB Instance. Amazon RDS will back up your database and transaction logs and store both for a user-specified retention period. This allows you to restore your DB Instance to any second during your retention period, up to the last 5 minutes. Your automatic backup retention period can be configured to up to 35 days. During the backup window, storage I/O may be suspended while your data is being backed up. This I/O suspension typically lasts a few minutes. This I/O suspension is avoided with Multi-AZ DB deployments, since the backup is taken from the standby. DB Snapshots are user-initiated backups of your DB Instance. These full database backups are stored by Amazon RDS until you explicitly delete them. You can copy DB snapshots of any size and move them between any of AWS’s public regions, or copy the same snapshot to multiple regions simultaneously. You can then create a new DB Instance from a DB Snapshot whenever you desire. DB Instance Replication Amazon cloud computing resources are housed in highly available data center facilities in different regions of the world, and each region contains multiple distinct locations called Availability Zones. Each Availability Zone is engineered to be isolated from failures in other Availability Zones, and to provide inexpensive, low-latency network connectivity to other Availability Zones in the same region. To architect for high availability of your Oracle, PostgreSQL, or MySQL databases, you can run your RDS DB instance in several Availability Zones, an option called a Multi-AZ deployment. When you select this option, Amazon automatically provisions and maintains a synchronous standby replica of your DB instance in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to the standby replica. In the event of DB instance or Availability Zone failure, Amazon RDS will automatically failover to the standby so that database operations can resume quickly without administrative intervention. Archived Page 59 Amazon Web Services Amazon Web Services: Overview of Security Processes For customers who use MySQL and need to scale beyond the capacity constraints of a single DB Instance for read-heavy database workloads, Amazon RDS provides a Read Replica option. Once you create a read replica, database updates on the source DB instance are replicated to the read replica using MySQL’s native, asynchronous replication. You can create multiple read replicas for a given source DB instance and distribute your application’s read traffic among them. Read replicas can be created with Multi-AZ deployments to gain read scaling benefits in addition to the enhanced database write availability and data durability provided by Multi-AZ deployments. Automatic Software Patching Amazon RDS will make sure that the relational database software powering your deployment stays up-to-date with the latest patches. When necessary, patches are applied during a maintenance window that you can control. You can think of the Amazon RDS maintenance window as an opportunity to control when DB Instance modifications (such as scaling DB Instance class) and software patching occur, in the event either are requested or required. If a “maintenance” event is scheduled for a given week, it will be initiated and completed at some point during the 30-minute maintenance window you identify. The only maintenance events that require Amazon RDS to take your DB Instance offline are scale compute operations (which generally take only a few minutes from start-to finish) or required software patching. Required patching is automatically scheduled only for patches that are security and durability related. Such patching occurs infrequently (typically once every few months) and should seldom require more than a fraction of your maintenance window. If you do not specify a preferred weekly maintenance window when creating your DB Instance, a 30-minute default value is assigned. If you wish to modify when maintenance is performed on your behalf, you can do so by modifying your DB Instance in the AWS Management Console or by using the ModifyDBInstance API. Each of your DB Instances can have different preferred maintenance windows, if you so choose. Running your DB Instance as a Multi-AZ deployment can further reduce the impact of a maintenance event, as Amazon RDS will conduct maintenance via the following steps: 1) Perform maintenance on standby, 2) Promote standby to primary, and 3) Perform maintenance on old primary, which becomes the new standby. Archived When an Amazon RDS DB Instance deletion API (DeleteDBInstance) is run, the DB Instance is marked for deletion. Once the instance no longer indicates ‘deleting’ status, it has been removed. At this point the instance is no longer accessible and unless a final Page 60 Amazon Web Services Amazon Web Services: Overview of Security Processes snapshot copy was asked for, it cannot be restored and will not be listed by any of the tools or APIs. Event Notification You can receive notifications of a variety of important events that can occur on your RDS instance, such as whether the instance was shut down, a backup was started, a failover occurred, the security group was changed, or your storage space is low. The Amazon RDS service groups events into categories that you can subscribe to so that you can be notified when an event in that category occurs. You can subscribe to an event category for a DB instance, DB snapshot, DB security group, or for a DB parameter group. RDS events are published via AWS SNS and sent to you as an email or text message. For more information about RDS notification event categories, refer to the Amazon Relational Database Service User Guide. Amazon Redshift Security Amazon Redshift is a petabyte-scale SQL data warehouse service that runs on highly optimized and managed AWS compute and storage resources. The service has been architected to not only scale up or down rapidly, but to significantly improve query speeds – even on extremely large datasets. To increase performance, Redshift uses techniques such as columnar storage, data compression, and zone maps to reduce the amount of IO needed to perform queries. It also has a massively parallel processing (MPP) architecture, parallelizing and distributing SQL operations to take advantage of all available resources. When you create a Redshift data warehouse, you provision a single-node or multi-node cluster, specifying the type and number of nodes that will make up the cluster. The node type determines the storage size, memory, and CPU of each node. Each multi-node cluster includes a leader node and two or more compute nodes. A leader node manages connections, parses queries, builds execution plans, and manages query execution in the compute nodes. The compute nodes store data, perform computations, and run queries as directed by the leader node. The leader node of each cluster is accessible through ODBC and JDBC endpoints, using standard PostgreSQL drivers. The compute nodes run on a separate, isolated network and are never accessed directly. After you provision a cluster, you can upload your dataset and perform data analysis queries by using common SQL- based tools and business intelligence applications. Archived Page 61 Amazon Web Services Amazon Web Services: Overview of Security Processes Cluster Access By default, clusters that you create are closed to everyone. Amazon Redshift enables you to configure firewall rules (security groups) to control network access to your data warehouse cluster. You can also run Redshift inside an Amazon VPC to isolate your data warehouse cluster in your own virtual network and connect it to your existing IT infrastructure using industry-standard encrypted IPsec VPN. The AWS account that creates the cluster has full access to the cluster. Within your AWS account, you can use AWS IAM to create user accounts and manage permissions for those accounts. By using IAM, you can grant different users permission to perform only the cluster operations that are necessary for their work. Like all databases, you must grant permission in Redshift at the database level in addition to granting access at the resource level. Database users are named user accounts that can connect to a database and are authenticated when they login to Amazon Redshift. In Redshift, you grant database user permissions on a per-cluster basis instead of on a per-table basis. However, a user can see data only in the table rows that were generated by his own activities; rows generated by other users are not visible to him. The user who creates a database object is its owner. By default, only a superuser or the owner of an object can query, modify, or grant permissions on the object. For users to use an object, you must grant the necessary permissions to the user or the group that contains the user. And only the owner of an object can modify or delete it. Data Backups Amazon Redshift distributes your data across all compute nodes in a cluster. When you run a cluster with at least two compute nodes, data on each node will always be mirrored on disks on another node, reducing the risk of data loss. In addition, all data written to a node in your cluster is continuously backed up to Amazon S3 using snapshots. Redshift stores your snapshots for a user-defined period, which can be from one to thirty-five days. You can also take your own snapshots at any time; these snapshots leverage all existing system snapshots and are retained until you explicitly delete them. Amazon Redshift continuously monitors the health of the cluster and automatically re replicates data from failed drives and replaces nodes as necessary. All of this happens without any effort on your part, although you may see a slight performance degradation during the re-replication process. Archived Page 62 Amazon Web Services Amazon Web Services: Overview of Security Processes You can use any system or user snapshot to restore your cluster using the AWS Management Console or the Amazon Redshift APIs. Your cluster is available as soon as the system metadata has been restored and you can start running queries while user data is spooled down in the background. Data Encryption When creating a cluster, you can choose to encrypt it in order to provide additional protection for your data at rest. When you enable encryption in your cluster, Amazon Redshift stores all data in user-created tables in an encrypted format using hardware accelerated AES-256 block encryption keys. This includes all data written to disk as well as any backups. Amazon Redshift uses a four-tier, key-based architecture for encryption. These keys consist of data encryption keys, a database key, a cluster key, and a master key: • Data encryption keys encrypt data blocks in the cluster. Each data block is assigned a randomly-generated AES- 256 key. These keys are encrypted by using the database key for the cluster. • The database key encrypts data encryption keys in the cluster. The database key is a randomly-generated AES- 256 key. It is stored on disk in a separate network from the Amazon Redshift cluster and passed to the cluster across a secure channel. • The cluster key encrypts the database key for the Amazon Redshift cluster. You can use either AWS or a hardware security module (HSM) to store the cluster key. HSMs provide direct control of key generation and management, and make key management separate and distinct from the application and the database. • The master key encrypts the cluster key if it is stored in AWS. The master key encrypts the cluster-key-encrypted database key if the cluster key is stored in an HSM. You can have Redshift rotate the encryption keys for your encrypted clusters at any time. As part of the rotation process, keys are also updated for all of the cluster's automatic and manual snapshots. Note: Enabling encryption in your cluster will impact performance, even though it is hardware accelerated. Encryption also applies to backups. Archived When restoring from an encrypted snapshot, the new cluster will be encrypted as well. Page 63 Amazon Web Services Amazon Web Services: Overview of Security Processes To encrypt your table load data files when you upload them to Amazon S3, you can use Amazon S3 server-side encryption. When you load the data from Amazon S3, the COPY command will decrypt the data as it loads the table. Database Audit Logging Amazon Redshift logs all SQL operations, including connection attempts, queries, and changes to your database. You can access these logs using SQL queries against system tables or choose to have them downloaded to a secure Amazon S3 bucket. You can then use these audit logs to monitor your cluster for security and troubleshooting purposes. Automatic Software Patching Amazon Redshift manages all the work of setting up, operating, and scaling your data warehouse, including provisioning capacity, monitoring the cluster, and applying patches and upgrades to the Amazon Redshift engine. Patches are applied only during specified maintenance windows. SSL Connections To protect your data in transit within the AWS cloud, Amazon Redshift uses hardware accelerated SSL to communicate with Amazon S3 or Amazon DynamoDB for COPY, UNLOAD, backup, and restore operations. You can encrypt the connection between your client and the cluster by specifying SSL in the parameter group associated with the cluster. To have your clients also authenticate the Redshift server, you can install the public key (.pem file) for the SSL certificate on your client and use the key to connect to your clusters. Amazon Redshift offers the newer, stronger cipher suites that use the Elliptic Curve Diffie-Hellman Ephemeral protocol. ECDHE allows SSL clients to provide Perfect Forward Secrecy between the client and the Redshift cluster. Perfect Forward Secrecy uses session keys that are ephemeral and not stored anywhere, which prevents the decoding of captured data by unauthorized third parties, even if the secret long-term key itself is compromised. You do not need to configure anything in Amazon Redshift to enable ECDHE; if you connect from a SQL client tool that uses ECDHE to encrypt communication between the client and server, Amazon Redshift will use the provided cipher list to make the appropriate connection. Archived Page 64 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon ElastiCache Security Amazon ElastiCache is a web service that makes it easy to set up, manage, and scale distributed in-memory cache environments in the cloud. The service improves the performance of web applications by allowing you to retrieve information from a fast, managed, in-memory caching system, instead of relying entirely on slower disk-based databases. It can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&A portals) or compute-intensive workloads (such as a recommendation engine). Caching improves application performance by storing critical pieces of data in memory for low-latency access. Cached information may include the results of I/O-intensive database queries or the results of computationally-intensive calculations. The Amazon ElastiCache service automates time-consuming management tasks for in memory cache environments, such as patch management, failure detection, and recovery. It works in conjunction with other Amazon Web Services (such as Amazon EC2, Amazon CloudWatch, and Amazon SNS) to provide a secure, high-performance, and managed in- memory cache. For example, an application running in Amazon EC2 can securely access an Amazon ElastiCache Cluster in the same region with very low latency. Using the Amazon ElastiCache service, you create a Cache Cluster, which is a collection of one or more Cache Nodes, each running an instance of the Memcached service. A Cache Node is a fixed-size chunk of secure, network- attached RAM. Each Cache Node runs an instance of the Memcached service, and has its own DNS name and port. Multiple types of Cache Nodes are supported, each with varying amounts of associated memory. A Cache Cluster can be set up with a specific number of Cache Nodes and a Cache Parameter Group that controls the properties for each Cache Node. All Cache Nodes within a Cache Cluster are designed to be of the same Node Type and have the same parameter and security group settings. Amazon ElastiCache allows you to control access to your Cache Clusters using Cache Security Groups. A Cache Security Group acts like a firewall, controlling network access to your Cache Cluster. By default, network access is turned off to your Cache Clusters. If you want your applications to access your Cache Cluster, you must explicitly enable access from hosts in specific EC2 security groups. Once ingress rules are configured, the same rules apply to all Cache Clusters associated with that Cache Security Group. Archived To allow network access to your Cache Cluster, create a Cache Security Group and use the Authorize Cache Security Group Ingress API or CLI command to authorize the desired EC2 security group (which in turn specifies the EC2 instances allowed). IP Page 65 Amazon Web Services Amazon Web Services: Overview of Security Processes range based access control is currently not enabled for Cache Clusters. All clients to a Cache Cluster must be within the EC2 network, and authorized via Cache Security Groups. ElastiCache for Redis provides backup and restore functionality, where you can create a snapshot of your entire Redis cluster as it exists at a specific point in time. You can schedule automatic, recurring daily snapshots or you can create a manual snapshot at any time. For automatic snapshots, you specify a retention period; manual snapshots are retained until you delete them. The snapshots are stored in Amazon S3 with high durability, and can be used for warm starts, backups, and archiving. Application Services Amazon Web Services offers a variety of managed services to use with your applications, including services that provide application streaming, queueing, push notification, email delivery, search, and transcoding. Amazon CloudSearch Security Amazon CloudSearch is a managed service in the cloud that makes it easy to set up, manage, and scale a search solution for your website. Amazon CloudSearch enables you to search large collections of data such as web pages, document files, forum posts, or product information. It enables you to quickly add search capabilities to your website without having to become a search expert or worry about hardware provisioning, setup, and maintenance. As your volume of data and traffic fluctuates, Amazon CloudSearch automatically scales to meet your needs. An Amazon CloudSearch domain encapsulates a collection of data you want to search, the search instances that process your search requests, and a configuration that controls how your data is indexed and searched. You create a separate search domain for each collection of data you want to make searchable. For each domain, you configure indexing options that describe the fields you want to include in your index and how you want to us them, text options that define domain-specific stopwords, stems, and synonyms, rank expressions that you can use to customize how search results are ranked, and access policies that control access to the domain’s document and search endpoints. All Amazon CloudSearch configuration requests must be authenticated using standard AWS authentication. Archived Page 66 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon CloudSearch provides separate endpoints for accessing the configuration, search, and document services: • The configuration service is accessed through a general endpoint: cloudsearch.us-east-1.amazonaws.com • The document service endpoint is used to submit documents to the domain for indexing and is accessed through a domain-specific endpoint: http://doc domainname-domainid.us-east- 1.cloudsearch.amazonaws.com/ • The search endpoint is used to submit search requests to the domain and is accessed through a domain-specific endpoint: http://search- domainname domainid.us-east-1.cloudsearch.amazonaws.com Like all AWS Services, Amazon CloudSearch requires that every request made to its control API be authenticated so only authenticated users can access and manage your CloudSearch domain. API requests are signed with an HMAC- SHA1 or HMAC-SHA256 signature calculated from the request and the user’s AWS Secret Access key. Additionally, the Amazon CloudSearch control API is accessible via SSL-encrypted endpoints. You can control access to Amazon CloudSearch management functions by creating users under your AWS Account using AWS IAM, and controlling which CloudSearch operations these users have permission to perform. Amazon Simple Queue Service (Amazon SQS) Security Amazon SQS is a highly reliable, scalable message queuing service that enables asynchronous message-based communication between distributed components of an application. The components can be computers or Amazon EC2 instances or a combination of both. With Amazon SQS, you can send any number of messages to an Amazon SQS queue at any time from any component. The messages can be retrieved from the same component or a different one right away or at a later time (within 4 days). Messages are highly durable; each message is persistently stored in highly available, highly reliable queues. Multiple processes can read/write from/to an Amazon SQS queue at the same time without interfering with each other. Amazon SQS access is granted based on an AWS Account or a user created with AWS IAM. Once authenticated, the AWS Account has full access to all user operations. An AWS IAM user, however, only has access to the operations and queues for which they have been granted access via policy. By default, access to each individual queue is restricted to the AWS Account that created it. However, you can allow other access to a queue, using either an SQS-generated policy or a policy you write. Archived Page 67 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon SQS is accessible via SSL-encrypted endpoints. The encrypted endpoints are accessible from both the Internet and from within Amazon EC2. Data stored within Amazon SQS is not encrypted by AWS; however, the user can encrypt data before it is uploaded to Amazon SQS, provided that the application utilizing the queue has a means to decrypt the message when retrieved. Encrypting messages before sending them to Amazon SQS helps protect against access to sensitive customer data by unauthorized persons, including AWS. Amazon Simple Notification Service (Amazon SNS) Security Amazon Simple Notification Service (Amazon SNS) is a web service that makes it easy to set up, operate, and send notifications from the cloud. It provides developers with a highly scalable, flexible, and cost-effective capability to publish messages from an application and immediately deliver them to subscribers or other applications. Amazon SNS provides a simple web services interface that can be used to create topics that customers want to notify applications (or people) about, subscribe clients to these topics, publish messages, and have these messages delivered over clients’ protocol of choice (i.e., HTTP/HTTPS, email, etc.). Amazon SNS delivers notifications to clients using a “push” mechanism that eliminates the need to periodically check or “poll” for new information and updates. Amazon SNS can be leveraged to build highly reliable, event-driven workflows and messaging applications without the need for complex middleware and application management. The potential uses for Amazon SNS include monitoring applications, workflow systems, time-sensitive information updates, mobile applications, and many others. Amazon SNS provides access control mechanisms so that topics and messages are secured against unauthorized access. Topic owners can set policies for a topic that restrict who can publish or subscribe to a topic. Additionally, topic owners can encrypt transmission by specifying that the delivery mechanism must be HTTPS. Amazon SNS access is granted based on an AWS Account or a user created with AWS IAM. Once authenticated, the AWS Account has full access to all user operations. An AWS IAM user, however, only has access to the operations and topics for which they have been granted access via policy. By default, access to each individual topic is restricted to the AWS Account that created it. However, you can allow other access to SNS, using either an SNS-generated policy or a policy you write. Archived Page 68 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon Simple Workflow Service (Amazon SWF) Security The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. Using Amazon SWF, you can structure the various processing steps in an application as “tasks” that drive work in distributed applications, and Amazon SWF coordinates these tasks in a reliable and scalable manner. Amazon SWF manages task execution dependencies, scheduling, and concurrency based on a developer’s application logic. The service stores tasks, dispatches them to application components, tracks their progress, and keeps their latest state. Amazon SWF provides simple API calls that can be executed from code written in any language and run on your EC2 instances, or any of your machines located anywhere in the world that can access the Internet. Amazon SWF acts as a coordination hub with which your application hosts interact. You create desired workflows with their associated tasks and any conditional logic you wish to apply and store them with Amazon SWF. Amazon SWF access is granted based on an AWS Account or a user created with AWS IAM. All actors that participate in the execution of a workflow— deciders, activity workers, workflow administrators—must be IAM users under the AWS Account that owns the Amazon SWF resources. You cannot grant users associated with other AWS Accounts access to your Amazon SWF workflows. An AWS IAM user, however, only has access to the workflows and resources for which they have been granted access via policy. Amazon Simple Email Service (Amazon SES) Security Amazon Simple Email Service (SES), built on Amazon’s reliable and scalable infrastructure, is a mail service that can both send and receive mail on behalf of your domain. Amazon SES helps you maximize email deliverability and stay informed of the delivery status of your emails. Amazon SES integrates with other AWS services, making it easy to send emails from applications being hosted on services such as Amazon EC2. Unfortunately, with other email systems, it's possible for a spammer to falsify an email header and spoof the originating email address so that it appears as though the email originated from a different source. To mitigate these problems, Amazon SES requires users to verify their email address or domain in order to confirm that they own it and to prevent others from using it. To verify a domain, Amazon SES requires the sender to publish a DNS record that Amazon SES supplies as proof of control over the domain. Archived Page 69 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon SES periodically reviews domain verification status, and revokes verification in cases where it is no longer valid. Amazon SES takes proactive steps to prevent questionable content from being sent, so that ISPs receive consistently high-quality email from our domains and therefore view Amazon SES as a trusted email origin. Below are some of the features that maximize deliverability and dependability for all of our senders: • Amazon SES uses content-filtering technologies to help detect and block messages containing viruses or malware before they can be sent. • Amazon SES maintains complaint feedback loops with major ISPs. Complaint feedback loops indicate which emails a recipient marked as spam. Amazon SES provides you access to these delivery metrics to help guide your sending strategy. • Amazon SES uses a variety of techniques to measure the quality of each user’s sending. These mechanisms help identify and disable attempts to use Amazon SES for unsolicited mail, and detect other sending patterns that would harm Amazon SES’s reputation with ISPs, mailbox providers, and anti-spam services. • Amazon SES supports authentication mechanisms such as Sender Policy Framework (SPF) and DomainKeys Identified Mail (DKIM). When you authenticate an email, you provide evidence to ISPs that you own the domain. Amazon SES makes it easy for you to authenticate your emails. If you configure your account to use Easy DKIM, Amazon SES will DKIM-sign your emails on your behalf, so you can focus on other aspects of your email-sending strategy. To ensure optimal deliverability, we recommend that you authenticate your emails. As with other AWS services, you use security credentials to verify who you are and whether you have permission to interact with Amazon SES. For information about which credentials to use, see Using Credentials with Amazon SES. Amazon SES also integrates with AWS IAM so that you can specify which Amazon SES API actions a user can perform. If you choose to communicate with Amazon SES through its SMTP interface, you are required to encrypt your connection using TLS. Amazon SES supports two mechanisms for establishing a TLS-encrypted connection: STARTTLS and TLS Wrapper. If you choose to communicate with Amazon SES over HTTP, then all communication will be protected by TLS through Amazon SES’s HTTPS endpoint. When delivering email to its Archived Page 70 Amazon Web Services Amazon Web Services: Overview of Security Processes final destination, Amazon SES encrypts the email content with opportunistic TLS, if supported by the receiver. Amazon Elastic Transcoder Service Security The Amazon Elastic Transcoder service simplifies and automates what is usually a complex process of converting media files from one format, size, or quality to another. The Elastic Transcoder service converts standard-definition (SD) or high-definition (HD) video files as well as audio files. It reads input from an Amazon S3 bucket, transcodes it, and writes the resulting file to another Amazon S3 bucket. You can use the same bucket for input and output, and the buckets can be in any AWS region. The Elastic Transcoder accepts input files in a wide variety of web, consumer, and professional formats. Output file types include the MP3, MP4, OGG, TS, WebM, HLS using MPEG-2 TS, and Smooth Streaming using fmp4 container types, storing H.264 or VP8 video and AAC, MP3, or Vorbis audio. You'll start with one or more input files, and create transcoding jobs in a type of workflow called a transcoding pipeline for each file. When you create the pipeline, you'll specify input and output buckets as well as an IAM role. Each job must reference a media conversion template called a transcoding preset, and will result in the generation of one or more output files. A preset tells the Elastic Transcoder what settings to use when processing a particular input file. You can specify many settings when you create a preset, including the sample rate, bit rate, resolution (output height and width), the number of reference and keyframes, a video bit rate, some thumbnail creation options, etc. A best effort is made to start jobs in the order in which they’re submitted, but this is not a hard guarantee and jobs typically finish out of order since they are worked on in parallel and vary in complexity. You can pause and resume any of your pipelines if necessary. Elastic Transcoder supports the use of SNS notifications when it starts and finishes each job, and when it needs to tell you that it has detected an error or warning condition. The SNS notification parameters are associated with each pipeline. It can also use the List Jobs by Status function to find all of the jobs with a given status (e.g., "Completed") or the Read Job function to retrieve detailed information about a particular job. Archived Like all other AWS services, Elastic Transcoder integrates with AWS Identity and Access Management (IAM), which allows you to control access to the service and to other AWS resources that Elastic Transcoder requires, including Amazon S3 buckets Page 71 Amazon Web Services Amazon Web Services: Overview of Security Processes and Amazon SNS topics. By default, IAM users have no access to Elastic Transcoder or to the resources that it uses. If you want IAM users to be able to work with Elastic Transcoder, you must explicitly grant them permissions. Amazon Elastic Transcoder requires every request made to its control API be authenticated so only authenticated processes or users can create, modify, or delete their own Amazon Transcoder pipelines and presets. Requests are signed with an HMAC-SHA256 signature calculated from the request and a key derived from the user’s secret key. Additionally, the Amazon Elastic Transcoder API is only accessible via SSL encrypted endpoints. Durability is provided by Amazon S3, where media files are redundantly stored on multiple devices across multiple facilities in an Amazon S3 region. For added protection against users accidently deleting media files, you can use the Versioning feature in Amazon S3 to preserve, retrieve, and restore every version of every object stored in an Amazon S3 bucket. You can further protect versions using Amazon S3 Versioning's MFA Delete feature. Once enabled for an Amazon S3 bucket, each version deletion request must include the six-digit code and serial number from your multi-factor authentication device. Amazon AppStream 2.0 Security The Amazon AppStream 2.0 service provides a framework for running streaming applications, particularly applications that require lightweight clients running on mobile devices. It enables you to store and run your application on powerful, parallel processing GPUs in the cloud and then stream input and output to any client device. This can be a pre-existing application that you modify to work with Amazon AppStream 2.0 or a new application that you design specifically to work with the service. The Amazon AppStream 2.0 SDK simplifies the development of interactive streaming applications and client applications. The SDK provides APIs that connect your customers’ devices directly to your application, capture and encode audio and video, stream content across the Internet in near real-time, decode content on client devices, and return user input to the application. Because your application's processing occurs in the cloud, it can scale to handle extremely large computational loads. Archived Amazon AppStream 2.0 deploys streaming applications on Amazon EC2. When you add a streaming application through the AWS Management Console, the service creates the AMI required to host your application and makes your application available Page 72 Amazon Web Services Amazon Web Services: Overview of Security Processes to streaming clients. The service scales your application as needed within the capacity limits you have set to meet demand. Clients using the Amazon AppStream 2.0 SDK automatically connect to your streamed application. In most cases, you’ll want to ensure that the user running the client is authorized to use your application before letting him obtain a session ID. We recommend that you use some sort of entitlement service, which is a service that authenticates clients and authorizes their connection to your application. In this case, the entitlement service will also call into the Amazon AppStream 2.0 REST API to create a new streaming session for the client. After the entitlement service creates a new session, it returns the session identifier to the authorized client as a single-use entitlement URL. The client then uses the entitlement URL to connect to the application. Your entitlement service can be hosted on an Amazon EC2 instance or on AWS Elastic Beanstalk. Amazon AppStream 2.0 utilizes an AWS CloudFormation template that automates the process of deploying a GPU EC2 instance that has the AppStream 2.0 Windows Application and Windows Client SDK libraries installed; is configured for SSH, RDC, or VPN access; and has an elastic IP address assigned to it. By using this template to deploy your standalone streaming server, all you need to do is upload your application to the server and run the command to launch it. You can then use the Amazon AppStream 2.0 Service Simulator tool to test your application in standalone mode before deploying it into production. Amazon AppStream 2.0 also utilizes the STX Protocol to manage the streaming of your application from AWS to local devices. The Amazon AppStream 2.0 STX Protocol is a proprietary protocol used to stream high-quality application video over varying network conditions; it monitors network conditions and automatically adapts the video stream to provide a low-latency and high- resolution experience to your customers. It minimizes latency while syncing audio and video as well as capturing input from your customers to be sent back to the application running in AWS. Analytics Services Amazon Web Services provides cloud-based analytics services to help you process and analyze any volume of data, whether your need is for managed Hadoop clusters, real time streaming data, petabyte scale data warehousing, or orchestration. Archived Page 73 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon EMR Security Amazon EMR is a managed web service you can use to run Hadoop clusters that process vast amounts of data by distributing the work and data among several servers. It utilizes an enhanced version of the Apache Hadoop framework running on the web scale infrastructure of Amazon EC2 and Amazon S3. You simply upload your input data and a data processing application into Amazon S3. Amazon EMR then launches the number of Amazon EC2 instances you specify. The service begins the job flow execution while pulling the input data from Amazon S3 into the launched Amazon EC2 instances. Once the job flow is finished, Amazon EMR transfers the output data to Amazon S3, where you can then retrieve it or use it as input in another job flow. When launching job flows on your behalf, Amazon EMR sets up two Amazon EC2 security groups: one for the master nodes and another for the slaves. The master security group has a port open for communication with the service. It also has the SSH port open to allow you to SSH into the instances, using the key specified at startup. The slaves start in a separate security group, which only allows interaction with the master instance. By default, both security groups are set up to not allow access from external sources, including Amazon EC2 instances belonging to other customers. Since these are security groups within your account, you can reconfigure them using the standard EC2 tools or dashboard. To protect customer input and output datasets, Amazon EMR transfers data to and from Amazon S3 using SSL. Amazon EMR provides several ways to control access to the resources of your cluster. You can use AWS IAM to create user accounts and roles and configure permissions that control which AWS features those users and roles can access. When you launch a cluster, you can associate an Amazon EC2 key pair with the cluster, which you can then use when you connect to the cluster using SSH. You can also set permissions that allow users other than the default Hadoop user to submit jobs to your cluster. By default, if an IAM user launches a cluster, that cluster is hidden from other IAM users on the AWS account. This filtering occurs on all Amazon EMR interfaces—the console, CLI, API, and SDKs—and helps prevent IAM users from accessing and inadvertently changing clusters created by other IAM users. It is useful for clusters that are intended to be viewed by only a single IAM user and the main AWS account. You also have the option to make a cluster visible and accessible to all IAM users under a single AWS account. Archived For an additional layer of protection, you can launch the EC2 instances of your EMR cluster into an Amazon VPC, which is like launching it into a private subnet. This allows Page 74 Amazon Web Services Amazon Web Services: Overview of Security Processes you to control access to the entire subnetwork. You can also launch the cluster into a VPC and enable the cluster to access resources on your internal network using a VPN connection. You can encrypt the input data before you upload it to Amazon S3 using any common data encryption tool. If you do encrypt the data before it’s uploaded, you then need to add a decryption step to the beginning of your job flow when Amazon Elastic MapReduce fetches the data from Amazon S3. Amazon Kinesis Security Amazon Kinesis is a managed service designed to handle real-time streaming of big data. It can accept any amount of data, from any number of sources, scaling up and down as needed. You can use Kinesis in situations that call for large-scale, real-time data ingestion and processing, such as server logs, social media or market data feeds, and web clickstream data. Applications read and write data records to Amazon Kinesis in streams. You can create any number of Kinesis streams to capture, store, and transport data. Amazon Kinesis automatically manages the infrastructure, storage, networking, and configuration needed to collect and process your data at the level of throughput your streaming applications need. You don’t have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services to enable real-time capture and storage of large-scale data. Amazon Kinesis also synchronously replicates data across three facilities in an AWS Region, providing high availability and data durability. In Amazon Kinesis, data records contain a sequence number, a partition key, and a data blob, which is an un-interpreted, immutable sequence of bytes. The Amazon Kinesis service does not inspect, interpret, or change the data in the blob in any way. Data records are accessible for only 24 hours from the time they are added to an Amazon Kinesis stream, and then they are automatically discarded. Your application is a consumer of an Amazon Kinesis stream, which typically runs on a fleet of Amazon EC2 instances. A Kinesis application uses the Amazon Kinesis Client Library to read from the Amazon Kinesis stream. The Kinesis Client Library takes care of a variety of details for you including failover, recovery, and load balancing, allowing your application to focus on processing the data as it becomes available. After Archived processing the record, your consumer code can pass it along to another Kinesis stream; write it to an Amazon S3 bucket, a Redshift data warehouse, or a DynamoDB table; or simply discard it. A connector library is available to help you integrate Kinesis with other Page 75 Amazon Web Services Amazon Web Services: Overview of Security Processes AWS services (such as DynamoDB, Redshift, and Amazon S3) as well as third-party products like Apache Storm. You can control logical access to Kinesis resources and management functions by creating users under your AWS Account using AWS IAM, and controlling which Kinesis operations these users have permission to perform. To facilitate running your producer or consumer applications on an Amazon EC2 instance, you can configure that instance with an IAM role. That way, AWS credentials that reflect the permissions associated with the IAM role are made available to applications on the instance, which means you don’t have to use your long-term AWS security credentials. Roles have the added benefit of providing temporary credentials that expire within a short timeframe, which adds an additional measure of protection. See the AWS Identity and Access Management User Guide for more information about IAM roles. The Amazon Kinesis API is only accessible via an SSL-encrypted endpoint (kinesis.us east-1.amazonaws.com) to help ensure secure transmission of your data to AWS. You must connect to that endpoint to access Kinesis, but you can then use the API to direct AWS Kinesis to create a stream in any AWS Region. AWS Data Pipeline Security The AWS Data Pipeline service helps you process and move data between different data sources at specified intervals using data-driven workflows and built-in dependency checking. When you create a pipeline, you define data sources, preconditions, destinations, processing steps, and an operational schedule. Once you define and activate a pipeline, it will run automatically according to the schedule you specified. With AWS Data Pipeline, you don’t have to worry about checking resource availability, managing inter-task dependencies, retrying transient failures/timeouts in individual tasks, or creating a failure notification system. AWS Data Pipeline takes care of launching the AWS services and resources your pipeline needs to process your data (e.g., Amazon EC2 or EMR) and transferring the results to storage (e.g., Amazon S3, RDS, DynamoDB, or EMR). When you use the console, AWS Data Pipeline creates the necessary IAM roles and policies, including a trusted entities list for you. IAM roles determine what your pipeline can access and the actions it can perform. Additionally, when your pipeline creates a resource, such as an EC2 instance, IAM roles determine the EC2 instance's permitted resources and actions. When you create a pipeline, you specify one IAM role that Archived governs your pipeline and another IAM role to govern your pipeline's resources (referred to as a "resource role"), which can be the same role for both. As part of the security best Page 76 Amazon Web Services Amazon Web Services: Overview of Security Processes practice of least privilege, we recommend that you consider the minimum permissions necessary for your pipeline to perform work and define the IAM roles accordingly. Like most AWS services, AWS Data Pipeline also provides the option of secure (HTTPS) endpoints for access via SSL. Deployment and Management Services Amazon Web Services provides a variety of tools to help with the deployment and management of your applications. This includes services that allow you to create individual user accounts with credentials for access to AWS services. It also includes services for creating and updating stacks of AWS resources, deploying applications on those resources, and monitoring the health of those AWS resources. Other tools help you manage cryptographic keys using hardware security modules (HSMs) and log AWS API activity for security and compliance purposes. AWS Identity and Access Management (IAM) IAM allows you to create multiple users and manage the permissions for each of these users within your AWS Account. A user is an identity (within an AWS Account) with unique security credentials that can be used to access AWS Services. IAM eliminates the need to share passwords or keys, and makes it easy to enable or disable a user’s access as appropriate. IAM enables you to implement security best practices, such as least privilege, by granting unique credentials to every user within your AWS Account and only granting permission to access the AWS services and resources required for the users to perform their jobs. IAM is secure by default; new users have no access to AWS until permissions are explicitly granted. IAM is also integrated with the AWS Marketplace, so that you can control who in your organization can subscribe to the software and services offered in the Marketplace. Since subscribing to certain software in the Marketplace launches an EC2 instance to run the software, this is an important access control feature. Using IAM to control access to the AWS Marketplace also enables AWS Account owners to have fine grained control over usage and software costs. IAM enables you to minimize the use of your AWS Account credentials. Once you create IAM user accounts, all interactions with AWS Services and resources should occur with IAM user security credentials. Archived Page 77 Amazon Web Services Amazon Web Services: Overview of Security Processes Roles An IAM role uses temporary security credentials to allow you to delegate access to users or services that normally don't have access to your AWS resources. A role is a set of permissions to access specific AWS resources, but these permissions are not tied to a specific IAM user or group. An authorized entity (e.g., mobile user, EC2 instance) assumes a role and receives temporary security credentials for authenticating to the resources defined in the role. Temporary security credentials provide enhanced security due to their short life- span (the default expiration is 12 hours) and the fact that they cannot be reused after they expire. This can be particularly useful in providing limited, controlled access in certain situations: • Federated (non-AWS) User Access. Federated users are users (or applications) who do not have AWS Accounts. With roles, you can give them access to your AWS resources for a limited amount of time. This is useful if you have non-AWS users that you can authenticate with an external service, such as Microsoft Active Directory, LDAP, or Kerberos. The temporary AWS credentials used with the roles provide identity federation between AWS and your non-AWS users in your corporate identity and authorization system. If your organization supports SAML 2.0 (Security Assertion Markup Language 2.0), you can create trust between your organization as an identity provider (IdP) and other organizations as service providers. In AWS, you can configure AWS as the service provider and use SAML to provide your users with federated single sign on (SSO) to the AWS Management Console or to get federated access to call AWS APIs. Roles are also useful if you create a mobile or web-based application that accesses AWS resources. AWS resources require security credentials for programmatic requests; however, you shouldn't embed long-term security credentials in your application because they are accessible to the application's users and can be difficult to rotate. Instead, you can let users sign in to your application using Login with Amazon, Facebook, or Google, and then use their authentication information to assume a role and get temporary security credentials. Archived Page 78 Amazon Web Services Amazon Web Services: Overview of Security Processes • Cross-Account Access. For organizations who use multiple AWS Accounts to manage their resources, you can set up roles to provide users who have permissions in one account to access resources under another account. For organizations who have personnel who only rarely need access to resources under another account, using roles helps ensures that credentials are provided temporarily, only as needed. • Applications Running on EC2 Instances that Need to Access AWS Resources. If an application runs on an Amazon EC2 instance and needs to make requests for AWS resources such as Amazon S3 buckets or a DynamoDB table, it must have security credentials. Using roles instead of creating individual IAM accounts for each application on each instance can save significant time for customers who manage a large number of instances or an elastically scaling fleet using AWS Auto Scaling. The temporary credentials include a security token, an Access Key ID, and a Secret Access Key. To give a user access to certain resources, you distribute the temporary security credentials to the user you are granting temporary access to. When the user makes calls to your resources, the user passes in the token and Access Key ID, and signs the request with the Secret Access Key. The token will not work with different access keys. How the user passes in the token depends on the API and version of the AWS product the user is making calls to. For more information about temporary security credentials, see AWS Security Token Service API Reference. The use of temporary credentials means additional protection for you because you don’t have to manage or distribute long-term credentials to temporary users. In addition, the temporary credentials get automatically loaded to the target instance so you don’t have to embed them somewhere unsafe like your code. Temporary credentials are automatically rotated or changed multiple times a day without any action on your part, and are stored securely by default. For more information about using IAM roles to auto-provision keys on EC2 instances, see the AWS Identity and Access Management Documentation. Amazon CloudWatch Security Archived Amazon CloudWatch is a web service that provides monitoring for AWS cloud resources, starting with Amazon EC2. It provides customers with visibility into resource utilization, operational performance, and overall demand patterns—including metrics Page 79 Amazon Web Services Amazon Web Services: Overview of Security Processes such as CPU utilization, disk reads and writes, and network traffic. You can set up CloudWatch alarms to notify you if certain thresholds are crossed, or to take other automated actions such as adding or removing EC2 instances if Auto Scaling is enabled. CloudWatch captures and summarizes utilization metrics natively for AWS resources, but you can also have other logs sent to CloudWatch to monitor. You can route your guest OS, application, and custom log files for the software installed on your EC2 instances to CloudWatch, where they will be stored in durable fashion for as long as you'd like. You can configure CloudWatch to monitor the incoming log entries for any desired symbols or messages and to surface the results as CloudWatch metrics. You could, for example, monitor your web server's log files for 404 errors to detect bad inbound links or invalid user messages to detect unauthorized login attempts to your guest OS. Like all AWS Services, Amazon CloudWatch requires that every request made to its control API be authenticated so only authenticated users can access and manage CloudWatch. Requests are signed with an HMAC-SHA1 signature calculated from the request and the user’s private key. Additionally, the Amazon CloudWatch control API is only accessible via SSL- encrypted endpoints. You can further control access to Amazon CloudWatch by creating users under your AWS Account using AWS IAM, and controlling what CloudWatch operations these users have permission to call. AWS CloudHSM Security The AWS CloudHSM service provides customers with dedicated access to a hardware security module (HSM) appliance designed to provide secure cryptographic key storage and operations within an intrusion-resistant, tamper- evident device. You can generate, store, and manage the cryptographic keys used for data encryption so that they are accessible only by you. AWS CloudHSM appliances are designed to securely store and process cryptographic key material for a wide variety of uses such as database encryption, Digital Rights Management (DRM), Public Key Infrastructure (PKI), authentication and authorization, document signing, and transaction processing. They support some of the strongest cryptographic algorithms available, including AES, RSA, and ECC, and many others. Archived The AWS CloudHSM service is designed to be used with Amazon EC2 and VPC, providing the appliance with its own private IP within a private subnet. You can connect to CloudHSM appliances from your EC2 servers through SSL/TLS, which uses two-way Page 80 Amazon Web Services Amazon Web Services: Overview of Security Processes digital certificate authentication and 256-bit SSL encryption to provide a secure communication channel. Selecting CloudHSM service in the same region as your EC2 instance decreases network latency, which can improve your application performance. You can configure a client on your EC2 instance that allows your applications to use the APIs provided by the HSM, including PKCS#11, MS CAPI and Java JCA/JCE (Java Cryptography Architecture/Java Cryptography Extensions). Before you begin using an HSM, you must set up at least one partition on the appliance. A cryptographic partition is a logical and physical security boundary that restricts access to your keys, so only you control your keys and the operations performed by the HSM. AWS has administrative credentials to the appliance, but these credentials can only be used to manage the appliance, not the HSM partitions on the appliance. AWS uses these credentials to monitor and maintain the health and availability of the appliance. AWS cannot extract your keys nor can AWS cause the appliance to perform any cryptographic operation using your keys. The HSM appliance has both physical and logical tamper detection and response mechanisms that erase the cryptographic key material and generate event logs if tampering is detected. The HSM is designed to detect tampering if the physical barrier of the HSM appliance is breached. In addition, after three unsuccessful attempts to access an HSM partition with HSM Admin credentials, the HSM appliance erases its HSM partitions. When your CloudHSM subscription ends and you have confirmed that the contents of the HSM are no longer needed, you must delete each partition and its contents as well as any logs. As part of the decommissioning process, AWS zeroizes the appliance, permanently erasing all key material. AWS CloudTrail Security AWS CloudTrail provides a log of user and system actions affecting AWS resources within your account. For each event recorded, you can see what service was accessed, what action was performed, any parameters for the action, and who made the request. For mutating actions, you can see the result of the action. Not only can you see which one of your users or services performed an action on an AWS service, but you can see whether it was as the AWS root account user or an IAM user, or whether it was with temporary security credentials for a role or federated user. Archived Page 81 Amazon Web Services Amazon Web Services: Overview of Security Processes CloudTrail captures information about API calls to an AWS resource, whether that call was made from the AWS Management Console, CLI, or an SDK. If the API request returned an error, CloudTrail provides the description of the error, including messages for authorization failures. It even captures AWS Management Console sign-in events, creating a log record every time an AWS account owner, a federated user, or an IAM user simply signs into the console. Once you have enabled CloudTrail, event logs are delivered about every 5 minutes to the Amazon S3 bucket of your choice. The log files are organized by AWS Account ID, region, service name, date, and time. You can configure CloudTrail so that it aggregates log files from multiple regions and/or accounts into a single Amazon S3 bucket. By default, a single trail will record and deliver events in all current and future regions. In addition to S3, you can send events to CloudWatch Logs, for custom metrics and alarming, or you can upload the logs to your favorite log management and analysis solutions to perform security analysis and detect user behavior patterns. For rapid response, you can create CloudWatch Events rules to take immediate action to specific events. By default, log files are stored indefinitely. The log files are automatically encrypted using Amazon S3's Server Side Encryption and will remain in the bucket until you choose to delete or archive them. For even more security you can use KMS to encrypt the log files using a key that you own. You can use Amazon S3 lifecycle configuration rules to automatically delete old log files or archive them to Amazon S3 Glacier for additional longevity at significant savings. By enabling the optional log file validation, you can validate that logs have not been added, deleted, or tampered with. Like every other AWS service, you can limit access to CloudTrail to only certain users. You can use IAM to control which AWS users can create, configure, or delete AWS CloudTrail trails as well as which users can start and stop logging. You can control access to the log files by applying IAM or Amazon S3 bucket policies. You can also add an additional layer of security by enabling MFA Delete on your Amazon S3 bucket. Mobile Services AWS mobile services make it easier for you to build, ship, run, monitor, optimize, and scale cloud-powered applications for mobile devices. These services also help you authenticate users to your mobile application, synchronize data, and collect and analyze application usage. Archived Page 82 Amazon Web Services Amazon Web Services: Overview of Security Processes Amazon Cognito Amazon Cognito provides identity and sync services for mobile and web-based applications. It simplifies the task of authenticating users and storing, managing, and syncing their data across multiple devices, platforms, and applications. It provides temporary, limited-privilege credentials for both authenticated and unauthenticated users without having to manage any backend infrastructure. Amazon Cognito works with well-known identity providers like Google, Facebook, and Amazon to authenticate end users of your mobile and web applications. You can take advantage of the identification and authorization features provided by these services instead of having to build and maintain your own. Your application authenticates with one of these identity providers using the provider’s SDK. Once the end user is authenticated with the provider, an OAuth or OpenID Connect token returned from the provider is passed by your application to Cognito, which returns a new Amazon Cognito ID for the user and a set of temporary, limited-privilege AWS credentials. To begin using Amazon Cognito, you create an identity pool through the Amazon Cognito console. The identity pool is a store of user identity information that is specific to your AWS account. During the creation of the identity pool, you will be asked to create a new IAM role or pick an existing one for your end users. An IAM role is a set of permissions to access specific AWS resources, but these permissions are not tied to a specific IAM user or group. An authorized entity (e.g., mobile user, EC2 instance) assumes a role and receives temporary security credentials for authenticating to the AWS resources defined in the role. Temporary security credentials provide enhanced security due to their short life-span (the default expiration is 12 hours) and the fact that they cannot be reused after they expire. The role you select has an impact on which AWS services your end users will be able to access with the temporary credentials. By default, Amazon Cognito creates a new role with limited permissions – end users only have access to the Amazon Cognito Sync service and Amazon Mobile Analytics. If your application needs access to other AWS resources such as Amazon S3 or DynamoDB, you can modify your roles directly from the IAM management console. With Amazon Cognito, there’s no need to create individual AWS accounts or even IAM accounts for every one of your web/mobile app’s end users who will need to access your AWS resources. In conjunction with IAM roles, mobile users can securely access AWS resources and application features, and even save data to the AWS cloud without having to create an account or log in. Archived However, if they choose to do this later, Amazon Cognito merges data and identification information. Because Amazon Cognito stores data locally as well as in the service, your Page 83 Amazon Web Services Amazon Web Services: Overview of Security Processes end users can continue to interact with their data even when they are offline. Their offline data may be stale, but anything they put into the dataset, they can immediately retrieve whether they are online or not. The client SDK manages a local SQLite store so that the application can work even when it is not connected. The SQLite store functions as a cache and is the target of all read and write operations. Cognito's sync facility compares the local version of the data to the cloud version, and pushes up or pulls down deltas as needed. Note that in order to sync data across devices, your identity pool must support authenticated identities. Unauthenticated identities are tied to the device, so unless an end user authenticates, no data can be synced across multiple devices. With Amazon Cognito, your application communicates directly with a supported public identity provider (Amazon, Facebook, or Google) to authenticate users. Amazon Cognito does not receive or store user credentials—only the OAuth or OpenID Connect token received from the identity provider. Once Amazon Cognito receives the token, it returns a new Amazon Cognito ID for the user and a set of temporary, limited-privilege AWS credentials. Each Amazon Cognito identity has access only to its own data in the sync store, and this data is encrypted when stored. In addition, all identity data is transmitted over HTTPS. The unique Amazon Cognito identifier on the device is stored in the appropriate secure location—on iOS for example, the Amazon Cognito identifier is stored in the iOS keychain. User data is cached in a local SQLite database within the application’s sandbox; if you require additional security, you can encrypt this identity data in the local cache by implementing encryption in your application. Amazon Mobile Analytics Amazon Mobile Analytics is a service for collecting, visualizing, and understanding mobile application usage data. It enables you to track customer behaviors, aggregate metrics, and identify meaningful patterns in your mobile applications. Amazon Mobile Analytics automatically calculates and updates usage metrics as the data is received from client devices running your app and displays the data in the console. You can integrate Amazon Mobile Analytics with your application without requiring users of your app to be authenticated with an identity provider (like Google, Facebook, or Amazon). For these unauthenticated users, Mobile Analytics works with Amazon Cognito to provide temporary, limited-privilege credentials. To do this, you first create an identity pool in Amazon Cognito. The identity pool will use IAM roles, which is a set of permissions not tied to a specific IAM user or group but which allows an entity to access Archived Page 84 Amazon Web Services Amazon Web Services: Overview of Security Processes specific AWS resources. The entity assumes a role and receives temporary security credentials for authenticating to the AWS resources defined in the role. By default, Amazon Cognito creates a new role with limited permissions – end users only have access to the Amazon Cognito Sync service and Amazon Mobile Analytics. If your application needs access to other AWS resources such as Amazon S3 or DynamoDB, you can modify your roles directly from the IAM management console. You can integrate the AWS Mobile SDK for Android or iOS into your application or use the Amazon Mobile Analytics REST API to send events from any connected device or service and visualize data in the reports. The Amazon Mobile Analytics API is only accessible via an SSL-encrypted endpoint (https://mobileanalytics.us-east 1.amazonaws.com). Applications AWS applications are managed services that enable you to provide your users with secure, centralized storage and work areas in the cloud. Amazon WorkSpaces Amazon WorkSpaces is a managed desktop service that allows you to quickly provision cloud-based desktops for your users. Simply choose a Windows 7 bundle that best meets the needs of your users and the number of WorkSpaces that you would like to launch. Once the WorkSpaces are ready, users receive an email informing them where they can download the relevant client and log into their WorkSpace. They can then access their cloud-based desktops from a variety of endpoint devices, including PCs, laptops, and mobile devices. However, your organization’s data is never sent to or stored on the end-user device because Amazon WorkSpaces uses PC-over-IP (PCoIP), which provides an interactive video stream without transmitting actual data. The PCoIP protocol compresses, encrypts, and encodes the users’ desktop computing experience and transmits ‘pixels only’ across any standard IP network to end- user devices. In order to access their WorkSpace, users must sign in using a set of unique credentials or their regular Active Directory credentials. When you integrate Amazon WorkSpaces with your corporate Active Directory, each WorkSpace joins your Active Directory domain and can be managed just like any other desktop in your organization. This Archived means that you can use Active Directory Group Policies to manage your users’ Page 85 Amazon Web Services Amazon Web Services: Overview of Security Processes WorkSpaces to specify configuration options that control the desktop. If you choose not to use Active Directory or other type of on-premises directory to manage your user WorkSpaces, you can create a private cloud directory within Amazon WorkSpaces that you can use for administration. To provide an additional layer of security, you can also require the use of multi- factor authentication upon sign in in the form of a hardware or software token. Amazon WorkSpaces supports MFA using an on-premise Remote Authentication Dial in User Service (RADIUS) server or any security provider that supports RADIUS authentication. It currently supports the PAP, CHAP, MS- CHAP1, and MS-CHAP2 protocols, along with RADIUS proxies. Each Workspace resides on its own EC2 instance within a VPC. You can create WorkSpaces in a VPC you already own or have the WorkSpaces service create one for you automatically using the WorkSpaces Quick Start option. When you use the Quick Start option, WorkSpaces not only creates the VPC, but it performs several other provisioning and configuration tasks for you, such as creating an Internet Gateway for the VPC, setting up a directory within the VPC that is used to store user and WorkSpace information, creating a directory administrator account, creating the specified user accounts and adding them to the directory, and creating the WorkSpace instances. Or the VPC can be connected to an on-premises network using a secure VPN connection to allow access to an existing on-premises Active Directory and other intranet resources. You can add a security group that you create in your Amazon VPC to all the WorkSpaces that belong to your Directory. This allows you to control network access from Amazon WorkSpaces in your VPC to other resources in your Amazon VPC and on-premises network. Persistent storage for WorkSpaces is provided by Amazon EBS and is automatically backed up twice a day to Amazon S3. If WorkSpaces Sync is enabled on a WorkSpace, the folder a user chooses to sync will be continuously backed up and stored in Amazon S3. You can also use WorkSpaces Sync on a Mac or PC to sync documents to or from your WorkSpace so that you can always have access to your data regardless of the desktop computer you are using. Because it’s a managed service, AWS takes care of several security and maintenance tasks like daily backups and patching. Updates are delivered automatically to your WorkSpaces during a weekly maintenance window. You can control how patching is configured for a user’s WorkSpace. By default, Windows Update is turned on, but you Archived have the ability to customize these settings, or use an alternative patch management approach if you desire. For the underlying OS, Windows Update is enabled by default Page 86 Amazon Web Services Amazon Web Services: Overview of Security Processes on WorkSpaces, and configured to install updates on a weekly basis. You can use an alternative patching approach or to configure Windows Update to perform updates at a time of your choosing. You can use IAM to control who on your team can perform administrative functions like creating or deleting WorkSpaces or setting up user directories. You can also set up a WorkSpace for directory administration, install your favorite Active Directory administration tools, and create organizational units and Group Policies in order to more easily apply Active Directory changes for all your WorkSpaces users. Amazon WorkDocs Amazon WorkDocs is a managed enterprise storage and sharing service with feedback capabilities for user collaboration. Users can store any type of file in a WorkDocs folder and allow others to view and download them. Commenting and annotation capabilities work on certain file types such as MS Word, and without requiring the application that was used to originally create the file. WorkDocs notifies contributors about review activities and deadlines via email and performs versioning of files that you have synced using the WorkDocs Sync application. User information is stored in an Active Directory-compatible network directory. You can either create a new directory in the cloud, or connect Amazon WorkDocs to your on premises directory. When you create a cloud directory using WorkDocs’ quick start setup, it also creates a directory administrator account with the administrator email as the username. An email is sent to your administrator with instructions to complete registration. The administrator then uses this account to manage your directory. When you create a cloud directory using WorkDocs’ quick start setup, it also creates and configures a VPC for use with the directory. If you need more control over the directory configuration, you can choose the standard setup, which allows you to specify your own directory domain name, as well as one of your existing VPCs to use with the directory. If you want to use one of your existing VPCs, the VPC must have an Internet gateway and at least two subnets. Each of the subnets must be in a different Availability Zone. Using the Amazon WorkDocs Management Console, administrators can view audit logs to track file and user activity by time, IP address, and device, and choose whether to allow users to share files with others outside their organization. Users can then control who can access individual files and disable downloads of files they share. Archived Page 87 Amazon Web Services Amazon Web Services: Overview of Security Processes All data in transit is encrypted using industry-standard SSL. The WorkDocs web and mobile applications and desktop sync clients transmit files directly to Amazon WorkDocs using SSL. WorkDocs users can also utilize Multi-Factor Authentication, or MFA, if their organization has deployed a Radius server. MFA uses the following factors: username, password, and methods supported by the Radius server. The protocols supported are PAP, CHAP, MS-CHAPv1, and MS- CHAPv2. You choose the AWS Region where each WorkDocs site’s files are stored. Amazon WorkDocs is currently available in the US-East (Virginia), US-West (Oregon), and EU (Ireland) AWS Regions. All files, comments, and annotations stored in WorkDocs are automatically encrypted with AES-256 encryption. Document Revisions Date Description March 2020 Updated compliance certifications, hypervisor, AWS Snowball. February 2019 Added information about deleting objects in Amazon S3 Glacier. December 2018 Edit made to the Amazon Redshift Security topic. May 2017 Added section on AWS Config Security Checks. April 2017 Added section on Amazon Elastic File System. March 2017 Migrated into new format. January 2017 Updated regions. Archived Page 88 
Amazon Web Services – Using AWS for Disaster Recovery October 2014 Using Amazon Web Services for Disaster Recovery October 2014 Glen Robinson, Attila Narin, and Chris Elleman Page 1 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Page 2 of 22 Contents Introduction ...............................................................................................................................................................3 Recovery Time Objective and Recovery Point Objective ................................................................................................4 Traditional DR Investment Practices ............................................................................................................................4 AWS Services and Features Essential for Disaster Recovery ...........................................................................................5 Example Disaster Recovery Scenarios with AWS ...........................................................................................................9 Backup and Restore ................................................................................................................................................9 Pilot Light for Quick Recovery into AWS ................................................................................................................. 11 Warm Standby Solution in AWS ............................................................................................................................. 14 Multi-Site Solution Deployed on AWS and On-Site .................................................................................................. 16 AWS Production to an AWS DR Solution Using Multiple AWS Regions ...................................................................... 18 Replication of Data ................................................................................................................................................... 18 Failing Back from a Disaster....................................................................................................................................... 19 Improving Your DR Plan ............................................................................................................................................ 20 Software Licensing and DR ........................................................................................................................................ 21 Conclusion ............................................................................................................................................................... 21 Further Reading........................................................................................................................................................ 22 Document Revisions ................................................................................................................................................. 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Abstract In the event of a disaster, you can quickly launch resources in Amazon Web Services (AWS) to ensure business continuity. This whitepaper highlights AWS services and features that you can leverage for your disaster recovery (DR) processes to significantly minimize the impact on your data, your system, and your overall business operations. The whitepaper also includes scenarios that show you, step-by-step, how to improve your DR plan and leverage the full potential of the AWS cloud for disaster recovery. Introduction Disaster recovery (DR) is about preparing for and recovering from a disaster. Any event that has a negative impact on a company’s business continuity or finances could be termed a disaster. This includes hardware or software failure, a network outage, a power outage, physical damage to a building like fire or flooding, human error, or some other significant event. To minimize the impact of a disaster, companies invest time and resources to plan and prepare, to train employees, and to document and update processes. The amount of investment for DR planning for a particular system can vary dramatically depending on the cost of a potential outage. Companies that have traditional physical environments typically must duplicate their infrastructure to ensure the availability of spare capacity in the event of a disaster. The infrastructure needs to be procured, installed, and maintained so that it is ready to support the anticipated capacity requirements. During normal operations, the infrastructure typically is under-utilized or over-provisioned. With Amazon Web Services (AWS), your company can scale up its infrastructure on an as-needed, pay-as-you-go basis. You get access to the same highly secure, reliable, and fast infrastructure that Amazon uses to run its own global network of websites. AWS also gives you the flexibility to quickly change and optimize resources during a DR event, which can result in significant cost savings. This whitepaper outlines best practices to improve your DR processes, from minimal investments to full-scale availability and fault tolerance, and shows you how you can use AWS services to reduce cost and ensure business continuity during a DR event. Page 3 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Recovery Time Objective and Recovery Point Objective This whitepaper uses two common industry terms for disaster planning: Recovery time objective (RTO)1 — The time it takes after a disruption to restore a business process to its service level, as defined by the operational level agreement (OLA). For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM. Recovery point objective (RPO)2 — The acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before 11:00 AM. Data loss will span only one hour, between 11:00 AM and 12:00 PM (noon). A company typically decides on an acceptable RTO and RPO based on the financial impact to the business when systems are unavailable. The company determines financial impact by considering many factors, such as the loss of business and damage to its reputation due to downtime and the lack of systems availability. IT organizations then plan solutions to provide cost-effective system recovery based on the RPO within the timeline and the service level established by the RTO. Traditional DR Investment Practices A traditional approach to DR involves different levels of off-site duplication of data and infrastructure. Critical business services are set up and maintained on this infrastructure and tested at regular intervals. The disaster recovery environment’s location and the source infrastructure should be a significant physical distance apart to ensure that the disaster recovery environment is isolated from faults that could impact the source site. At a minimum, the infrastructure that is required to support the duplicate environment should include the following:  Facilities to house the infrastructure, including power and cooling.  Security to ensure the physical protection of assets.  Suitable capacity to scale the environment.  Support for repairing, replacing, and refreshing the infrastructure.  Contractual agreements with an Internet service provider (ISP) to provide Internet connectivity that can sustain bandwidth utilization for the environment under a full load.  Network infrastructure such as firewalls, routers, switches, and load balancers.  Enough server capacity to run all mission-critical services, including storage appliances for the supporting data, and servers to run applications and backend services such as user authentication, Domain Name System (DNS), Dynamic Host Configuration Protocol (DHCP), monitoring, and alerting. 1From http://en.wikipedia.org/wiki/Recovery_time_objective 2From http://en.wikipedia.org/wiki/Recovery_point_objective Page 4 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 AWS Services and Features Essential for Disaster Recovery Before we discuss the various approaches to DR, it is important to review the AWS services and features that are the most relevant to disaster recovery. This section provides a summary. In the preparation phase of DR, it is important to consider the use of services and features that support data migration and durable storage, because they enable you to restore backed-up, critical data to AWS when disaster strikes. For some of the scenarios that involve either a scaled-down or a fully scaled deployment of your system in AWS, compute resources will be required as well. When reacting to a disaster, it is essential to either quickly commission compute resources to run your system in AWS or to orchestrate the failover to already running resources in AWS. The essential infrastructure pieces include DNS, networking features, and various Amazon Elastic Compute Cloud (Amazon EC2) features described later in this section. Regions Amazon Web Services are available in multiple regions around the globe, so you can choose the most appropriate location for your DR site, in addition to the site where your system is fully deployed. AWS has multiple general purpose regions in the Americas, EMEA, and Asia Pacific that anyone with an AWS account can access. Special-use regions are also available for government agencies and for China. See the full list of available regions here. Storage Amazon Simple Storage Service (Amazon S3) provides a highly durable storage infrastructure designed for mission critical and primary data storage. Objects are redundantly stored on multiple devices across multiple facilities within a region, designed to provide a durability of 99.999999999% (11 9s). AWS provides further protection for data retention and archiving through versioning in Amazon S3, AWS multi-factor authentication (AWS MFA), bucket policies, and AWS Identity and Access Management (IAM). Amazon Glacier provides extremely low-cost storage for data archiving and backup. Objects (or archives, as they are known in Amazon Glacier) are optimized for infrequent access, for which retrieval times of several hours are adequate. Amazon Glacier is designed for the same durability as Amazon S3. Amazon Elastic Block Store (Amazon EBS) provides the ability to create point-in-time snapshots of data volumes. You can use the snapshots as the starting point for new Amazon EBS volumes, and you can protect your data for long-term durability because snapshots are stored within Amazon S3. After a volume is created, you can attach it to a running Amazon EC2 instance. Amazon EBS volumes provide off-instance storage that persists independently from the life of an instance and is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component. AWS Import/Export accelerates moving large amounts of data into and out of AWS by using portable storage devices for transport. AWS Import/Export bypasses the Internet and transfers your data directly onto and off of storage devices by means of the high-speed internal network of Amazon. For data sets of significant size, AWS Import/Export is often faster than Internet transfer and more cost effective than upgrading your connectivity. You can use AWS Import/Export to migrate data into and out of Amazon S3 buckets and Amazon Glacier vaults or into Amazon EBS snapshots. AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and highly secure integration between your on-premises IT environment and the storage infrastructure of AWS. Page 5 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 AWS Storage Gateway supports three different configurations: Gateway-cached volumes — You can store your primary data in Amazon S3 and retain your frequently accessed data locally. Gateway-cached volumes provide substantial cost savings on primary storage, minimize the need to scale your storage on-premises, and retain low-latency access to your frequently accessed data. Gateway-stored volumes — In the event that you need low-latency access to your entire data set, you can configure your gateway to store your primary data locally, and asynchronously back up point-in-time snapshots of this data to Amazon S3. Gateway-stored volumes provide durable and inexpensive off-site backups that you can recover locally or from Amazon EC2 if, for example, you need replacement capacity for disaster recovery. Gateway-virtual tape library (gateway-VTL) — With gateway-VTL, you can have an almost limitless collection of virtual tapes. You can store each virtual tape in a virtual tape library (VTL) backed by Amazon S3 or a virtual tape shelf (VTS) backed by Amazon Glacier. The virtual tape library exposes an industry standard iSCSI interface that provides your backup application with on-line access to the virtual tapes. When you no longer require immediate or frequent access to data contained on a virtual tape, you can use your backup application to move it from its VTL to your VTS to further reduce your storage costs. Compute Amazon Elastic Compute Cloud (Amazon EC2) provides resizable compute capacity in the cloud. Within minutes, you can create Amazon EC2 instances, which are virtual machines over which you have complete control. In the context of DR, the ability to rapidly create virtual machines that you can control is critical. To describe every feature of Amazon EC2 is outside the scope of this document; instead; we focus on the aspects of Amazon EC2 that are most relevant to DR. Amazon Machine Images (AMIs) are preconfigured with operating systems, and some preconfigured AMIs might also include application stacks. You can also configure your own AMIs. In the context of DR, we strongly recommend that you configure and identify your own AMIs so that they can launch as part of your recovery procedure. Such AMIs should be preconfigured with your operating system of choice plus appropriate pieces of the application stack. Availability Zones are distinct locations that are engineered to be insulated from failures in other Availability Zones. They also provide inexpensive, low-latency network connectivity to other Availability Zones in the same region. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location. Regions consist of one or more Availability Zones. The Amazon EC2 VM Import Connector virtual appliance enables you to import virtual machine images from your existing environment to Amazon EC2 instances. Networking When you are dealing with a disaster, it’s very likely that you will have to modify network settings as your system is failing over to another site. AWS offers several services and features that enable you to manage and modify network settings. Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. It gives developers and businesses a reliable, cost-effective way to route users to Internet applications. Amazon Route 53 includes a number of global load-balancing capabilities (which can be effective when you are dealing with DR scenarios such as DNS endpoint health checks) and the ability to failover between multiple endpoints and even static websites hosted in Amazon S3. Elastic IP addresses are static IP addresses designed for dynamic cloud computing. However, unlike traditional static IP addresses, Elastic IP addresses enable you to mask instance or Availability Zone failures by programmatically remapping Page 6 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 your public IP addresses to instances in your account in a particular region. For DR, you can also pre-allocate some IP addresses for the most critical systems so that their IP addresses are already known before disaster strikes. This can simplify the execution of the DR plan. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances. It enables you to achieve even greater fault tolerance in your applications by seamlessly providing the load-balancing capacity that is needed in response to incoming application traffic. Just as you can pre-allocate Elastic IP addresses, you can pre-allocate your load balancer so that its DNS name is already known, which can simplify the execution of your DR plan. Amazon Virtual Private Cloud (Amazon VPC) lets you provision a private, isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. This enables you to create a VPN connection between your corporate data center and your VPC, and leverage the AWS cloud as an extension of your corporate data center. In the context of DR, you can use Amazon VPC to extend your existing network topology to the cloud; this can be especially appropriate when recovering enterprise applications that are typically on the internal network. Amazon Direct Connect makes it easy to set up a dedicated network connection from your premises to AWS. In many cases, this can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. Databases For your database needs, consider using these AWS services: Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. You can use Amazon RDS either in the preparation phase for DR to hold your critical data in a database that is already running, or in the recovery phase to run your production database. When you want to look at multiple regions, Amazon RDS gives you the ability to snapshot data from one region to another, and also to have a read replica running in another region. Amazon DynamoDB is a fast, fully managed NoSQL database service that makes it simple and cost-effective to store and retrieve any amount of data and serve any level of request traffic. It has reliable throughput and single-digit, millisecond latency. You can also use it in the preparation phase to copy data to DynamoDB in another region or to Amazon S3. During the recovery phase of DR, you can scale up seamlessly in a matter of minutes with a single click or API call. Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools. You can use Amazon Redshift in the preparation phase to snapshot your data warehouse to be durably stored in Amazon S3 within the same region or copied to another region. During the recovery phase of DR, you can quickly restore your data warehouse into the same region or within another AWS region. You can also install and run your choice of database software on Amazon EC2, and you can choose from a variety of leading database systems. For more information about database options on AWS, see Running Databases on AWS. Page 7 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Deployment orchestration Deployment automation and post-startup software installation/configuration processes and tools can be used in Amazon EC2. We highly recommend investments in this area. This can be very helpful in the recovery phase, enabling you to create the required set of resources in an automated way. AWS CloudFormation gives developers and systems administrators an easy way to create a collection of related AWS resources and provision them in an orderly and predictable fashion. You can create templates for your environments and deploy associated collections of resources (called a stack) as needed. AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, and Docker. You can deploy your application code, and AWS Elastic Beanstalk will provision the operating environment for your applications. AWS OpsWorks is an application management service that makes it easy to deploy and operate applications of all types and sizes. You can define your environment as a series of layers, and configure each layer as a tier of your application. AWS OpsWorks has automatic host replacement, so in the event of an instance failure it will be automatically replaced. You can use AWS OpsWorks in the preparation phase to template your environment, and you can combine it with AWS CloudFormation in the recovery phase. You can quickly provision a new stack from the stored configuration that supports the defined RTO. Security and compliance There are many security-related features across the AWS services. We recommend that you review the Security Best Practices whitepaper. AWS also provides further risk and compliance information in the AWS Security Center. A full discussion of security is out of scope for this paper. Page 8 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Example Disaster Recovery Scenarios with AWS This section outlines four DR scenarios that highlight the use of AWS and compare AWS with traditional DR methods. The following figure shows a spectrum for the four scenarios, arranged by how quickly a system can be available to users after a DR event. Figure 1: Spectrum of Disaster Recovery Options AWS enables you to cost-effectively operate each of these DR strategies. It’s important to note that these are just examples of possible approaches, and variations and combinations of these are possible. If your application is already running on AWS, then multiple regions can be employed and the same DR strategies will still apply. Backup and Restore In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location. There are many commercial and open-source backup solutions that integrate with Amazon S3. You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. Amazon Glacier is a low-cost alternative starting from $0.01/GB per month. Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution. AWS Storage Gateway enables snapshots of your on-premises data volumes to be transparently copied into Amazon S3 for backup. You can subsequently create local volumes or Amazon EBS volumes from these snapshots. Storage-cached volumes allow you to store your primary data in Amazon S3, but keep your frequently accessed data local for low-latency access. As with AWS Storage Gateway, you can snapshot the data volumes to give highly durable backup. In the event of DR, you can restore the cache volumes either to a second site running a storage cache gateway or to Amazon EC2. You can use the gateway-VTL configuration of AWS Storage Gateway as a backup target for your existing backup management software. This can be used as a replacement for traditional magnetic tape backup. For systems running on AWS, you also can back up into Amazon S3. Snapshots of Amazon EBS volumes, Amazon RDS databases, and Amazon Redshift data warehouses can be stored in Amazon S3. Alternatively, you can copy files directly into Amazon S3, or you can choose to create backup files and copy those to Amazon S3. There are many backup solutions that store data directly in Amazon S3, and these can be used from Amazon EC2 systems as well. Page 9 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 The following figure shows data backup options to Amazon S3, from either on-site infrastructure or from AWS. Figure 2: Data Backup Options to Amazon S3 from On-Site Infrastructure or from AWS. Of course, the backup of your data is only half of the story. If disaster strikes, you’ll need to recover your data quickly and reliably. You should ensure that your systems are configured to retain and secure your data, and you should test your data recovery processes. The following diagram shows how you can quickly restore a system from Amazon S3 backups to Amazon EC2. Figure 3: Restoring a System from Amazon S3 Backups to Amazon EC2 Page 10 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Key steps for backup and restore: 1. Select an appropriate tool or method to back up your data into AWS. 2. Ensure that you have an appropriate retention policy for this data. 3. Ensure that appropriate security measures are in place for this data, including encryption and access policies. 4. Regularly test the recovery of this data and the restoration of your system. Pilot Light for Quick Recovery into AWS The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. Infrastructure elements for the pilot light itself typically include your database servers, which would replicate data to Amazon EC2 or Amazon RDS. Depending on the system, there might be other critical data outside of the database that needs to be replicated to AWS. This is the critical core of the system (the pilot light) around which all other infrastructure pieces in AWS (the rest of the furnace) can quickly be provisioned to restore the complete system. To provision the remainder of the infrastructure to restore business-critical services, you would typically have some pre configured servers bundled as Amazon Machine Images (AMIs), which are ready to be started up at a moment’s notice. When starting recovery, instances from these AMIs come up quickly with their pre-defined role (for example, Web or App Server) within the deployment around the pilot light. From a networking point of view, you have two main options for provisioning:  Use Elastic IP addresses, which can be pre-allocated and identified in the preparation phase for DR, and associate them with your instances. Note that for MAC address-based software licensing, you can use elastic network interfaces (ENIs), which have a MAC address that can also be pre-allocated to provision licenses against. You can associate these with your instances, just as you would with Elastic IP addresses.  Use Elastic Load Balancing (ELB) to distribute traffic to multiple instances. You would then update your DNS records to point at your Amazon EC2 instance or point to your load balancer using a CNAME. We recommend this option for traditional web-based applications. For less critical systems, you can ensure that you have any installation packages and configuration information available in AWS, for example, in the form of an Amazon EBS snapshot. This will speed up the application server setup, because you can quickly create multiple volumes in multiple Availability Zones to attach to Amazon EC2 instances. You can then install and configure accordingly, for example, by using the backup-and-restore method. The pilot light method gives you a quicker recovery time than the backup-and-restore method because the core pieces of the system are already running and are continually kept up to date. AWS enables you to automate the provisioning and configuration of the infrastructure resources, which can be a significant benefit to save time and help protect against human errors. However, you will still need to perform some installation and configuration tasks to recover the applications fully. Page 11 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Preparation phase The following figure shows the preparation phase, in which you need to have your regularly changing data replicated to the pilot light, the small core around which the full environment will be started in the recovery phase. Your less frequently updated data, such as operating systems and applications, can be periodically updated and stored as AMIs. Figure 4: The Preparation Phase of the Pilot Light Scenario Key steps for preparation: 1. Set up Amazon EC2 instances to replicate or mirror data. 2. Ensure that you have all supporting custom software packages available in AWS. 3. Create and maintain AMIs of key servers where fast recovery is required. 4. Regularly run these servers, test them, and apply any software updates and configuration changes. 5. Consider automating the provisioning of AWS resources. Recovery phase To recover the remainder of the environment around the pilot light, you can start your systems from the AMIs within minutes on the appropriate instance types. For your dynamic data servers, you can resize them to handle production volumes as needed or add capacity accordingly. Horizontal scaling often is the most cost-effective and scalable approach to add capacity to a system. For example, you can add more web servers at peak times. However, you can also choose larger Amazon EC2 instance types, and thus scale vertically for applications such as relational databases. From a networking perspective, any required DNS updates can be done in parallel. Page 12 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 After recovery, you should ensure that redundancy is restored as quickly as possible. A failure of your DR environment shortly after your production environment fails is unlikely, but you should be aware of this risk. Continue to take regular backups of your system, and consider additional redundancy at the data layer. The following figure shows the recovery phase of the pilot light scenario. Figure 5: The Recovery Phase of the Pilot Light Scenario. Key steps for recovery: 1. Start your application Amazon EC2 instances from your custom AMIs. 2. Resize existing database/data store instances to process the increased traffic. 3. Add additional database/data store instances to give the DR site resilience in the data tier; if you are using Amazon RDS, turn on Multi-AZ to improve resilience. 4. Change DNS to point at the Amazon EC2 servers. 5. Install and configure any non-AMI based systems, ideally in an automated way. Page 13 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Warm Standby Solution in AWS The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on. These servers can be running on a minimum-sized fleet of Amazon EC2 instances on the smallest sizes possible. This solution is not scaled to take a full-production load, but it is fully functional. It can be used for non-production work, such as testing, quality assurance, and internal use. In a disaster, the system is scaled up quickly to handle the production load. In AWS, this can be done by adding more instances to the load balancer and by resizing the small capacity servers to run on larger Amazon EC2 instance types. As stated in the preceding section, horizontal scaling is preferred over vertical scaling. Preparation phase The following figure shows the preparation phase for a warm standby solution, in which an on-site solution and an AWS solution run side-by-side. Figure 6: The Preparation Phase of the Warm Standby Scenario. Page 14 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Key steps for preparation: 1. Set up Amazon EC2 instances to replicate or mirror data. 2. Create and maintain AMIs. 3. Run your application using a minimal footprint of Amazon EC2 instances or AWS infrastructure. 4. Patch and update software and configuration files in line with your live environment. Recovery phase In the case of failure of the production system, the standby environment will be scaled up for production load, and DNS records will be changed to route all traffic to AWS. Figure 7: The Recovery Phase of the Warm Standby Scenario. Key steps for recovery: 1. Increase the size of the Amazon EC2 fleets in service with the load balancer (horizontal scaling). 2. Start applications on larger Amazon EC2 instance types as needed (vertical scaling). 3. Either manually change the DNS records, or use Amazon Route 53 automated health checks so that all traffic is routed to the AWS environment. 4. Consider using Auto Scaling to right-size the fleet or accommodate the increased load. 5. Add resilience or scale up your database. Page 15 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Multi-Site Solution Deployed on AWS and On-Site A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose. For more information about recovery point options, see the Recovery Time Objective and Recovery Point Objective section in this whitepaper. In addition to recovery point options, there are various replication methods, such as synchronous and asynchronous methods. For more information, see the Replication of Data section in this whitepaper. You can use a DNS service that supports weighted routing, such as Amazon Route 53, to route production traffic to different sites that deliver the same application or service. A proportion of traffic will go to your infrastructure in AWS, and the remainder will go to your on-site infrastructure. In an on-site disaster situation, you can adjust the DNS weighting and send all traffic to the AWS servers. The capacity of the AWS service can be rapidly increased to handle the full production load. You can use Amazon EC2 Auto Scaling to automate this process. You might need some application logic to detect the failure of the primary database services and cut over to the parallel database services running in AWS. The cost of this scenario is determined by how much production traffic is handled by AWS during normal operation. In the recovery phase, you pay only for what you use for the duration that the DR environment is required at full scale. You can further reduce cost by purchasing Amazon EC2 Reserved Instances for your “always on” AWS servers. Preparation phase The following figure shows how you can use the weighted routing policy of the Amazon Route 53 DNS to route a portion of your traffic to the AWS site. The application on AWS might access data sources in the on-site production system. Data is replicated or mirrored to the AWS infrastructure. Figure 8: The Preparation Phase of the Multi-Site Scenario. Page 16 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Key steps for preparation: 1. Set up your AWS environment to duplicate your production environment. 2. Set up DNS weighting, or similar traffic routing technology, to distribute incoming requests to both sites. Configure automated failover to re-route traffic away from the affected site. Recovery phase The following figure shows the change in traffic routing in the event of an on-site disaster. Traffic is cut over to the AWS infrastructure by updating DNS, and all traffic and supporting data queries are supported by the AWS infrastructure. Figure 9: The Recovery Phase of the Multi-Site Scenario Involving On-Site and AWS Infrastructure. Key steps for recovery: 1. Either manually or by using DNS failover, change the DNS weighting so that all requests are sent to the AWS site. 2. Have application logic for failover to use the local AWS database servers for all queries. 3. Consider using Auto Scaling to automatically right-size the AWS fleet. You can further increase the availability of your multi-site solution by designing Multi-AZ architectures. For more information about how to design applications that span multiple availability zones, see the Building Fault-Tolerant Applications on AWS whitepaper. Page 17 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 AWS Production to an AWS DR Solution Using Multiple AWS Regions Applications deployed on AWS have multi-site capability by means of multiple Availability Zones. Availability Zones are distinct locations that are engineered to be insulated from each other. They provide inexpensive, low-latency network connectivity within the same region. Some applications might have an additional requirement to deploy their components using multiple regions; this can be a business or regulatory requirement. Any of the preceding scenarios in this whitepaper can be deployed using separate AWS regions. The advantages for both production and DR scenarios include the following:  You don’t need to negotiate contracts with another provider in another region  You can use the same underlying AWS technologies across regions  You can use the same tools or APIs For more information, see the Migrating AWS Resources to a New Region whitepaper. Replication of Data When you replicate data to a remote location, you should consider these factors:  Distance between the sites — Larger distances typically are subject to more latency or jitter.  Available bandwidth — The breadth and variability of the interconnections.  Data rate required by your application — The data rate should be lower than the available bandwidth.  Replication technology — The replication technology should be parallel (so that it can use the network effectively). There are two main approaches for replicating data: synchronous and asynchronous. Synchronous replication Data is atomically updated in multiple locations. This puts a dependency on network performance and availability. In AWS, Availability Zones within a region are well connected, but physically separated. For example, when deployed in Multi-AZ mode, Amazon RDS uses synchronous replication to duplicate data in a second Availability Zone. This ensures that data is not lost if the primary Availability Zone becomes unavailable. Asynchronous replication Data is not atomically updated in multiple locations. It is transferred as network performance and availability allows, and the application continues to write data that might not be fully replicated yet. Many database systems support asynchronous data replication. The database replica can be located remotely, and the replica does not have to be completely synchronized with the primary database server. This is acceptable in many scenarios, for example, as a backup source or reporting/read-only use cases. In addition to database systems, you can also extend it to network file systems and data volumes. We recommend that you understand the replication technology used in your software solution. A detailed analysis of replication technology is beyond the scope of this paper. Page 18 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 AWS regions are completely independent of each other, but there are no differences in the way you access them and use them. This enables you to create DR processes that span continental distances, without the challenges or costs that this would normally incur. You can back up data and systems to two or more AWS regions, allowing service restoration even in the face of extremely large-scale disasters. You can use AWS regions to serve your users around the globe with relatively low complexity to your operational processes. Failing Back from a Disaster Once you have restored your primary site to a working state, you will need to restore your normal service, which is often referred to as a “fail back.” Depending on your DR strategy, this typically means reversing the flow of data replication so that any data updates received while the primary site was down can be replicated back, without the loss of data. The following steps outline the different fail-back approaches: Backup and restore 1. Freeze data changes to the DR site. 2. Take a backup. 3. Restore the backup to the primary site. 4. Re-point users to the primary site. 5. Unfreeze the changes. Pilot light, warm standby, and multi-site 1. Establish reverse mirroring/replication from the DR site back to the primary site, once the primary site has caught up with the changes. 2. Freeze data changes to the DR site. 3. Re-point users to the primary site. 4. Unfreeze the changes. Page 19 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Improving Your DR Plan This section describes the important steps you should follow to establish a strong DR plan. Testing After your DR solution is in place, it needs to be tested. You can test frequently, which is one of the key advantages of deploying on AWS. “Game day” is when you exercise a failover to the DR environment, ensuring that sufficient documentation is in place to make the process as simple as possible should the real event take place. Spinning up a duplicate environment for testing your game-day scenarios is quick and cost-effective on AWS, and you typically don’t need to touch your production environment. You can use AWS CloudFormation to deploy complete environments on AWS. This uses a template to describe the AWS resources and any associated dependencies or runtime parameters that are required to create a full environment. Differentiating your tests is key to ensuring that you are covered against a multitude of different types of disasters. The following are examples of possible game-day scenarios:  Power loss to a site or a set of servers  Loss of ISP connectivity to a single site  Virus impacting core business services that affects multi-sites  User error that causes the loss of data, requiring a point-in-time recovery Monitoring and alerting You need to have regular checks and sufficient monitoring in place to alert you when your DR environment has been impacted by server failure, connectivity issues, and application issues. Amazon CloudWatch provides access to metrics about AWS resources, as well as custom metrics that can be application–centric or even business-centric. You can set up alarms based on defined thresholds on any of the metrics and, where required, you can set up Amazon SNS to send alerts in case of unexpected behavior. You can use any monitoring solutions on AWS, and you can also continue to use any existing monitoring and alerting tools that your company uses to monitor your instance metrics, as well as guest OS stats and application health. Backups After you have switched to your DR environment, you should continue to make regular backups. Testing backup and restore regularly is essential as a fall-back solution. AWS gives you the flexibility to perform frequent, inexpensive DR tests without needing the DR infrastructure to be “always on.” User access You can secure access to resources in your DR environment by using AWS Identity and Access Management (IAM). With IAM, you can create role-based and user-based security policies that segregate user responsibilities and restrict user access to specified resources and tasks in your DR environment. Page 20 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 System access You can also create roles for your Amazon EC2 resources, so that only users who are assigned to specified roles can perform defined actions on your DR environment, such as accessing an Amazon S3 bucket or re-pointing an Elastic IP address. Automation You can automate the deployment of applications onto AWS-based servers and your on-premises servers by using configuration management or orchestration software. This allows you to handle application and configuration change management across both environments with ease. There are several popular orchestration software options available. For a list of solution providers, see the AWS Partner Directory.3 AWS CloudFormation works in conjunction with several tools to provision infrastructure services in an automated way. Higher levels of abstraction are also available with AWS OpsWorks or AWS Elastic Beanstalk. The overall goal is to automate your instances as much as possible. For more information, see the Architecting for the Cloud: Best Practices whitepaper. You can use Auto Scaling to ensure that your pool of instances is appropriately sized to meet the demand based on the metrics that you specify in AWS CloudWatch. This means that in a DR situation, as your user base starts to use the environment more, the solution can scale up dynamically to meet this increased demand. After the event is over and usage potentially decreases, the solution can scale back down to a minimum level of servers. Software Licensing and DR Ensuring that you are correctly licensed for your AWS environment is as important as licensing for any other environment. AWS provides a variety of models to make licensing easier for you to manage. For example, “Bring Your Own License” is possible for several software components or operating systems. Alternately, there is a range of software for which the cost of the license is included in the hourly charge. This is known as “License included.” “Bring your Own License” enables you to leverage your existing software investments during a disaster. “License included” minimizes up-front license costs for a DR site that doesn’t get used on a day-to-day basis. If at any stage you are in doubt about your licenses and how they apply to AWS, contact your license reseller. Conclusion Many options and variations for DR exist. This paper highlights some of the common scenarios, ranging from simple backup and restore to fault tolerant, multi-site solutions. AWS gives you fine-grained control and many building blocks to build the appropriate DR solution, given your DR objectives (RTO and RPO) and budget. The AWS services are available on-demand, and you pay only for what you use. This is a key advantage for DR, where significant infrastructure is needed quickly, but only in the event of a disaster. This whitepaper has shown how AWS provides flexible, cost-effective infrastructure solutions, enabling you to have a more effective DR plan. 3Solution providers can be found at http://aws.amazon.com/solutions/solution-providers/ Page 21 of 22 Amazon Web Services – Using AWS for Disaster Recovery October 2014 Further Reading   Amazon S3 Getting Started Guide: http://docs.amazonwebservices.com/AmazonS3/latest/gsg/ Amazon EC2 Getting Started Guide: http://docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/      AWS Partner Directory (for a list of AWS solution providers): http://aws.amazon.com/solutions/solution-providers/ AWS Security and Compliance Center: http://aws.amazon.com/security/ AWS Architecture Center: http://aws.amazon.com/architecture Whitepaper: Designing Fault-Tolerant Applications in the AWS Cloud Other AWS technical whitepapers: http://aws.amazon.com/whitepapers Document Revisions We’ve made the following changes to this whitepaper since its original publication in January, 2012:      Updated information about AWS regions Added information about new services: Amazon Glacier, Amazon Redshift, AWS OpsWorks, AWS Elastic Beanstalk, and Amazon DynamoDB Added information about elastic network interfaces (ENIs) Added information about various features of AWS services for DR scenarios using multiple AWS regions Added information about AWS Storage Gateway virtual tape libraries Page 22 of 22  
Backup, Archive, and Restore Approaches Using AWS Pawan Agnihotri AWS Certified Solutions Architect – Professional Amazon Web Services November 2014 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Contents Abstract Introduction Why Use AWS Backup and Archive Cloud Native Snapshot Options for Amazon EBS Creating Consistent or Hot Backups Multivolume Backups Backing Up Databases Backups for Amazon Relational Database Service Backup and Recovery of the Amazon Machine Image (AMI) On Premises Hybrid Cloud Paradigms Protecting Configurations Rather Than Servers Using Storage Fit for Purpose Automating Infrastructure Conclusion Appendices Terms Partner Solutions 3 3 4 5 5 6 7 8 9 10 11 12 15 19 19 21 23 23 25 25 26 Page 2 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Abstract Over the past couple of years enterprise data has grown substantially, and the growth is accelerating. The need to protect the data has grown along with it. The increase in data also brings an increase in the complexity of methods used for the backing up the data. Questions such as durability and scalability of the backup solution are now commonplace. A common question is: How does cloud help my backup and archival needs? This document aims to answer this question and propose some solutions around using cloud to back up your data. It discusses best practices of protecting your data using cloud services from AWS. This guide for backup, archive and restore will assist enterprise solution architects, backup architects, and IT administrators who are responsible for the design and deployment of the data protection for their corporate IT environments. Introduction As a backup architect or engineer, you are responsible for backups and archive for your enterprise. You have to manage the infrastructure as well as the backup operations. This may include managing tapes, sending tapes offsite, managing tape drives, managing backup servers, managing backup software, creating backup policies, insuring the backup data is secure, meeting compliance requirements for data retention, and performing restores. Furthermore, cost cutting puts pressure on your budgets and, with business open for more hours, your window to perform the backup is getting smaller. These are some of the challenges that are faced by backup teams across many enterprises. The legacy environments are hard to scale, you need more tape and tape drives, and more storage capacity to back up the avalanche of data that the business is producing. For those of you dealing with backups and restores, you may be employing many different systems, processes, and techniques available in the market. Additionally, you may have to support multiple configurations. With AWS, organizations can obtain a flexible, secure, and cost-effective IT infrastructure in much the same way that national electric grids enable homes and organizations to plug into a centrally managed, efficient, and cost-effective energy source. When freed from creating their own electricity, organizations were able to focus on the core competencies of their business and the needs of their customers. Please review some of the terms related to backup and archiving in the appendix which will be used throughout this whitepaper. Page 3 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Why Use AWS Amazon Web Services (AWS) is a secure, high-performance, flexible, cost-effective, and easy-to-use cloud computing platform. AWS takes care of the undifferentiated heavy lifting and provides the user with the necessary tools to accomplish the task of backing up the vast amounts of data from a variety of sources. The first question asked by many customers is about security: Will my data be secure in the cloud? Amazon Web Services takes security very seriously; every service that we launch focuses on security as the foundation. Our storage services like Amazon Simple Storage Service1 (Amazon S3) provide strong capabilities for access control and encryption both at rest and in transit. For encryption at rest, customers can use their own encryption keys2 with the Amazon S3 server side giving them control over their data. Switching to AWS offers many advantages:  Durability – Amazon S3 and Amazon Glacier3 are designed for 99.999999999% durability for the objects stored in them.  Security – AWS provides a number of options for access control and encrypting data in transit and at rest.  Global Infrastructure – Amazon Web Services are available across the globe so you can back up and store data in the region that meets your compliance requirement.  Compliance –AWS infrastructure is designed and managed in alignment with regulations, standards and best-practices including (as of the date of this publication) SOC, SSAE 16, ISO 27001, PCI DSS, HIPPA, and FedRamp so you can easily fit the backup solution into your existing compliance regimen.  Scalability – With AWS, you don’t have to worry about capacity. You can scale your consumption up or down as your needs change.  Lower TCO – The AWS scale of operations drives service costs down and helps lower the overall TCO of the storage. AWS often passes these cost savings on to the customer. As of the date of this publication, AWS has lowered prices 45 times since they began offering web services. 1http://aws.amazon.com/s3/ 2http://aws.amazon.com/blogs/aws/s3-encryption-with-your-keys/ 3http://aws.amazon.com/glacier/ Page 4 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Backup and Archive Developing a comprehensive strategy for backing up and restoring data is not a simple task. In some industries, regulatory requirements for data security, privacy, and records retention can be important factors to consider when developing a backup strategy. A good backup process can be defined based on the objectives: 1. Backing up file data 2. Backing up database 3. Backing up machine images In the following sections we describe the backup and archives approaches based on the organization of your infrastructure. IT infrastructure can broadly be categorized into the following scenarios - Cloud native, on premises, and hybrid. Cloud Native This scenario describes a workload environment that exists entirely on AWS. This includes web servers, application servers, databases, Active Directory, monitoring servers, etc. See Figure 1: AWS Native Scenario. Figure 1: AWS Native Scenario Page 5 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 With all the services in AWS, you can leverage many of the built-in features to accomplish the backup-archive tasks. Snapshot Options for Amazon EBS In AWS “file data” can be stored on either Amazon S3 or Amazon Elastic Block Store4 (Amazon EBS) volumes. Let’s take a look at how you can backup data on these. Amazon Elastic Compute Cloud5 (Amazon EC2) can use Amazon EBS volumes to store block-based data. You can use this block storage for databases, or formatted into any OS-supported file system. Amazon EBS provides the ability to create snapshots (backups) of any Amazon EBS volume and write a copy of the data in the volume to Amazon S3, where it is stored redundantly in multiple Availability Zones. You can create the Amazon EBS snapshot by using the AWS Management Console, the command line interface (CLI), or the APIs. Using the Elastic Block Store Volumes page6 of the Amazon EC2 console, click Actions and then click Create Snapshot to commence the creation of a snapshot that is stored in Amazon S3. Figure 2: Creating a Snapshot from Amazon EBS Using the Console You can also create the snapshot using the CLI command ec2-create-snapshot. When you create a snapshot, you protect your data directly to durable disk-based storage. You can schedule and issue the commands on a regular basis. And due to the economical pricing of Amazon S3, you can retain many generations of data. Further, 4http://aws.amazon.com/ebs/ 5http://aws.amazon.com/ec2/ 6https://console.aws.amazon.com/ec2/v2/#Volumes Page 6 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 because snapshots are block-based, you consume space only for changed data after the initial snapshot is created. To restore data from a snapshot, use AWS Management Console, the command line interface (CLI), or the APIs to create a new volume from an existing snapshot. For example, to restore a volume to a prior point-in-time backup, you could use the following sequence: 1. Create a new volume from the backup snapshot by using the following command: > ec2-create-volume –z us-west-1b –snapshot MySnapshotName 2. Within the Amazon EC2 instance, unmount the existing volume (e.g., by using umount in Linux or the Logical Volume Manager in Windows). 3. Detach the existing volume from the instance by using the following command: > ec2-detach-volume OldVolume 4. Attach the new volume that was created from the snapshot by using the following command: > ec2-attach-volume VolumeID –I InstanceID –d Device 5. Remount the volume on the running instance. This process is a fast and reliable way to restore full volume data as needed. If you need only a partial restore, you can attach the volume to the running instance under a different device name, mount it, and then use operating system copy commands to copy the data from the backup volume to the production volume. Amazon EBS snapshots can also be copied between AWS regions using the Amazon EBS snapshot copy capability that is available from the console or command line, as explained in the Amazon Elastic Compute Cloud User Guide.7 You can use this feature to store your backup in another region without having to manage the underlying replication technology. Creating Consistent or Hot Backups When you back up a system, the ideal is to have the system in a quiet state where it is not performing any I/O. From a backup perspective, the ideal state is a machine that is 7http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html Page 7 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 accepting no traffic. But this ideal is increasingly rare as 24/7 IT operations become the norm. Consequently, it is necessary to quiesce the file system or database in order to make a clean backup. How you do this depends on your database and/or file system, so due diligence is required. To summarize the process for a database:  If possible, put the database into hot backup mode. Alternatively, create a read replica copy of the database; this is a copy of the database that is up to date but runs on a separate instance. Keep in mind that on AWS you can run this instance for the duration required to perform the backup and then close it down, saving resources.  Issue the relevant Amazon EBS snapshot commands.  Take the database out of hot backup mode or, if using a read replica, terminate the read replica instance. Backing up a file system works similarly and depends highly on the capabilities of the particular operating system or file system. An example of a file system that can flush its data for a consistent backup is xfs (see xfs_freeze).8 If the file system in question does not support the ability to freeze, you should unmount it, issue the snapshot command, and then remount the file system. Alternatively, you can facilitate this process by using a logical volume manager that supports freezing of I/O. Because the snapshot process is fast to execute and captures a point in time, the volumes you are backing up only need be unmounted for a matter of seconds. This ensures that the backup window is as small as possible and that outage time is predictable and can be effectively scheduled. While the data copy process of creating the snapshot may take longer, the snapshot activity requiring the volume to be un mounted is very quick. Don’t confuse the two processes when structuring your backup regime. Multivolume Backups In some cases, you may stripe data across multiple Amazon EBS volumes by using a logical volume manager in order to increase potential throughput. When using a logical volume manager (e.g., mdadm or LVM), it is important to perform the backup from the volume manager layer rather than the underlying devices. This ensures all metadata is consistent and that the various subcomponent volumes are coherent. You can take a number of approaches to accomplish this, an example being the script created by alestic.com.9 8https://access.redhat.com/documentation/en US/Red_Hat_Enterprise_Linux/6/html/Storage_Administration_Guide/xfsfreeze.html 9https://github.com/alestic/ec2-consistent-snapshot Page 8 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 You can also perform backups of this nature from the logical volume manager or file system level. In these cases, using a “traditional” backup agent enables the data to be backed up over the network. A number of agent-based backup solutions are available on the internet and the AWS Marketplace.10 It is important to remember that agent-based backup software expects a consistent server name/IP address. As a result, using these tools in concert with instances deployed in a virtual private cloud (VPC)11 is the best method to ensure reliability. An alternative approach is to create a replica of the primary system volumes that exist on a single large volume. This simplifies the backup process, as only one large volume needs to be backed up, and the backup does not take place on the primary system. However, it is important to ascertain whether the single volume can perform sufficiently to maintain changes during the backup and whether the maximum volume size is appropriate for the application. Backing Up Databases AWS has many options for running databases. You can run your own database on an Amazon EC2 instance or use one of the managed services offering. If you are running your own database on an Amazon EC2 instance, you can back up data to files using database native tools (e.g., MySQL,12 Oracle,13, 14 MSSQL,15 postgresSQL16) or create a snapshot of the volumes containing the data. Backing up data for database differs from the web and application layers. In general, databases contain larger amounts of business data (tens of GB to multiple TB) in database-specific formats that must be retained and protected at all times. In these cases, you can leverage efficient data movement techniques such as snapshots to create backups that are fast, reliable, and space efficient. For databases that are built upon RAID sets of Amazon EBS volumes (and have total storage less than 1 TB), an alternative backup approach is to asynchronously replicate data to another database instance built using a single Amazon EBS volume. While the destination Amazon EBS volume will have slower performance, it is not being used for data access, and you can easily send a snapshot to Amazon S3 using the Amazon EBS 10 https://aws.amazon.com/marketplace/ 11 http://aws.amazon.com/vpc/ 12 http://dev.mysql.com/doc/refman/5.7/en/backup-and-recovery.html 13 https://media.amazonwebservices.com/AWS_Amazon_Oracle_Backups.pdf 14 http://docs.oracle.com/cd/E11882_01/backup.112/e10642/rcmbckba.htm#BRADV8003 15 http://msdn.microsoft.com/en-us/library/ms187510.aspx 16 http://www.postgresql.org/docs/9.3/static/backup.html Page 9 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 snapshot capability (see “Snapshot Options for Amazon EBS” earlier in this paper section). Backups for Amazon Relational Database Service The Amazon Relational Database Service (Amazon RDS)17 includes automated backups. This means that you do not need to issue specific commands to create backups of your database. Amazon RDS provides two different methods for backing up and restoring your DB Instance(s); automated backups and database snapshots (DB snapshots).  Automated backups enable point-in-time recovery of your DB Instance. When automated backups are turned on for your DB instance, Amazon RDS automatically performs a full daily backup of your data (during your preferred backup window) and captures transaction logs (as updates to your DB instance are made). When you initiate a point-in-time recovery, transaction logs are applied to the most appropriate daily backup in order to restore your DB instance to the specific time you requested. Amazon RDS retains backups of a DB instance for a limited, user specified period of time called the retention period, which, as of the date of this publication, by default is one day but can be set to up to thirty-five days. You can initiate a point-in-time restore and specify any second during your retention period, up to the Latest Restorable Time. You can use the DescribeDBInstances API call to return the latest restorable time for your DB instance(s), which is typically within the last five minutes. Alternatively, you can find the Latest Restorable Time for a DB instance by selecting it in the AWS Management Console and looking in the Description tab in the lower panel of the console.  DB snapshots are user-initiated and enable you to back up your DB instance in a known state as frequently as you wish, and then restore to that specific state at any time. DB snapshots can be created with the AWS Management Console or by using the CreateDBSnapshot API call. The snapshots are kept until you explicitly delete them with the console or the DeleteDBSnapshot API call. Note that when you restore to a point in time or from a DB snapshot, a new DB instance is created with a new endpoint. (If you want, you can delete the old DB instance by using the AWS Management Console or a DeleteDBInstance call.) You do this so you can create multiple DB instances from a specific DB snapshot or point in time. 17 http://aws.amazon.com/rds/ Page 10 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Backup of the Amazon Machine Image (AMI) Next we look at “machine images.” AWS stores system images in what are called Amazon Machine Images or AMI for short. These images consist of the template for the root volume required to launch an instance. To save your instance’s as a machine image you simply backup the root volume as an AMI. Figure 3: Using AMI to backup and launch an instance An AMI that you register is automatically stored in your account using Amazon EBS snapshots. These snapshots reside in Amazon S3 and are highly durable. This means that the underlying storage mechanism for the AMIs is protected from multiple failure scenarios. Page 11 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Figure 4: Using the EC2 console to create a machine image Once you have created an AMI of your Amazon EC2 instance you can use the AMI to recreate the instance or launch more copies of the instance. It is also possible to copy AMIs from one region to another. Consequently, you can save a copy of a system image to another region. On Premises This scenario describes a workload environment with no component in the cloud. All resources, including web servers, application servers, databases, Active Directory, monitoring, and more, are hosted either in the customer data center or colocation. See the following figure. Page 12 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Colocation Hosting Internet Customer Interconnect Network Corporate Data Center Application Servers Application Servers Routers Workstations Switches File Servers SAN Storage Application Servers Branch Office Workstations Management Server Routers Switches Database Servers SAN Storage Web Servers Workstations Workstations Workstations Switches Routers Application Servers Database Servers File Servers SAN Storage Application Servers Figure 5: On-premises environment Database Servers File Servers AWS can be leveraged very nicely for this scenario to help with backup and archiving. Using AWS storage services lets you focus on the backup and archiving task, leaving the heavy lifting on the storage side to AWS. With AWS you do not have to worry about storage scaling or infrastructure capacity to accomplish the backup task. Amazon storage services such as Amazon S3 and Amazon Glacier are natively API based and available via the Internet. This allows backup software vendors to directly integrate their applications with storage solutions provided by AWS as represented in the following figure. You can look at our partner directory18 for the backup software vendors who work with AWS. The primary solution in this scenario is to use backup and archive software that directly interfaces with AWS through the APIs. Here the backup software is AWS aware and will 18 http://www.aws-partner-directory.com/PartnerDirectory/PartnerSearch?type=ISV Page 13 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 back up the data from the on premises servers directly to Amazon S3 or Amazon Glacier. Figure 6: Backup connector to Amazon S3 or Amazon Glacier If your existing backup software does not natively support the AWS cloud, the alternate solution is to use our storage gateway products. AWS Storage Gateway19 is a virtual appliance that provides seamless and secure integration between your data center and AWS’s storage infrastructure. The service allows you to securely store data in the AWS cloud for scalable and cost-effective storage. The AWS Storage Gateway supports industry-standard storage protocols that work with your existing applications while securely storing all of your data encrypted in Amazon S3 or Amazon Glacier. Figure 7: Connecting on-premises to AWS storage 19 http://aws.amazon.com/storagegateway/ Page 14 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 AWS Storage Gateway supports three configurations:  Gateway-cached volumes – You can store your primary data in Amazon S3 and retain your frequently accessed data locally. Gateway-cached volumes provide substantial cost savings on primary storage, minimize the need to scale your storage on premises, and retain low-latency access to your frequently accessed data.  Gateway-stored volumes – In the event you need low-latency access to your entire data set, you can configure your on-premises data gateway to store your primary data locally, and asynchronously back up point-in-time snapshots of this data to Amazon S3. Gateway-stored volumes provide durable and inexpensive off-site backups that you can recover locally or from Amazon EC2.  Gateway-virtual tape library (gateway-VTL) – With gateway-VTL you can have a limitless collection of virtual tapes. Each virtual tape can be stored in a virtual tape library backed by Amazon S3 or a virtual tape shelf backed by Amazon Glacier. The virtual tape library exposes an industry standard iSCSI interface, which provides your backup application with online access to the virtual tapes. When you no longer require immediate or frequent access to data contained on a virtual tape, you can use your backup application to move it from its virtual tape library to your virtual tape shelf to further reduce your storage costs. These gateways act as plug-and-play devices providing standard iSCSI devices, which can be integrated into your backup or archive framework. You can use the iSCSI disk devices as storage pools for your backup software or the gateway-VTL to offload tape based backup or archive directly to Amazon S3 or Amazon Glacier. Using this method your backup and archives are automatically offsite (for compliance purposes) and stored on durable media, eliminating the complexity and security risks of off-site tape management. To offer flexibility to the customer, a multitude of third-party appliances and gateway devices work with Amazon storage services and can be found on our partner network website.20 Hybrid The two infrastructure deployments addressed up to this point, “cloud native” and “on premises,” can be combined into a hybrid scenario whose workload environment has infrastructure components in AWS as well as on premises. Resources, including web servers, application servers, databases, Active Directory, monitoring, and more, are 20 http://www.aws-partner-directory.com/ Page 15 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 hosted either in the customer data center or AWS. Applications running in the AWS are connected to applications running in the customer premises. This scenario is emerging as a very common case for enterprise workloads. Many enterprises have data centers of their own while leveraging AWS to augment capacity. These customer data centers are often connected to the AWS network by high capacity network links. For example, with AWS Direct Connect21 you can establish private dedicated connectivity from your premises to AWS. Figure 8: A hybrid infrastructure scenario You can leverage AWS to help with backup and archiving for this scenario as well. The techniques you use are a combination of the methods described previously in cloud native and on-premises solutions. Hybrid Techniques If you already have an existing framework that backs up data for your on-premises servers, then it is easy to extend that framework to your AWS resources over a VPN 21 http://aws.amazon.com/directconnect/ Page 16 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 connection or AWS Direct Connect. You will install the backup agent on the Amazon EC2 instances and back them up per the existing data protection policies. Depending on your backup framework setup, you may have a master backup server along with one or more media servers. You may consider moving the master backup server to an Amazon EC2 instance to automatically protect your master backup server against on-premises disasters and have a highly available backup infrastructure. To manage the backup data flows, you may also consider creating one or more media servers on Amazon EC2 instances. This will help the cloud-based resources backup to a local media target rather than go over the network back to the on-premises environment. You can also leverage the AWS Storage Gateway or other third-party storage gateways from the AWS Marketplace to connect your backup framework to Amazon storage services. The storage gateways are connected to the media servers allowing data to be securely and durably stored on Amazon S3 or Amazon Glacier. Figure 9: Leveraging gateways in the hybrid scenario Page 17 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Use Cases A use case can help explain the on-premises and hybrid scenarios: Assume that you are managing an environment where you are backing up a mixture of standalone servers, virtual machines, and database servers. This environment has 1,000 servers, and you backup operating system, file data, virtual machine images, and database backups. You have 20 databases to back up, which are a mixture of MySQL, MSSQL, and Oracle. You use “myqldump” to create a database dump file to disk for MySQL backups. Your backup software provides plugins or agents to back up operating system, virtual machine images, data, and MSSQL databases. Additionally this software has tight integration to backup Oracle database using RMAN. To support the above environment, your backup software has a global catalogue server or master server that controls the backup, archive and restore activities as well as multiple media serves that are connected to disk-based storage and LTO tape drives. Case 1: As the very first step, you check the vendor site to see if there is a plugin or built-in support for cloud storage backup and archive. If the software has cloud storage backup options, you can proceed to configure it. Many vendors support Amazon S3 as an option for cloud storage and Amazon Glacier for cloud archive. You can create the target bucket either from within the backup software or use the AWS Management Console to create a bucket in Amazon S3. Next you configure the media servers to create storage pools that use the Amazon S3 bucket. Once the storage pool is configured, the backup software starts using Amazon S3 to store the backup data. Case 2: If your backup software does not natively support cloud storage for backup or archive, you can use a storage gateway device as a bridge between the backup software and Amazon S3 or Amazon Glacier. If you want to attach disk-based storage to your media server, you can download the gateway – cached volumes storage gateway. If you want to attach tape drives to your media server you can download the gateway virtual tape library storage gateway. You can download the storage gateway from the AWS Management Console. Once the gateway is downloaded and activated, you can create iSCSI targets, which can be attached to the media servers. The media server sees these iSCSI targets as local disks or tape drives. You can then configure these into the storage pools and used for backups or archives. Page 18 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Figure 10: Choose and download the appropriate storage gateway Cloud Paradigms As cloud-based computing has evolved, so has the strategy for using it in backup and recovery. Protecting Configurations Rather Than Servers The Amazon EC2 service simplifies the backup and recovery of a standard server, such as a web server or application server. Traditionally, you would back up the complete server via a central backup server. With Amazon EC2 you can focus on protecting configuration and stateful data, rather than the server itself. This set of data is much smaller than the aggregate set of server data, which typically includes various application files, operating system files, temporary files, and so on. This change of approach means that regular nightly incremental or weekly full backups can take far less time and consume less storage space. Page 19 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Figure 11: Traditional backup approach When a compute instance is started in Amazon EC2, it is based upon an AMI and can also connect to existing storage volumes such as Amazon EBS. In addition, when launching a new instance, it is possible to pass “user data”22 to the instance that can be accessed internally as dynamic configuration parameters. A sample workflow is as follows:  Launch a new instance of a web server, passing it the “identity” of the web server and any security credentials required for initial setup. The instance is based upon a prebuilt AMI that contains the operating system and relevant web-server application (e.g., Apache or IIS).  Upon startup, a boot script accesses a designated and secured Amazon S3 bucket that contains the specified configuration file(s). 22 http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/index.html?AESDG-chapter-instancedata.html Page 20 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Figure 12: Amazon EC2 Backup Approach  The configuration file contains various instructions for setting up the server (e.g., web server parameters, locations of related servers, additional software to install, and patch updates).  The server executes the specified configuration and is ready for service. An open source tool for performing this process, called cloud-init,23 is already installed on Amazon Linux AMIs and is also available for a number of other Linux distributions. In this case, there is no need to back up the server itself. The relevant configuration is contained in the combination of the AMI and the configuration file(s). So the only components requiring backup and recovery are the AMI and configuration file(s). Using Storage Fit for Purpose In traditional storage model you have a choice of SAN or NAS based storage. Amazon Web Services provides many storage options. If your data and workflow need a file- or object-based store then Amazon S3 is the best solution. Amazon S3 is highly durable data store designed for 99.999999999% data durability. The data is internally replicated across multiple data centers. In addition Amazon S3 provides many features such as versioning. With versioning enabled, you can automatically save versions of objects that are overwritten.24 This provides an automatic backup capability for the data stored in Amazon S3. Amazon S3 also offers data lifecycle management features so you can automatically archive or delete data when certain time criteria are met. 23 https://launchpad.net/cloud-init 24 http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html Page 21 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Figure 13: Using the console to enable versioning for an S3 bucket If your data and workflow need long-term retention with low chances of retrieval then Amazon Glacier is the best solution. Amazon Glacier is a storage service optimized for infrequently used data, or “cold data.” The service provides durable and extremely low cost storage with security features for data archiving and backup. With Amazon Glacier, you can store your data cost-effectively for months, years, or even decades. With Amazon S3’s lifecycle management, you can automatically move data from Amazon S3 to Amazon Glacier based on the lifecycle policy. Figure 14: Amazon Glacier storage If your data and workflow require a file system to store files or database data then Amazon EBS is the best storage option. Amazon EBS provides many features such as high durability and reliability, encryption, provisioned IOPS, and point-in-time snapshots amongst others. The built-in volume snapshot feature is a good option for backing up data. Page 22 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Automating Infrastructure One of the main advantages of using Amazon Web Services is that capacity is available to you on demand. You have no need to preprovision your infrastructure for future or backup use. Using tools such as AWS CloudFormation25 and AWS OpsWorks,26 you can automate the build out of your infrastructure, as explained in the Bootstrapping Applications whitepaper.27 With this approach, you can operate your infrastructure as code. You are then not tied to a specific system image that you have to backup. The application can be backed up in code repositories and used to create a full-blown infrastructure at the time it is needed. Anytime you need to create a server, you launch an automated deployment of the application, which creates the infrastructure within minutes to host your application. Conclusion The growth in data and an explosion in the creation and use of machine-generated data are increasing the need for robust, scalable and secure backup solutions. At the same time, organizations are struggling to deal with an explosion in retained data for compliance or business reuse. Providing IT teams with services and solutions that are optimized for usability in backup and archival environments is a critical requirement. Amazon Web Services provides cost-effective and scalable solutions to help organizations balance their requirements for backup and archiving. These services integrate well with new as well as existing technologies the customers are working with today. Gartner has recognized AWS as a leader in providing public cloud storage services28. AWS is well positioned to help organizations move their workloads to the cloud-based platforms that are the next generation of backup. Notices © 2014, Amazon Web Services, Inc. or its affiliates. All rights reserved. This document is provided for informational purposes only. It represents AWS’s current product offerings as of the date of issue of this document, which are subject to change without notice. Customers are responsible for making their own independent assessment of the information in this document and any use of AWS’s products or services, each of which is provided “as is” without warranty of any kind, whether express or implied. This document does not create any warranties, representations, contractual commitments, conditions or assurances from AWS, its affiliates, suppliers or licensors. The responsibilities and liabilities of AWS to its 25 http://aws.amazon.com/cloudformation/ 26 http://aws.amazon.com/opsworks/ 27 https://s3.amazonaws.com/cloudformation-examples/BoostrappingApplicationsWithAWSCloudFormation.pdf 28 http://www.gartner.com/technology/reprints.do?id=1-1WWKTQ3&ct=140709&st=sb Page 23 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Page 24 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Appendices Terms Some important terms often used in backup and restore discussions  Archive – A strategy for long-term retention of data for use in case of compliance, regulatory, or historical records requirement.  Backup – A strategy of copying files or databases for short-term retention for use in case of failure or corruption.  Backup Frequency – The time between consecutive backups.  Data Lifecycle Management – The process of managing data information throughout its lifecycle, from requirements through retirement.  Data Versioning – Maintaining multiple versions of data for backup purposes.  File / Data Backup – The process of copying individual data files to a backup medium so that they will be preserved.  Image Backup – An exact copy of a drive or storage device containing the complete contents and structure representing the operating system and all the data associated with it, including the system state and application configurations.  Off-Site Backups – The process of storing the copy of data in a geographically different location from the source.  Restore – A process that involves copying backup files from secondary storage (tape, zip disk, or other backup media) to hard disk. A restore is performed in order to return data to its original condition if files have become damaged or to copy or move data to a new location.  Retention – The amount of time that a given set of data remains available for restore. Some backup products rely on daily copies of data and measure retention in terms of days. Others retain a number of copies of data changes regardless of the amount of time.  RPO – The maximum tolerable period in which data might be lost from an IT service due to a major incident.  RTO – The targeted duration of time and a service level within which a business process must be restored after a disaster (or disruption) in order to avoid unacceptable consequences associated with a break in business continuity. Page 25 of 26 Amazon Web Services – Backup, Archive and Restore Approaches Using AWS November 2014 Partner Solutions  Avere – Hybrid cloud NAS and AWS: http://www.averesystems.com/amazon-web-services  Commvault – Cloud Integration with Amazon Web Services: http://www.commvault.com/resource-library/1843/commvault-amazon-web services-solution-brief.pdf  CTERA – CTERA Cloud Storage Services Platform and Amazon Web Services: http://www.ctera.com/amazon-aws-cloud-storage-platform  NetApp Riverbed SteelStore™ – cloud storage gateway: http://www.riverbed.com/partners/find-a-partner/find-a-partner-tool/aws partner.html#Cloud_Storage  Symantec Solutions for Amazon Web Services – Symantec Netbackup Platform: http://www.symantec.com/content/en/us/enterprise/fact_sheets/b-nbu-DS solutions-for-amazon-web-services-21281095.en-us.pdf  Zmanda – Backup to Amazon S3: http://www.zmanda.com/backup-Amazon-S3.html Page 26 of 26  
Amazon Virtual Private Cloud Connectivity Options January 2018 © 2018, Amazon Web Services, Inc. or its affiliates. All rights reserved. Notices This document is provided for informational purposes only. It represents AWS’s current product offerings and practices as of the date of issue of this document, which are subject to change without notice. Customers are responsible for making their own independent assessment of the information in this document and any use of AWS’s products or services, each of which is provided “as is” without warranty of any kind, whether express or implied. This document does not create any warranties, representations, contractual commitments, conditions or assurances from AWS, its affiliates, suppliers or licensors. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Contents Introduction Network-to-Amazon VPC Connectivity Options AWS Managed VPN AWS Direct Connect AWS Direct Connect + VPN AWS VPN CloudHub Software VPN Transit VPC Amazon VPC-to-Amazon VPC Connectivity Options VPC Peering Software VPN Software-to-AWS Managed VPN AWS Managed VPN AWS Direct Connect AWS PrivateLink Internal User-to-Amazon VPC Connectivity Options Software Remote-Access VPN Conclusion Appendix A: High-Level HA Architecture for Software VPN Instances VPN Monitoring Contributors Document Revisions 1 2 4 6 8 10 11 13 14 16 17 19 20 22 25 26 27 29 30 31 31 32 Abstract Amazon Virtual Private Cloud (Amazon VPC) lets customers provision a private, isolated section of the Amazon Web Services (AWS) Cloud where they can launch AWS resources in a virtual network using customer-defined IP address ranges. Amazon VPC provides customers with several options for connecting their AWS virtual networks with other remote networks. This document describes several common network connectivity options available to our customers. These include connectivity options for integrating remote customer networks with Amazon VPC and connecting multiple Amazon VPCs into a contiguous virtual network. This whitepaper is intended for corporate network architects and engineers or Amazon VPC administrators who would like to review the available connectivity options. It provides an overview of the various options to facilitate network connectivity discussions as well as pointers to additional documentation and resources with more detailed information or examples. Amazon Web Services – Amazon VPC Connectivity Options Introduction Amazon VPC provides multiple network connectivity options for you to leverage depending on your current network designs and requirements. These connectivity options include leveraging either the internet or an AWS Direct Connect connection as the network backbone and terminating the connection into either AWS or user-managed network endpoints. Additionally, with AWS, you can choose how network routing is delivered between Amazon VPC and your networks, leveraging either AWS or user-managed network equipment and routes. This whitepaper considers the following options with an overview and a high-level comparison of each: User Network–to–Amazon VPC Connectivity Options • AWS Managed VPN – Describes establishing a VPN connection from your network equipment on a remote network to AWS managed network equipment attached to your Amazon VPC. • AWS Direct Connect – Describes establishing a private, logical connection from your remote network to Amazon VPC, leveraging AWS Direct Connect. • AWS Direct Connect + VPN – Describes establishing a private, encrypted connection from your remote network to Amazon VPC, leveraging AWS Direct Connect. • AWS VPN CloudHub – Describes establishing a hub-and-spoke model for connecting remote branch offices. • Software VPN – Describes establishing a VPN connection from your equipment on a remote network to a user-managed software VPN appliance running inside an Amazon VPC. • Transit VPC – Describes establishing a global transit network on AWS using Software VPN in conjunction with AWS managed VPN. Amazon VPC–to–Amazon VPC Connectivity Options • VPC Peering – Describes the AWS-recommended approach for connecting multiple Amazon VPCs within and across regions using the Amazon VPC peering feature. Page 1 Amazon Web Services – Amazon VPC Connectivity Options • Software VPN – Describes connecting multiple Amazon VPCs using VPN connections established between user-managed software VPN appliances running inside of each Amazon VPC. • Software-to-AWS Managed VPN – Describes connecting multiple Amazon VPCs with a VPN connection established between a user- managed software VPN appliance in one Amazon VPC and AWS managed network equipment attached to the other Amazon VPC. • AWS Managed VPN – Describes connecting multiple Amazon VPCs, leveraging multiple VPN connections between your remote network and each of your Amazon VPCs. • AWS Direct Connect – Describes connecting multiple Amazon VPCs, leveraging logical connections on customer-managed AWS Direct Connect routers. • AWS PrivateLink – Describes connecting multiple Amazon VPCs, leveraging VPC interface endpoints and VPC endpoint services. Internal User-to-Amazon VPC Connectivity Options • Software Remote-Access VPN – In addition to customer network–to– Amazon VPC connectivity options for connecting remote users to VPC resources, this section describes leveraging a remote-access solution for providing end-user VPN access into an Amazon VPC. Network-to-Amazon VPC Connectivity Options This section provides design patterns for you to connect remote networks with your Amazon VPC environment. These options are useful for integrating AWS resources with your existing on-site services (for example, monitoring, authentication, security, data or other systems) by extending your internal networks into the AWS Cloud. This network extension also allows your internal users to seamlessly connect to resources hosted on AWS just like any other internally facing resource. VPC connectivity to remote customer networks is best achieved when using non-overlapping IP ranges for each network being connected. For example, if Page 2 Amazon Web Services – Amazon VPC Connectivity Options you’d like to connect one or more VPCs to your home network, make sure they are configured with unique Classless Inter-Domain Routing (CIDR) ranges. We advise allocating a single, contiguous, non-overlapping CIDR block to be used by each VPC. For additional information about Amazon VPC routing and constraints, see the Amazon VPC Frequently Asked Questions.1 Option Use Case Advantages Limitations AWS Managed VPN AWS managed IPsec VPN connection over the internet Reuse existing VPN equipment and processes Reuse existing internet connections AWS managed endpoint includes multi-data center redundancy and automated failover Supports static routes or dynamic Border Gateway Protocol (BGP) peering and routing policies Network latency, variability, and availability are dependent on internet conditions Customer managed endpoint is responsible for implementing redundancy and failover (if required) Customer device must support single-hop BGP (when leveraging BGP for dynamic routing) AWS Direct Connect Dedicated network connection over private lines More predictable network performance Reduced bandwidth costs 1 or 10 Gbps provisioned connections May require additional telecom and hosting provider relationships or new network circuits to be provisioned Supports BGP peering and routing policies AWS Direct Connect + VPN IPsec VPN connection over private lines Same as the previous option with the addition of a secure IPsec VPN connection Same as the previous option with a little additional VPN complexity Page 3 Amazon Web Services – Amazon VPC Connectivity Options Option Use Case Advantages Limitations AWS VPN CloudHub Connect remote branch offices in a hub-and-spoke model for primary or backup connectivity Reuse existing internet connections and AWS VPN connections (for example, use AWS VPN CloudHub as backup connectivity to a third-party MPLS network) AWS managed virtual private gateway includes multi-data center redundancy and automated failover Supports BGP for exchanging routes and routing priorities (for example, prefer MPLS connections over backup AWS VPN connections) Network latency, variability, and availability are dependent on the internet User managed branch office endpoints are responsible for implementing redundancy and failover (if required) Software VPN Software appliance- based VPN connection over the internet Supports a wider array of VPN vendors, products, and protocols Customer is responsible for implementing HA (high availability) solutions for all VPN endpoints (if required) Fully customer-managed solution Transit VPC Same as the previous section Software appliance- based VPN connection with hub VPC AWS managed IPsec VPN connection for spoke VPC connection Same as the previous option with the addition of AWS managed VPN connection between hub and spoke VPCs AWS Managed VPN Amazon VPC provides the option of creating an IPsec VPN connection between remote customer networks and their Amazon VPC over the internet, as shown in the following figure. Consider taking this approach when you want to take advantage of an AWS managed VPN endpoint that includes automated multi– data center redundancy and failover built into the AWS side of the VPN connection. Although not shown, the Amazon virtual private gateway represents two distinct VPN endpoints, physically located in separate data centers to increase the availability of your VPN connection. Page 4 Amazon Web Services – Amazon VPC Connectivity Options AWS managed VPN The virtual private gateway also supports and encourages multiple user gateway connections so you can implement redundancy and failover on your side of the VPN connection as shown in the following figure. Both dynamic and static routing options are provided to give you flexibility in your routing configuration. Dynamic routing uses BGP peering to exchange routing information between AWS and these remote endpoints. With dynamic routing, you can also specify routing priorities, policies, and weights (metrics) in your BGP advertisements and influence the network path between your networks and AWS. It is important to note that when you use BGP, both the IPSec and the BGP connections must be terminated on the same user gateway device, so it must be capable of terminating both IPSec and BGP connections. Page 5 Amazon Web Services – Amazon VPC Connectivity Options Redundant AWS managed VPN connections Additional Resources • Adding a Virtual Private Gateway to Your VPC2 • Customer Gateway device minimum requirements3 • Customer Gateway devices known to work with Amazon VPC4 AWS Direct Connect AWS Direct Connect makes it easy to establish a dedicated connection from an on-premises network to Amazon VPC. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment. This private connection can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections. AWS Direct Connect lets you establish 1 Gbps or 10 Gbps dedicated network connections (or multiple connections) between AWS networks and one of the AWS Direct Connect locations. It uses industry-standard VLANs to access Page 6 Amazon Web Services – Amazon VPC Connectivity Options Amazon Elastic Compute Cloud (Amazon EC2) instances running within an Amazon VPC using private IP addresses. You can choose from an ecosystem of WAN service providers for integrating your AWS Direct Connect endpoint in an AWS Direct Connect location with your remote networks. The following figure illustrates this pattern. You can also work with your provider to create sub-1G connection or use link aggregation group (LAG) to aggregate multiple 1 gigabit or 10 gigabit connections at a single AWS Direct Connect endpoint, allowing you to treat them as a single, managed connection. AWS Direct Connect AWS Direct Connect allows you to connect your AWS Direct Connect connection to one or more VPCs in your account that are located in the same or different regions. You can use Direct Connect gateway to achieve this. A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any public region and access it from all other public regions. This feature also allows you to connect to any of the participating VPCs from any Direct Connect location, further reducing your costs for using AWS services on a cross-region basis. The following figure illustrates this pattern. Page 7 Amazon Web Services – Amazon VPC Connectivity Options AWS Direct Connect Gateway Additional Resources • AWS Direct Connect product page5 • AWS Direct Connect locations6 • AWS Direct Connect FAQs • AWS Direct Connect LAGs • AWS Direct Connect Gateways 7 • Getting Started with AWS Direct Connect8 AWS Direct Connect + VPN With AWS Direct Connect + VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections. You can use AWS Direct Connect to establish a dedicated network connection between your network create a logical connection to public AWS resources, such as an Amazon virtual private gateway IPsec endpoint. This solution combines the AWS managed benefits of the VPN solution with low latency, increased Page 8 Amazon Web Services – Amazon VPC Connectivity Options bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. The following figure illustrates this option. EC2 Instances Availability Zone AWS Public Direct Connect Virtual AWS Direct Customer WAN VPN EC2 Instances VPC Subnet 2 Amazon VPC Private Remote Servers AWS Direct Connect and VPN Additional Resources • AWS Direct Connect product page9 • AWS Direct Connect FAQs10 • Adding a Virtual Private Gateway to Your VPC11 Page 9 Amazon Web Services – Amazon VPC Connectivity Options AWS VPN CloudHub Building on the AWS managed VPN and AWS Direct Connect options described previously, you can securely communicate from one site to another using the AWS VPN CloudHub. The AWS VPN CloudHub operates on a simple hub-and- spoke model that you can use with or without a VPC. Use this design if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low cost hub-and-spoke model for primary or backup connectivity between these remote offices. The following figure depicts the AWS VPN CloudHub architecture, with blue dashed lines indicating network traffic between remote sites being routed over their AWS VPN connections. EC2 Instances Availability Zone EC2 Instances VPC Subnet 2 Customer Customer Virtual Customer AWS VPN CloudHub Customer Network Customer Network Customer Network AWS VPN CloudHub leverages an Amazon VPC virtual private gateway with multiple gateways, each using unique BGP autonomous system numbers (ASNs). Your gateways advertise the appropriate routes (BGP prefixes) over their VPN connections. These routing advertisements are received and Page 10 Amazon Web Services – Amazon VPC Connectivity Options readvertised to each BGP peer so that each site can send data to and receive data from the other sites. The remote network prefixes for each spoke must have unique ASNs, and the sites must not have overlapping IP ranges. Each site can also send and receive data from the VPC as if they were using a standard VPN connection. This option can be combined with AWS Direct Connect or other VPN options (for example, multiple gateways per site for redundancy or backbone routing that you provide) depending on your requirements. Additional Resources • AWS VPN CloudHub12 • Amazon VPC VPN Guide13 • Customer Gateway device minimum requirements14 • Customer Gateway devices known to work with Amazon VPC15 • AWS Direct Connect product page16 Software VPN Amazon VPC offers you the flexibility to fully manage both sides of your Amazon VPC connectivity by creating a VPN connection between your remote network and a software VPN appliance running in your Amazon VPC network. This option is recommended if you must manage both ends of the VPN connection either for compliance purposes or for leveraging gateway devices that are not currently supported by Amazon VPC’s VPN solution. The following figure shows this option. Page 11 Amazon Web Services – Amazon VPC Connectivity Options Software VPN Appliance Availability Zone VPC Router EC2 Instances VPC Subnet 2 internet VPN internet Clients Customer VPN Clients Remote Servers Software VPN You can choose from an ecosystem of multiple partners and open source communities that have produced software VPN appliances that run on Amazon EC2. These include products from well-known security companies like Check Point, Astaro, OpenVPN Technologies, and Microsoft, as well as popular open source tools like OpenVPN, Openswan, and IPsec-Tools. Along with this choice comes the responsibility for you to manage the software appliance, including configuration, patches, and upgrades. Note that this design introduces a potential single point of failure into the network design because the software VPN appliance runs on a single Amazon EC2 instance. For additional information, see Appendix A: High-Level HA Architecture for Software VPN Instances. Additional Resources • VPN Appliances from the AWS Marketplace17 • Tech Brief - Connecting Cisco ASA to VPC EC2 Instance (IPSec)18 Page 12 Amazon Web Services – Amazon VPC Connectivity Options • Tech Brief - Connecting Multiple VPCs with EC2 Instances (IPSec)19 • Tech Brief - Connecting Multiple VPCs with EC2 Instances (SSL)20 Transit VPC Building on the Software VPN design mentioned above, you can create a global transit network on AWS. A transit VPC is a common strategy for connecting multiple, geographically disperse VPCs and remote networks in order to create a global network transit center. A transit VPC simplifies network management and minimizes the number of connections required to connect multiple VPCs and remote networks. The following figure illustrates this design. Software VPN and Transit VPC Page 13 Amazon Web Services – Amazon VPC Connectivity Options Along with providing direct network routing between VPCs and on-premises networks, this design also enables the transit VPC to implement more complex routing rules, such as network address translation between overlapping network ranges, or to add additional network-level packet filtering or inspection. The transit VPC design can be used to support important use cases like, private networking, shared connectivity and cross account AWS usage. Additional Resources • Tech Brief - Global Transit Network • Solution - Transit VPC Amazon VPC-to-Amazon VPC Connectivity Options Use these design patterns when you want to integrate multiple Amazon VPCs into a larger virtual network. This is useful if you require multiple VPCs due to security, billing, presence in multiple regions, or internal charge-back requirements to more easily integrate AWS resources between Amazon VPCs. You can also combine these patterns with the Network–to–Amazon VPC Connectivity Options for creating a corporate network that spans remote networks and multiple VPCs. VPC connectivity between VPCs is best achieved when using non-overlapping IP ranges for each VPC being connected. For example, if you’d like to connect multiple VPCs, make sure each VPC is configured with unique Classless Inter- Domain Routing (CIDR) ranges. Therefore, we advise you to allocate a single, contiguous, non-overlapping CIDR block to be used by each VPC. For additional information about Amazon VPC routing and constraints, see the Amazon VPC Frequently Asked Questions.21 Page 14 Amazon Web Services – Amazon VPC Connectivity Options Option Use Case Advantages Limitations VPC Peering AWS-provided network connectivity between two VPCs. Leverages AWS networking infrastructure Does not rely on VPN instances or a separate piece of physical hardware No single point of failure No bandwidth bottleneck VPC peering does not support transitive peering relationships. Software VPN Software appliance- based VPN connections between VPCs Leverages AWS networking equipment in- region and internet pipes between regions Supports a wider array of VPN vendors, products, and protocols Managed entirely by you You are responsible for implementing HA solutions for all VPN endpoints (if required) VPN instances could become a network bottleneck Software-to- AWS managed VPN Software appliance to VPN connection between VPCs Leverages AWS networking equipment in- region and internet pipes between regions AWS managed endpoint includes multi-data center redundancy and automated failover You are responsible for implementing HA solutions for the software appliance VPN endpoints (if required) VPN instances could become a network bottleneck AWS managed VPN VPC-to-VPC routing managed by you over IPsec VPN connections using your equipment and the internet Reuse existing Amazon VPC VPN connections AWS managed endpoint includes multi-data center redundancy and automated failover Supports static routes and dynamic BGP peering and routing policies Network latency, variability, and availability depend on internet conditions The endpoint you manage is responsible for implementing redundancy and failover (if required) AWS Direct Connect VPC-to-VPC routing managed by you using your equipment in an AWS Direct Connect location and private lines Consistent network performance Reduced bandwidth costs 1 or 10 Gbps provisioned connections Supports static routes and BGP peering and routing policies May require additional telecom and hosting provider relationships Page 15 Amazon Web Services – Amazon VPC Connectivity Options Option Use Case Advantages Limitations AWS PrivateLink AWS-provided network connectivity between two VPCs using interface endpoints. VPC Peering Leverages AWS networking infrastructure No single point of failure VPC Endpoint services only available in AWS region in which they are created. A VPC peering connection is a networking connection between two VPCs that enables routing using each VPC’s private IP addresses as if they were in the same network. This is the AWS recommended method for connecting VPCs. VPC peering connections can be created between your own VPCs or with a VPC in another AWS account. VPC peering also supports inter-region peering. Traffic using inter-region VPC Peering always stays on the global AWS backbone and never traverses the public internet, thereby reducing threat vectors, such as common exploits and DDoS attacks. VPC-to-VPC peering Page 16 Amazon Web Services – Amazon VPC Connectivity Options AWS uses the existing infrastructure of a VPC to create VPC peering connections. These connections are neither a gateway nor a VPN connection and do not rely on a separate piece of physical hardware. Therefore, they do not introduce a potential single point of failure or network bandwidth bottleneck between VPCs. Additionally, VPC routing tables, security groups, and network access control lists can be leveraged to control which subnets or instances are able to utilize the VPC peering connection. A VPC peering connection can help you to facilitate the transfer of data between VPCs. You can use them to connect VPCs when you have more than one AWS account, to connect a management or shared services VPC to application- or customer-specific VPCs, or to connect seamlessly with a partner’s VPC. For more examples of scenarios in which you can use a VPC peering connection, see the Amazon VPC Peering Guide.22 Additional Resources • Amazon VPC User Guide23 • Amazon VPC Peering Guide24 Software VPN Amazon VPC provides network routing flexibility. This includes the ability to create secure VPN tunnels between two or more software VPN appliances to connect multiple VPCs into a larger virtual private network so that instances in each VPC can seamlessly connect to each other using private IP addresses. This option is recommended when you want to connect VPCs across multiple AWS Regions and manage both ends of the VPN connection using your preferred VPN software provider. This option uses an internet gateway attached to each VPC to facilitate communication between the software VPN appliances. Page 17 Amazon Web Services – Amazon VPC Connectivity Options Inter-region VPC-to-VPC routing You can choose from an ecosystem of multiple partners and open source communities that have produced software VPN appliances that run on Amazon EC2. These include products from well-known security companies like Check Point, Sophos, OpenVPN Technologies, and Microsoft, as well as popular open source tools like OpenVPN, Openswan, and IPsec-Tools. Along with this choice comes the responsibility for you to manage the software appliance including configuration, patches, and upgrades. Note that this design introduces a potential single point of failure into the network design as the software VPN appliance runs on a single Amazon EC2 instance. For additional information, see Appendix A: High-Level HA Architecture for Software VPN Instances. Additional Resources • VPN Appliances from the AWS Marketplace25 • Tech Brief - Connecting Multiple VPCs with EC2 Instances (IPSec)26 • Tech Brief - Connecting Multiple VPCs with EC2 Instances (SSL)27 Page 18 Amazon Web Services – Amazon VPC Connectivity Options Software-to-AWS Managed VPN Amazon VPC provides the flexibility to combine the AWS managed VPN and software VPN options to connect multiple VPCs. With this design, you can create secure VPN tunnels between a software VPN appliance and a virtual private gateway to connect multiple VPCs into a larger virtual private network, allowing instances in each VPC to seamlessly connect to each other using private IP addresses. This option is recommended when you want to connect VPCs across multiple AWS regions and would like to take advantage of the AWS managed VPN endpoint including automated multi-data center redundancy and failover built into the virtual private gateway side of the VPN connection. This option uses a virtual private gateway in one Amazon VPC and a combination of an internet gateway and software VPN appliance in another Amazon VPC as shown in the following figure. Intra-region VPC-to-VPC routing Page 19 Amazon Web Services – Amazon VPC Connectivity Options Note that this design introduces a potential single point of failure into the network design as the software VPN appliance runs on a single Amazon EC2 instance. For additional information, see Appendix A: High-Level HA Architecture for Software VPN Instances. Additional Resources • Tech Brief - Connecting Multiple VPCs with Sophos Security Gateway28 • Configuring Windows Server 2008 R2 as a Customer Gateway for Amazon Virtual Private Cloud29 AWS Managed VPN Amazon VPC provides the option of creating an IPsec VPN to connect your remote networks with your Amazon VPCs over the internet. You can take advantage of multiple VPN connections to route traffic between your Amazon VPCs as shown in the following figure. Page 20 Amazon Web Services – Amazon VPC Connectivity Options Routing traffic between VPCs Page 21 Amazon Web Services – Amazon VPC Connectivity Options We recommend this approach when you want to take advantage of AWS managed VPN endpoints including the automated multi-data center redundancy and failover built into the AWS side of each VPN connection. Although not shown, the Amazon virtual private gateway represents two distinct VPN endpoints, physically located in separate data centers to increase the availability of each VPN connection. Amazon virtual private gateway also supports multiple customer gateway connections (as described in the Customer Network–to–Amazon VPC Options and AWS managed VPN sections and shown in the figure Redundant AWS managed VPN connections), allowing you to implement redundancy and failover on your side of the VPN connection. This solution can also leverage BGP peering to exchange routing information between AWS and these remote endpoints. You can specify routing priorities, policies, and weights (metrics) in your BGP advertisements to influence the network path traffic will take to and from your networks and AWS. This approach is suboptimal from a routing perspective since the traffic must traverse the internet to get to and from your network, but it gives you a lot of flexibility for controlling and managing routing on your local and remote networks, and the potential ability to reuse VPN connections. Additional Resources • Amazon VPC Users Guide30 • Customer Gateway device minimum requirements31 • Customer Gateway devices known to work with Amazon VPC32 • Tech Brief - Connecting a Single Router to Multiple VPCs33 AWS Direct Connect AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to your Amazon VPC or among Amazon VPCs. This option can potentially reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than the other VPC-to-VPC connectivity options. Page 22 Amazon Web Services – Amazon VPC Connectivity Options You can divide a physical AWS Direct Connect connection into multiple logical connections, one for each VPC. You can then use these logical connections for routing traffic between VPCs, as shown in the following figure. In addition to intra-region routing, you can connect AWS Direct Connect locations in other regions using your existing WAN providers and leverage AWS Direct Connect to route traffic between regions over your WAN backbone network. Page 23 Amazon Web Services – Amazon VPC Connectivity Options Intra-region VPC-to-VPC routing with AWS Direct Connect Page 24 Amazon Web Services – Amazon VPC Connectivity Options We recommend this approach if you’re already an AWS Direct Connect customer or would like to take advantage of AWS Direct Connect’s reduced network costs, increased bandwidth throughput, and more consistent network experience. AWS Direct Connect can provide very efficient routing since traffic can take advantage of 1 Gbps or 10 Gbps fiber connections physically attached to the AWS network in each region. Additionally, this service gives you the most flexibility for controlling and managing routing on your local and remote networks, as well as the potential ability to reuse AWS Direct Connect connections. Additional Resources • AWS Direct Connect product page34 • AWS Direct Connect locations35 • AWS Direct Connect FAQs36 • Get Started with AWS Direct Connect37 AWS PrivateLink An interface VPC endpoint (AWS PrivateLink) enables you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS accounts (referred to as endpoint services), and supported AWS Marketplace partner services. The interface endpoints are created directly inside of your VPC, using elastic network interfaces and IP addresses in your VPC’s subnets. The service is now in your VPC, enabling connectivity to AWS services or AWS PrivateLink-powered service via private IP addresses. That means that VPC Security Groups can be used to manage access to the endpoints. Also, interface endpoints can be accessed from your premises via AWS Direct Connect. In the following diagram, the account owner of VPC B is a service provider, and account owner of VPC A is service consumer. Page 25 Amazon Web Services – Amazon VPC Connectivity Options VPC-to-VPC routing with AWS PrivateLink We recommend this approach if you want to use services offered by another VPC securely over private connection. You can create an interface endpoint to keep all traffic within AWS network. Additional Resources • Interface VPC Endpoints • VPC Endpoint Services Internal User-to-Amazon VPC Connectivity Options Internal user access to Amazon VPC resources is typically accomplished either through your network–to-Amazon VPC options or the use of software remote- access VPNs to connect internal users to VPC resources. With the former option, you can reuse your existing on-premises and remote-access solutions for managing end-user access, while still providing a seamless experience connecting to AWS hosted resources. Describing on-premises internal and remote access solutions in any more detail than what has been described in Page 26 Amazon Web Services – Amazon VPC Connectivity Options Customer Network–to–Amazon VPC Options is beyond the scope of this document. With software remote-access VPN, you can leverage low cost, elastic, and secure AWS services to implement remote-access solutions while also providing a seamless experience connecting to AWS hosted resources. In addition, you can combine software remote-access VPNs with your network-to-Amazon VPC options to provide remote access to internal networks if desired. This option is typically preferred by smaller companies with less extensive remote networks or who have not already built and deployed remote access solutions for their employees. The following table outlines the advantages and limitations of these options. Option Use Case Advantages Limitations Network-to- Amazon VPC Connectivity Options Virtual extension of your data center into AWS Requires existing end- user internal and remote access implementations Leverages existing end- user internal and remote- access policies and technologies Software Remote- Access VPN Cloud-based remote- access solution to Amazon VPC and/or internal networks Leverages low-cost, elastic, and secure web services provided by AWS for implementing a remote access solution Could be redundant if internal and remote access implementations already exist Software Remote-Access VPN You can choose from an ecosystem of multiple partners and open source communities that have produced remote-access solutions that run on Amazon EC2. These include products from well-known security companies like Check Point, Sophos, OpenVPN Technologies, and Microsoft. The following figure shows a simple remote-access solution leveraging an internal remote user database. Page 27 Amazon Web Services – Amazon VPC Connectivity Options Remote-access solution Remote-access solutions range in complexity, support multiple client authentication options (including multifactor authentication) and can be integrated with either Amazon VPC or remotely hosted identity and access management solutions (leveraging one of the network-to-Amazon VPC options) like Microsoft Active Directory or other LDAP/multifactor authentication solutions. The following figure shows this combination, allowing the remote- access server to leverage internal access management solutions if desired. Page 28 Amazon Web Services – Amazon VPC Connectivity Options Combination remote-access solution As with the software VPN options, the customer is responsible for managing the remote access software including user management, configuration, patches and upgrades. Additionally, consider that this design introduces a potential single point of failure into the network design as the remote access server runs on a single Amazon EC2 instance. For additional information, see Appendix A: High-Level HA Architecture for Software VPN Instances. Additional Resources • VPN Appliances from the AWS Marketplace38 • OpenVPN Access Server Quick Start Guide39 Conclusion AWS provides a number of efficient, secure connectivity options to help you get the most out of AWS when integrating your remote networks with Amazon VPC. Page 29 Amazon Web Services – Amazon VPC Connectivity Options The options provided in this whitepaper highlight several of the connectivity options and patterns that customers have used to successfully integrate their remote networks or multiple Amazon VPC networks. You can use the information provided here to determine the most appropriate mechanism for connecting the infrastructure required to run your business regardless of where it is physically located or hosted. Appendix A: High-Level HA Architecture for Software VPN Instances Creating a fully resilient VPC connection for software VPN instances requires the setup and configuration of multiple VPN instances and a monitoring instance to monitor the health of the VPN connections. High-level HA design We recommend configuring your VPC route tables to leverage all VPN instances simultaneously by directing traffic from all of the subnets in one Availability Page 30 Amazon Web Services – Amazon VPC Connectivity Options Zone through its respective VPN instances in the same Availability Zone. Each VPN instance then provides VPN connectivity for instances that share the same Availability Zone. VPN Monitoring To monitor Software based VPN appliance you can create a VPN Monitor. The VPN monitor is a custom instance that you will need to run the VPN monitoring scripts. This instance is intended to run and monitor the state of VPN connection and VPN instances. If a VPN instance or connection goes down, the monitor needs to stop, terminate, or restart the VPN instance while also rerouting traffic from the affected subnets to the working VPN instance until both connections are functional again. Since customer requirements vary, AWS does not currently provide prescriptive guidance for setting up this monitoring instance. However, an example script for enabling HA between NAT instances could be used as a starting point for creating an HA solution for Software VPN instances. We recommend that you think through the necessary business logic to provide notification or attempt to automatically repair network connectivity in the event of a VPN connection failure. Additionally, you can monitor the AWS Managed VPN tunnels using Amazon CloudWatch metrics, which collects data points from the VPN service into readable, near real-time metrics. Each VPN connection collects and publishes a variety of tunnel metrics to Amazon CloudWatch. These metrics will allow you to monitor tunnel health, activity, and create automated actions. Contributors The following individuals contributed to this document: • Garvit Singh, Solutions Builder , AWS Solution Architecture • Steve Morad, Senior Manager, Solution Builders , AWS Solution Architecture • Sohaib Tahir, Solutions Architect, AWS Solution Architecture Page 31 Amazon Web Services – Amazon VPC Connectivity Options Document Revisions Date Description January 2018 Updated information throughout. Focus on the following designs/features: transit VPC, direct connect gateway, and private link July 2014 First publication Notes 1 http://aws.amazon.com/vpc/faqs/ 2 3 http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_VP N.html https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#CGRe quirements 4 https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#Device sTested 5 http://aws.amazon.com/directconnect/ 6 http://aws.amazon.com/directconnect/#details 7 http://aws.amazon.com/directconnect/faqs/ 8 http://docs.amazonwebservices.com/DirectConnect/latest/GettingStartedGui de/Welcome.html 9 http://aws.amazon.com/directconnect/ 10 http://aws.amazon.com/directconnect/faqs/ 11 12 13 http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_VP N.html http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPN_Cl oudHub.html http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_VP Page 32 Amazon Web Services – Amazon VPC Connectivity Options N.html Page 33 Amazon Web Services – Amazon VPC Connectivity Options 14 http://aws.amazon.com/vpc/faqs/#C8 15 http://aws.amazon.com/vpc/faqs/#C9 16 http://aws.amazon.com/directconnect/ 17 https://aws.amazon.com/marketplace/search/results/ref%3Dbrs_navgno_se arch_box?searchTerms=vpn 18 http://aws.amazon.com/articles/8800869755706543 19 http://aws.amazon.com/articles/5472675506466066 Although these guides specifically address connecting multiple Amazon VPCs, they are easily adaptable to support this network configuration by substituting one of the VPCs with an on-premises VPN device connecting to an IPsec or SSL software VPN appliance running in an Amazon VPC. 20 https://aws.amazon.com/articles/0639686206802544 21 http://aws.amazon.com/vpc/faqs/ 22 http://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/ 23 http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc- peering.html 24 http://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/ 25 https://aws.amazon.com/marketplace/search/results/ref%3Dbrs_navgno_se arch_box?searchTerms=vpn 26 http://aws.amazon.com/articles/5472675506466066 27 http://aws.amazon.com/articles/0639686206802544 28 http://aws.amazon.com/articles/1909971399457482 29 http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/Custom erGateway-Windows.html 30 http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_VP N.html 31 https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#CGRe quirements Page 34 Amazon Web Services – Amazon VPC Connectivity Options 32 https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#Device sTested Page 35 Amazon Web Services – Amazon VPC Connectivity Options 33 http://aws.amazon.com/vpc/faqs/#C9 34 http://aws.amazon.com/directconnect/ 35 http://aws.amazon.com/directconnect/#details 36 http://aws.amazon.com/directconnect/faqs/ 37 http://docs.amazonwebservices.com/DirectConnect/latest/GettingStartedGui de/Welcome.html 38 https://aws.amazon.com/marketplace/search/results/ref%3Dbrs_navgno_se arch_box?searchTerms=vpn 39 http://docs.openvpn.net/how-to-tutorialsguides/virtual-platforms/amazon- ec2-appliance-ami-quick-start-guide/ Page 36 
AWS Database Migration Service Best Practices August 2016 This paper has been archived. For the latest technical content about this subject, see the AWS Whitepapers & Guides page: http://aws.amazon.com/whitepapers Archived Amazon Web Services – AWS Database Migration Service Best Practices August 2016 © 2016, Amazon Web Services, Inc. or its affiliates. All rights reserved. Notices This document is provided for informational purposes only. It represents AWS’s current product offerings and practices as of the date of issue of this document, which are subject to change without notice. Customers are responsible for making their own independent assessment of the information in this document and any use of AWS’s products or services, each of which is provided “as is” without warranty of any kind, whether express or implied. This document does not create any warranties, representations, contractual commitments, conditions or assurances from AWS, its affiliates, suppliers or licensors. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Archived Page 2 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Page 3 of 17 Contents Abstract 4 Introduction 4 Provisioning a Replication Server 6 Instance Class 6 Storage 6 Multi-AZ 7 Source Endpoint 7 Target Endpoint 7 Task 8 Migration Type 8 Start Task on Create 8 Target Table Prep Mode 8 LOB Controls 9 Enable Logging 10 Monitoring Your Tasks 10 Host Metrics 10 Replication Task Metrics 10 Table Metrics 10 Performance Expectations 11 Increasing Performance 11 Load Multiple Tables in Parallel 11 Remove Bottlenecks on the Target 11 Use Multiple Tasks 11 Improving LOB Performance 12 Optimizing Change Processing 12 Reducing Load on Your Source System 12 Frequently Asked Questions 13 What are the main reasons for performing a database migration? 13 Archived Amazon Web Services – AWS Database Migration Service Best Practices August 2016 What steps does a typical migration project include? How Much Load Will the Migration Process Add to My Source Database? How Long Does a Typical Database Migration Take? I’m Changing Engines–How Can I Migrate My Complete Schema? Why Doesn’t AWS DMS Migrate My Entire Schema? Who Can Help Me with My Database Migration Project? What Are the Main Reasons to Switch Database Engines? How Can I Migrate from Unsupported Database Engine Versions? When Should I NOT Use DMS? 13 14 14 14 14 15 15 15 16 When Should I Use a Native Replication Mechanism Instead of the DMS and the AWS Schema Conversion Tool? 16 What Is the Maximum Size of Database That DMS Can Handle? What if I Want to Migrate from Classic to VPC? Conclusion Contributors Abstract 16 17 17 17 Today, as many companies move database workloads to Amazon Web Services (AWS), they are often also interested in changing their primary database engine. Most current methods for migrating databases to the cloud or switching engines require an extended outage. The AWS Database Migration Service helps organizations to migrate database workloads to AWS or change database engines while minimizing any associated downtime. This paper outlines best practices for using AWS DMS. Introduction AWS Database Migration Service allows you to migrate data from a source database to a target database. During a migration, the service tracks changes being made on the source database so that they can be applied to the target database to eventually keep the two databases in sync. Although the source and target databases can be of the same engine type, they don’t need to be. The possible types of migrations are: Archived 1. Homogenous migrations (migrations between the same engine types) 2. Heterogeneous migrations (migrations between different engine types) Page 4 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 At a high level, when using AWS DMS a user provisions a replication server, defines source and target endpoints, and creates a task to migrate data between the source and target databases. A typical task consists of three major phases: the full load, the application of cached changes, and ongoing replication. During the full load, data is loaded from tables on the source database to tables on the target database, eight tables at a time (the default). While the full load is in progress, changes made to the tables that are being loaded are cached on the replication server; these are the cached changes. It’s important to know that the capturing of changes for a given table doesn’t begin until the full load for that table starts; in other words, the start of change capture for each individual table will be different. After the full load for a given table is complete, you can begin to apply the cached changes for that table immediately. When ALL tables are loaded, you begin to collect changes as transactions for the ongoing replication phase. After all cached changes are applied, your tables are consistent transactionally and you move to the ongoing replication phase, applying changes as transactions. Upon initial entry into the ongoing replication phase, there will be a backlog of transactions causing some lag between the source and target databases. After working through this backlog, the system will eventually reach a steady state. At this point, when you’re ready, you can:  Shut down your applications.  Allow any remaining transactions to be applied to the target.  Restart your applications pointing at the new target database. AWS DMS will create the target schema objects that are needed to perform the migration. However, AWS DMS takes a minimalist approach and creates only those objects required to efficiently migrate the data. In other words, AWS DMS will create tables, primary keys, and in some cases, unique indexes. It will not create secondary indexes, non-primary key constraints, data defaults, or other objects that are not required to efficiently migrate the data from the source system. In most cases, when performing a migration, you will also want to migrate most or all of the source schema. If you are performing a homogeneous migration, you can accomplish this by using your engine’s native tools to perform a no-data export/import of the schema. If your migration is heterogeneous, you can use the AWS Schema Conversion Tool (AWS SCT) to generate a complete target schema for you. Note Any inter-table dependencies, such as foreign key constraints, must be disabled during the “full load” and “cached change application” phases of AWS Archived DMS processing. Also, if performance is an issue, it will be beneficial to remove or disable secondary indexes during the migration process. Page 5 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Provisioning a Replication Server AWS DMS is a managed service that runs on an Amazon Elastic Compute Cloud (Amazon EC2) instance. The service connects to the source database, reads the source data, formats the data for consumption by the target database, and loads the data into the target database. Most of this processing happens in memory, however, large transactions may require some buffering on disk. Cached transactions and log files are also written to disk. The following sections describe what you should consider when selecting your replication server. Instance Class Some of the smaller instance classes are sufficient for testing the service or for small migrations. If your migration involves a large number of tables, or if you intend to run multiple concurrent replication tasks, you should consider using one of the larger instances because the service consumes a fair amount of memory and CPU. Note T2 type instances are designed to provide moderate baseline performance and the capability to burst to significantly higher performance, as required by your workload. They are intended for workloads that don't use the full CPU often or consistently, but that occasionally need to burst. T2 instances are well suited for general purpose workloads, such as web servers, developer environments, and small databases. If you’re troubleshooting a slow migration and using a T2 instance type, look at the CPU Utilization host metric to see if you’re bursting over the baseline for that instance type. Storage Depending on the instance class, your replication server will come with either 50 GB or 100 GB of data storage. This storage is used for log files and any cached changes that are collected during the load. If your source system is busy or takes large transactions, or if you’re running multiple tasks on the replication server, you might need to increase this amount of storage. However, the default amount is usually sufficient. Note All storage volumes in AWS DMS are GP2 or General Purpose SSDs. GP2 volumes come with a base performance of three I/O Operations Per Second Archived (IOPS), with abilities to burst up to 3,000 IOPS on a credit basis. As a rule of thumb, check the ReadIOPS and WriteIOPS metrics for the replication instance and be sure the sum of these values does not cross the base performance for that volume. Page 6 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Multi-AZ Selecting a Multi-AZ instance can protect your migration from storage failures. Most migrations are transient and not intended to run for long periods of time. If you’re using AWS DMS for ongoing replication purposes, selecting a Multi-AZ instance can improve your availability should a storage issue occur. Source Endpoint The change capture process, used when replicating ongoing changes, collects changes from the database logs by using the database engines native API, no client side install is required. Each engine has specific configuration requirements for exposing this change stream to a given user account (for details, see the AWS Key Management Service documentation). Most engines require some additional configuration to make the change data consumable in a meaningful way without data loss for the capture process. (For example, Oracle requires the addition of supplemental logging, and MySQL requires row-level bin logging.) Note When capturing changes from an Amazon Relational Database Service (Amazon RDS) source, ensure backups are enabled and the source is configured to retain change logs for a sufficiently long time (usually 24 hours). Target Endpoint Whenever possible, AWS DMS attempts to create the target schema for you, including underlying tables and primary keys. However, sometimes this isn’t possible. For example, when the target is Oracle, AWS DMS doesn’t create the target schema for security reasons. In MySQL, you have the option through extra connection parameters to have AWS DMS migrate objects to the specified database or to have AWS DMS create each database for you as it finds the database on the source. Note For the purposes of this paper, in Oracle a user and schema are synonymous. In MySQL, schema is synonymous with database. Both SQL Server Archived and Postgres have a concept of database AND schema. In this paper, we’re referring to the schema. Page 7 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Task The following section highlights common and important options to consider when creating a task. Migration Type  Migrate existing data. If you can afford an outage that’s long enough to copy your existing data, this is a good option to choose. This option simply migrates the data from your source system to your target, creating tables as needed.  Migrate existing data and replicate ongoing changes. This option performs a full data load while capturing changes on the source. After the full load is complete, captured changes are applied to the target. Eventually, the application of changes will reach a steady state. At that point, you can shut down your applications, let the remaining changes flow through to the target, and restart your applications to point at the target.  Replicate data changes only. In some situations it may be more efficient to copy the existing data by using a method outside of AWS DMS. For example, in a homogeneous migration, using native export/import tools can be more efficient at loading the bulk data. When this is the case, you can use AWS DMS to replicate changes as of the point in time at which you started your bulk load to bring and keep your source and target systems in sync. When replicating data changes only, you need to specify a time from which AWS DMS will begin to read changes from the database change logs. It’s important to keep these logs available on the server for a period of time to ensure AWS DMS has access to these changes. This is typically achieved by keeping the logs available for 24 hours (or longer) during the migration process. Start Task on Create By default, AWS DMS will start your task as soon as you create it. In some situations, it’s helpful to postpone the start of the task. For example, using the AWS Command Line Interface (AWS CLI), you may have a process that creates a task and a different process that starts the task, based on some triggering event. Target Table Prep Mode Archived Target table prep mode tells AWS DMS what to do with tables that already exist. If a table that is a member of a migration doesn’t yet exist on the target, AWS DMS will create the table. By default, AWS DMS will drop and recreate any existing tables on the target in preparation for a full load or a reload. If you’re pre-creating your schema, set your target table prep mode to truncate, causing AWS DMS to truncate existing tables prior to load or reload. When the table Page 8 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 prep mode is set to do nothing, any data that exists in the target tables is left as is. This can be useful when consolidating data from multiple systems into a single table using multiple tasks. AWS DMS performs these steps when it creates a target table:  The source database column data type is converted into an intermediate AWS DMS data type.  The AWS DMS data type is converted into the target data type. This data type conversion is performed for both heterogeneous and homogeneous migrations. In a homogeneous migration, this data type conversion may lead to target data types not matching source data types exactly. For example, in some situations it’s necessary to triple the size of varchar columns to account for multi-byte characters. We recommend going through the AWS DMS documentation on source and target data types to see if all the data types you use are supported. If the resultant data types aren’t to your liking when you’re using AWS DMS to create your objects, you can pre-create those objects on the target database. If you do pre create some or all of your target objects, be sure to choose the truncate or do nothing options for target table preparation mode. LOB Controls Due to their unknown and sometimes large size, large objects (LOBs) require more processing and resources than standard objects. To help with tuning migrations of systems that contain LOBs, AWS DMS offers the following options:  Don’t include LOB columns. When this option is selected, tables that include LOB columns are migrated in full, however, any columns containing LOBs will be omitted.  Full LOB mode. When you select full LOB mode, AWS DMS assumes no information regarding the size of the LOB data. LOBs are migrated in full, in successive pieces, whose size is determined by the LOB chunk size. Changing the LOB chunk size affects the memory consumption of AWS DMS; a large LOB chunk size requires more memory and processing. Memory is consumed per LOB, per row. If you have a table containing three LOBs, and are moving data 1,000 rows at a time, an LOB chunk size of 32 k will require 3*32*1000 = 96,000 k of memory for processing. Ideally, the LOB chunk size should be set to allow AWS DMS to retrieve the majority of LOBs in as few chunks as possible. For example, if 90 percent of your LOBs are less than 32 k, then setting the LOB chunk size to 32 k would be reasonable, assuming you have the memory to accommodate the setting. Archived  Limited LOB mode. When limited LOB mode is selected, any LOBs that are larger than max LOB size are truncated to max LOB size and a warning is issued to the log file. Using limited LOB mode is almost always more efficient and faster than full LOB mode. You can usually query your data dictionary to determine the size of the largest LOB in a table, setting max LOB size to something slightly larger than this (don’t forget to account for multi-byte characters). If you have a table in which most LOBs are small, with a few Page 9 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 large outliers, it may be a good idea to move the large LOBs into their own table and use two tasks to consolidate the tables on the target. LOB columns are transferred only if the source table has a primary key or a unique index on the table. Transfer of data containing LOBs is a two-step process: 1. The containing row on the target is created without the LOB data. 2. The table is updated with the LOB data. The process was designed this way to accommodate the methods source database engines use to manage LOBs and changes to LOB data. Enable Logging It’s always a good idea to enable logging because many informational and warning messages are written to the logs. However, be advised that you’ll incur a small charge, as the logs are made accessible by using Amazon CloudWatch. Find appropriate entries in the logs by looking for lines that start with the following:  Lines starting with “E:” – Errors  Lines starting with “W:” – Warnings  Lines starting with “I:” – Informational messages You can use grep (on UNIX-based text editors) or search (for Windows-based text editors) to find exactly what you’re looking for in a huge task log. Monitoring Your Tasks There are several options for monitoring your tasks using the AWS DMS console. Host Metrics You can find host metrics on your replication instances monitoring tab. Here, you can monitor whether your replication instance is sized appropriately. Replication Task Metrics Metrics for replication tasks, including incoming and committed changes, and latency between the replication host and source/target databases can be found on the task monitoring tab for each particular task. Table Metrics Archived Individual table metrics can be found under the table statistics tab for each individual task. These metrics include: the number of rows loaded during the full load; the number of inserts, updates, and deletes since the task started; and the number of DDL operations since the task started. Page 10 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Performance Expectations There are a number of factors that will affect the performance of your migration: resource availability on the source, available network throughput, resource capacity of the replication server, ability of the target to ingest changes, type and distribution of source data, number of objects to be migrated, and so on. In our tests, we have been able to migrate a terabyte of data in approximately 12–13 hours (under “ideal” conditions). Our tests were performed using source databases running on EC2, and in Amazon RDS with target databases in RDS. Our source databases contained a representative amount of relatively evenly distributed data with a few large tables containing up to 250 GB of data. Increasing Performance The performance of your migration will be limited by one or more bottlenecks you encounter along the way. The following are a few things you can do to increase performance. Load Multiple Tables in Parallel By default, AWS DMS loads eight tables at a time. You may see some performance improvement by increasing this slightly when you’re using a very large replication server; however, at some point increasing this parallelism will reduce performance. If your replication server is smaller, you should reduce this number. Remove Bottlenecks on the Target During the migration, try to remove any processes that would compete for write resources on your target database. This includes disabling unnecessary triggers, validation, secondary indexes, and so on. When migrating to an RDS database, it’s a good idea to disable backups and Multi-AZ on the target until you’re ready to cutover. Similarly, when migrating to non-RDS systems, disabling any logging on the target until cutover is usually a good idea. Use Multiple Tasks Sometimes using multiple tasks for a single migration can improve performance. If you have sets of tables that don’t participate in common transactions, it may be possible to divide your migration into multiple tasks. Note Transactional consistency is maintained within a task. Therefore, it’s important that tables in separate tasks don’t participate in common Archived transactions. Additionally, each task will independently read the transaction stream. Therefore, be careful not to put too much stress on the source system. For very large systems or systems with many LOBs, you may also consider using multiple replication servers, each containing one or more tasks. A review of the Page 11 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 host statistics of your replication server can help you determine whether this might be a good option. Improving LOB Performance Pay attention to the LOB parameters. Whenever possible, use limited LOB mode. If you have a table which consists of a few large LOBs and mostly smaller LOBs, consider breaking up the table into a table that contains the large LOBs and a table that contains the small LOBs prior to the migration. You can then use a task in limited LOB mode to migrate the table containing small LOBs, and a task in full LOB mode to migrate the table containing large LOBs. Important In LOB processing, LOBs are migrated using a two-step process: first, the containing row is created without the LOB, and then the row is updated with the LOB data. Therefore, even if the LOB column is NOT NULLABLE on the source, it must be nullable on the target during the migration. Optimizing Change Processing By default, AWS DMS processes changes in a transactional mode, which preserves transactional integrity. If you can afford temporary lapses in transactional integrity, you can turn on batch optimized apply. Batch optimized apply groups transactions and applies them in batches for efficiency purposes. Note Using batch optimized apply will almost certainly violate referential integrity constraints. Therefore, you should disable them during the migration process and enable them as part of the cutover process. Reducing Load on Your Source System During a migration, AWS DMS performs a full table scan of each source table being processed (usually in parallel). Additionally, each task periodically queries the source for change information. To perform change processing, you may be required to increase the amount of data written to your database’s change log. If you find you are overburdening your source database, you can reduce the number of tasks or tables per task of your migration. If you prefer not to add load to your source, consider performing the migration from a read copy of your source system. Archived Page 12 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Note Using a read copy will increase the replication lag. Frequently Asked Questions What Are the Main Reasons for Performing a Database migration? Would you like to move your database from a commercial engine to an open source alternative? Perhaps you want to move your on-premises database into the AWS Cloud. Would you like to divide your database into functional pieces? Maybe you’d like to move some of your data from RDS into Amazon Redshift. These and other similar scenarios can be considered “database migrations”. What Steps Does a Typical Migration Project Include? This of course depends on the reason for and type of migration you choose to perform. At a minimum, you’ll want to do the following. Perform an Assessment In an assessment, you determine the basic framework of your migration and discover things in your environment that you’ll need to change to make a migration successful. The following are some questions to ask:  Which objects do I want to migrate?  Are my data types compatible with those covered by AWS DMS?  Does my source system have the necessary capacity and is it configured to support a migration?  What is my target and how should I configure it to get the required or desired capacity? Prototype Migration Configuration This is typically an iterative process. It’s a good idea to use a small test migration consisting of a couple of tables to verify you’ve got things properly configured. Once you’ve verified your configuration, test the migration with any objects you suspect could be difficult. These can include LOB objects, character set conversions, complex data types, and so on. When you’ve worked out any kinks related to complexity, test your largest tables to see what sort of throughput you can achieve for them. Design Your Migration Archived Concurrently with the prototyping stage, you should determine exactly how you intend to migrate your application. The steps can vary dramatically, depending on the type of migration you’re performing. Page 13 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 Testing Your End-to-End Migration After you have completed your prototyping, it’s a good idea to test a complete migration. Are all objects accounted for? Does the migration fit within expected time limits? Are there any errors or warnings in the log files that are a concern? Perform Your Migration After you’re satisfied that you’ve got a comprehensive migration plan and have tested your migration end-to-end, it’s time to perform your migration! How Much Load Will the Migration Process Add to My Source Database? This a complex question with no specific answer. The load on a source database is dependent upon several things. During a migration, AWS DMS performs a full table scan of the source table for each table processed in parallel. Additionally, each task periodically queries the source for change information. To perform change processing, you may be required to increase the amount of data written to your databases change log. If your tasks contain a Change Data Capture (CDC) component, the size, location, and retention of log files can have an impact on the load. How Long Does a Typical Database Migration Take? The following are items that determine the length of your migration: total amount of data being migrated, amount and size of LOB data, size of the largest tables, total number of objects being migrated, secondary indexes created on the target before the migration, resources available on the source system, resources available on the target system, resources available on the replication server, network throughput, and so on. Clearly, there is no one formula that will predict how long your migration will take. The best way to gauge how long your particular migration will take is to test it. I’m Changing Engines–How Can I Migrate My Complete Schema? As previously stated, AWS DMS will only create those objects needed to perform an optimized migration of your data. You can use the free AWS Schema Conversion Tool (AWS SCT) to convert an entire schema from one database engine to another. The AWS SCT can be used with AWS DMS to facilitate the migration of your entire system. Archived Why Doesn’t AWS DMS Migrate My Entire Schema? All database engines supported by AWS DMS have native tools that you can use to export and import your schema in a homogeneous environment. Amazon has developed the AWS SCT to facilitate the migration of your schema in a heterogeneous environment. The AWS DMS is Page 14 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 intended to be used with one of these methods to perform a complete migration of your database. Who Can Help Me with My Database Migration Project? Most of Amazon’s customers should be able to complete a database migration project by themselves. However, if your project is challenging, or you are short on resources, one of our migration partners should be able to help you out. For details, please visit https://aws.amazon.com/partners. What Are the Main Reasons to Switch Database Engines? There are two main reasons we see people switching engines:  Modernization. The customer wants to use a modern framework or platform for their application portfolio, and these platforms are available only on more modern SQL or NoSQL database engines.  License fees. The customer wants to migrate to an open source engine to reduce license fees. How Can I Migrate from Unsupported Database Engine Versions? Amazon has tried to make AWS DMS compatible with as many supported database versions as possible. However, some database versions don’t support the necessary features required by AWS DMS, especially with respect to change capture and apply. Currently, to fully migrate from an unsupported database engine, you must first upgrade your database to a supported engine. Alternatively, you may be able to perform a complete migration from an “unsupported” version if you don’t need the change capture, and apply capabilities of DMS. If you are performing a homogeneous migration, one of the following methods might work for you:  MySQL: Importing and Exporting Data From a MySQL DB Instance  Oracle: Importing Data Into Oracle on Amazon RDS  SQL Server: Importing and Exporting SQL Server Databases  PostgreSQL: Importing Data into PostgreSQL on Amazon RDS Archived Page 15 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 When Should I NOT Use DMS? Most databases offer a native method for migrating between servers or platforms. Sometimes, using a simple backup and restore or export/import is the most efficient way to migrate your data into AWS. If you’re considering a homogeneous migration, you should first assess whether a suitable native option exists. In some situations, you might choose to use the native tools to perform the bulk load and use DMS to capture and apply changes that occur during the bulk load. For example when migrating between different flavors of MySQL or Amazon Aurora, creating and promoting a read replica is most likely your best option. See Importing and Exporting Data From a MySQL DB Instance. When Should I Use a Native Replication Mechanism Instead of the DMS and the AWS Schema Conversion Tool? This is very much related to the previous question. If you can successfully set up a replica of your primary database in your target environment by using native tools more easily than you can with DMS, you should consider using that native method for migrating your system. Some examples include:  Read replicas – MySQL  Standby databases – Oracle, Postgres  AlwaysOn availability groups – SQL Server Note AlwaysOn is not supported in RDS. What Is the Maximum Size of Database That DMS Can Handle? This depends on your environment, the distribution of data, and how busy your source system is. The best way to determine whether your particular system is a candidate for DMS is to test it out. Start slowly, to get the configuration worked out, add some complex objects, and finally attempt a full load as a test. As a ballpark maximum figure: Under mostly ideal conditions (EC2 to RDS, cross region), over the course of a weekend (approximately 33 hours) we were able to migrate five terabytes of relatively evenly distributed data, including four large (250 GB) tables, a huge (1 TB) table, 1,000 small to moderately sized tables, three tables that contained LOBs varying between 25 GB and 75 GB, and 10,000 very small tables. Archived Page 16 of 17 Amazon Web Services – AWS Database Migration Service Best Practices August 2016 What if I Want to Migrate from Classic to VPC? DMS can be used to help minimize database-related outages when moving a database from outside a VPC into a VPC. The following are the basic strategies for migrating into a VPC:  Generic EC2 Classic to VPC Migration Guide: Migrating from a Linux Instance in EC2 Classic to a Linux Instance in a VPC  Specific Procedures for RDS: Moving a DB Instance Not in a VPC into a VPC Conclusion This paper outlined best practices for using AWS DMS to migrate data from a source database to a target database, and offers answers to several frequently asked questions about migrations. As companies move database workloads to AWS, they are often also interested in changing their primary database engine. Most current methods for migrating databases to the cloud or switching engines require an extended outage. The AWS DMS helps to migrate database workloads to AWS or change database engines while minimizing any associated downtime. Contributors The following individuals and organizations contributed to this document:  Ed Murray, Senior Database Engineer, Amazon RDS/AWS DMS  Arun Thiagarajan, Cloud Support Engineer, AWS Premium Support Archived Page 17 of 17  
Managing Your AWS Infrastructure at Scale This paper has been archived. For the latest technical guidance on AWS Infrastructure, see the AWS Whitepapers & Guides page: https://aws.amazon.com/whitepapers/ Shaun Pearce Steven Bryen February 2015 Archived Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 © 2015, Amazon Web Services, Inc. or its affiliates. All rights reserved. Notices This document is provided for informational purposes only. It represents AWS’s current product offerings and practices as of the date of issue of this document, which are subject to change without notice. Customers are responsible for making their own independent assessment of the information in this document and any use of AWS’s products or services, each of which is provided “as is” without warranty of any kind, whether express or implied. This document does not create any warranties, representations, contractual commitments, conditions or assurances from AWS, its affiliates, suppliers or licensors. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Archived Page 2 of 32 Archived Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Page 3 of 32 Contents Abstract 4 Introduction 4 Provisioning New EC2 Instances 6 Creating Your Own AMI 7 Managing AMI Builds 9 Dynamic Configuration 12 Scripting Your Own Solution 12 Using Configuration Management Tools 16 Using AWS Services to Help Manage Your Environments 22 AWS Elastic Beanstalk 22 AWS OpsWorks 23 AWS CloudFormation 24 User Data 24 cfn-init 25 Using the Services Together 26 Managing Application and Instance State 27 Structured Application Data 28 Amazon RDS 28 Amazon DynamoDB 28 Unstructured Application Data 29 User Session Data 29 Amazon ElastiCache 29 System Metrics 30 Amazon CloudWatch 30 Log Management 31 Amazon CloudWatch Logs 31 Conclusion 32 Further Reading 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Abstract Amazon Web Services (AWS) enables organizations to deploy large-scale application infrastructures across multiple geographic locations. When deploying these large, cloud based applications, it’s important to ensure that the cost and complexity of operating such systems does not increase in direct proportion to their size. This whitepaper is intended for existing and potential customers—especially architects, developers, and sysops administrators—who want to deploy and manage their infrastructure in a scalable and predictable way on AWS. In this whitepaper, we describe tools and techniques to provision new instances, configure the instances to meet your requirements, and deploy your application code. We also introduce strategies to ensure that your instances remain stateless, resulting in an architecture that is more scalable and fault tolerant. The techniques we describe allow you to scale your service from a single instance to thousands of instances while maintaining a consistent set of processes and tools to manage them. For the purposes of this whitepaper, we assume that you have knowledge of basic scripting and core services such as Amazon Elastic Compute Cloud (Amazon EC2). Introduction When designing and implementing large, cloud-based applications, it’s important to consider how your infrastructure will be managed to ensure the cost and complexity of running such systems is minimized. When you first begin using Amazon EC2, it is easy to manage your EC2 instances just like regular virtualized servers running in your data center. You can create an instance, log in, configure the operating system, install any additional packages, and install your application code. You can maintain the instance by installing security patches, rolling out new deployments of your code, and modifying the configuration as needed. Despite the operational overhead, you can continue to manage your instances in this way for a long time. However, your instances will inevitably begin to diverge from their original specification, which can lead to inconsistencies with other instances in the same environment. This divergence from a known baseline can become a huge challenge when managing large fleets of instances across multiple environments. Ultimately, it will lead to service issues because your environments will become less predictable and more difficult to maintain. Archived The AWS platform provides you with a set of tools to address this challenge with a different approach. By using Amazon EC2 and associated services, you can specify and manage the desired end state of your infrastructure independently of the EC2 instances and other running components. Page 4 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 For example, with a traditional approach you would alter the configuration of an Apache server running across your web servers by logging in to each server in turn and manually making the change. By using the AWS platform, you can take a different approach by changing the underlying specification of your web servers and launching new EC2 instances to replace the old ones. This ensures that each instance remains identical; it also reduces the effort to implement the change and reduces the likelihood of errors being introduced. When you start to think of your infrastructure as being defined independently of the running EC2 instances and other components in your environments, you can take greater advantage of the benefits of dynamic cloud environments: • Software-defined infrastructure – By defining your infrastructure using a set of software artifacts, you can leverage many of the tools and techniques that are used when developing software components. This includes managing the evolution of your infrastructure in a version control system, as well as using continuous integration (CI) processes to continually test and validate infrastructure changes before deploying them to production. • Auto Scaling and self-healing – If you automatically provision your new instances from a consistent specification, you can use Auto Scaling groups to manage the number of instances in an EC2 fleet. For example, you can set a condition to add new EC2 instances in increments to the Auto Scaling group when the average utilization of your EC2 fleet is high. You can also use Auto Scaling to detect impaired EC2 instances and unhealthy applications, and replace the instances without your intervention. • Fast environment provisioning – You can quickly and easily provision consistent environments, which opens up new ways of working within your teams. For example, you can provision a new environment to allow testers to validate a new version of your application in their own, personal test environments that are isolated from other changes. • Reduce costs – Now that you can provision environments quickly, you also have the option to remove them when they are no longer needed. This reduces costs because you pay only for the resources that you use. • Blue-green deployments – You can deploy new versions of your application by provisioning new instances (containing a new version of the code) beside your existing infrastructure. You can then switch traffic between environments in an approach known as blue-green deployments. This has many benefits over traditional deployment strategies, including the ability to quickly and easily roll back a deployment in the event of an issue. Archived To leverage these advantages, your infrastructure must have the following capabilities: Page 5 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 1. New infrastructure components are automatically provisioned from a known, version- controlled baseline in a repeatable and predictable manner 2. All instances are stateless so that they can be removed and destroyed at any time, without the risk of losing application state or system data The following figure shows the overall process: 1 EC2 Instance Version Control System 2 Durable Storage Figure 1: Instance Lifecycle and State Management The following sections outline tools and techniques that you can use to build a system with these capabilities. By moving to an architecture where your instances can be easily provisioned and destroyed with no loss of data, you can fundamentally change the way you manage your infrastructure. Ultimately, you can scale your infrastructure over time without significantly increasing the operational overhead associated with it. Provisioning New EC2 Instances A number of external events will require you to provision new instances into your environments: • Creating new instances or replicating existing environments • Replacing a failed instance in an existing environment • Responding to a “scale up” event to add additional instances to an Auto Scaling group • Deploying a new version of your software stack (by using blue-green deployments) Some of these events are difficult or even impossible to predict, so it’s important that the process to create new instances into your environment is fully automated, repeatable, and consistent. Archived The process of automatically provisioning new instances and bringing them into service is known as bootstrapping. There are multiple approaches to bootstrapping your Amazon EC2 instances. The two most popular approaches are to either create your own Page 6 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Amazon Machine Image (AMI) or to use dynamic configuration. We explain both approaches in the following sections. Creating Your Own AMI An Amazon Machine Image (AMI) is a template that provides all of the information required to launch an Amazon EC2 instance. At a minimum it contains the base operating system, but it may also include additional configuration and software. You can launch multiple instances of an AMI, and you can also launch different types of instances from a single AMI. You have several options when launching a new EC2 instance: • Select an AMI provided by AWS • Select an AMI provided by the community • Select an AMI containing pre-configured software from the AWS Marketplace1 • Create a custom AMI If launching an instance from a base AMI containing only the operating system, you can further customize the instance with additional configuration and software after it has been launched. If you create a custom AMI, you can launch an instance that already contains your complete software stack, thereby removing the need for any run-time configuration. However, before you decide whether to create a custom AMI, you should understand the advantages and disadvantages. Advantages of custom AMIs • Increases speed – All configuration is packaged into the AMI itself, which significantly increases the speed in which new instances can be launched. This is particularly useful during Auto Scaling events. • Reduces external dependencies – Packaging everything into an AMI means that there is no dependency on the availability of external services when launching new instances (for example, package or code repositories). • Removes the reliance on complex configuration scripts at launch time – By preconfiguring your AMI, scaling events and instance replacements no longer rely on the successful completion of configuration scripts at launch time. This reduces the likelihood of operational issues caused by erroneous scripts. Disadvantages of custom AMIs Archived 1https://aws.amazon.com/marketplace Page 7 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 • Loss of agility – Packaging everything into an AMI means that even simple code changes and defect fixes will require you to produce a new AMI. This increases the time it takes to develop, test, and release enhancements and fixes to your application. • Complexity – Managing the AMI build process can be complex. You need a process that enables the creation of consistent, repeatable AMIs where the changes between revisions are identifiable and auditable. • Run-time configuration requirements – You might need to make additional customizations to your AMIs based on run-time information that cannot be known at the time the AMI is created. For example, the database connection string required by your application might change depending on where the AMI is used. Given these advantages and disadvantages, we recommend a hybrid approach: build static components of your stack into AMIs, and configure dynamic aspects that change regularly (such as application code) at run time. Consider the following factors to help you decide what configuration to include within a custom AMI and what to include in dynamic run-time scripts: • Frequency of deployments – How often are you likely to deploy enhancements to your system, and at what level in your stack will you make the deployments? For example, you might deploy changes to your application on a daily basis, but you might upgrade your JVM version far less frequently. • Reduction on external dependencies – If the configuration of your system depends on other external systems, you might decide to carry out these configuration steps as part of an AMI build rather than at the time of launching an instance. • Requirements to scale quickly – Will your application use Auto Scaling groups to adjust to changes in load? If so, how quickly will the load on the application increase? This will dictate the speed in which you need to provision new instances into your EC2 fleet. Once you have assessed your application stack based on the preceding criteria, you can decide which elements of your stack to include in a custom AMI and which will be configured dynamically at the time of launch. The following figure shows a typical Java web application stack and how it could be managed across AMIs and dynamic scripts. Archived Page 8 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 App Config Application Code App Frameworks Apache Tomcat JVM OS Users & Grps OS EC2 Instance App Config Application Code App Frameworks Apache Bootstrapping Code Base AMI Tomcat JVM OS Users & Grps OS EC2 Instance App Config Application Code Bootstrapping Code Foundational AMI App Frameworks Apache Tomcat JVM OS Users & Grps OS EC2 Instance Figure 2: Base, Foundational, and Full AMI Models Bootstrapping Code Full stack AMI In the base AMI model, only the OS image is maintained as an AMI. The AMI can be an AWS-managed image, or an AMI that you manage that contains your own OS image. In the foundational AMI model, elements of a stack that change infrequently (for example, components such as the JVM and application server) are built into the AMI. In the full stack AMI model, all elements of the stack are built into the AMI. This model is useful if your application changes infrequently, or if your application has rapid auto scaling requirements (which means that dynamically installing the application isn’t feasible). However, even if you build your application into the AMI, it still might be advantageous to dynamically configure the application at run time because it increases the flexibility of the AMI. For example, it enables you to use your AMIs across multiple environments. Managing AMI Builds Many people start by manually configuring their AMIs using a process similar to the following: Archived 1. Launch the latest version of the AMI 2. Log in to the instance and manually reconfigure it (for example, by making package updates or installing new applications) 3. Create a new AMI based on the running instance Page 9 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Although this manual process is sufficient for simple applications, it is difficult to manage in more complex environments where AMI updates are needed regularly. It’s essential to have a consistent, repeatable process to create your AMIs. It’s also important to be able to audit what has changed between one version of your AMI and another. One way to achieve this is to manage the customization of a base AMI by using automated scripts. You can develop your own scripts, or you can use a configuration management tool. For more information about configuration management tools, see the Using Configuration Management Tools section in this whitepaper. Using automated scripts has a number of advantages over the manual method. Automation significantly speeds up the AMI creation process. In addition, you can use version control for your scripts/configuration files, which results in a repeatable process where the change between AMI versions is transparent and auditable. This automated process is similar to the manual process: 1. Launch the latest version of the AMI 2. Execute the automated configuration using your tool of choice 3. Create a new AMI image based on the running instance You can use a third-party tool such as Packer2 to help automate the process. However, many find that this approach is still too time consuming for an environment with multiple, frequent AMI builds across multiple environments. If you use the Linux operating system, you can reduce the time it takes to create a new AMI by customizing an Amazon Elastic Block Store (Amazon EBS) volume rather than a running instance. An Amazon EBS volume is a durable, block-level storage device that you can attach to a single Amazon EC2 instance. It is possible to create an Amazon EBS volume from a base AMI snapshot and customise this volume before storing it as a new AMI. This replaces the time taken to initialize an EC2 instance with the far shorter time needed to create and attach an EBS volume. In addition, this approach makes use of the incremental nature of Amazon EBS snapshots. An EBS snapshot is a point-in-time backup of an EBS volume that is stored in Amazon S3. Snapshots are incremental backups, meaning that only the blocks on the device that have changed after your most recent snapshot are saved. For example, if a configuration update changes only 100 MB of the blocks on an 8 GB EBS volume, only 100 MB will be stored to Amazon S3. Archived 2https://packer.io Page 10 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 To achieve this, you need a long running EC2 instance that is responsible for attaching a new EBS volume based on the latest AMI build, executing the scripts needed to customize the volume, creating a snapshot of the volume, and registering the snapshot as a new version of your AMI. For example, Netflix uses this technique in their open source tool called aminator.3 The following figure shows this process. Figure 3: Using EBS Snapshots to Speed Up Deployments 1. Create the volume from the latest AMI snapshot 2. Attach the volume to the instance responsible for building new AMIs 3. Run automated provisioning scripts to update the AMI configuration 4. Snapshot the volume 5. Register the snapshot as a new version of the AMI Archived 3https://github.com/Netflix/aminator Page 11 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Dynamic Configuration Now that you have decided what to include into your AMI and what should be dynamically configured at run time, you need to decide how to complete the dynamic configuration and bootstrapping process. There are many tools and techniques that you can use to configure your instances, ranging from simple scripts to complex, centralized configuration management tools. Scripting Your Own Solution Depending on how much pre-configuration has been included into your AMI, you might need only a single script or set of scripts as a simple, elegant way to configure the final elements of your application stack. User Data and cloud-init When you launch a new EC2 instance by using either the AWS Management Console or the API, you have the option of passing user data to the instance. You can retrieve the user data from the instance through the EC2 metadata service, and use it to perform automated tasks to configure instances as they are first launched. When a Linux instance is launched, the initialization instructions passed into the instance by means of the user data are executed by using a technology called cloud-init. The cloud-init package is an open source application built by Canonical. It’s included in many base Linux AMIs (to find out if your distribution supports cloud-init, see the distribution-specific documentation). Amazon Linux, a Linux distribution created and maintained by AWS, contains a customized version of cloud-init. You can pass two types of user data, either shell scripts or cloud-init directives, to cloud-init running on your EC2 instance. For example, the following shell script can be passed to an instance to update all installed packages and to configure the instance as a PHP web server: #!/bin/sh yum update -y yum -y install httpd php php-mysql chkconfig httpd on /etc/init.d/httpd start The following user data achieves the same result, but uses a set of cloud-init directives: #cloud-config Archived Page 12 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 repo_update: true repo_upgrade: all packages: - httpd - php - php-mysql runcmd: - service httpd start - chkconfig httpd on AWS Windows AMIs contain an additional service, EC2Config, that is installed by AWS. The EC2Config service performs tasks on the instance such as activating Windows, setting the Administrator password, writing to the AWS console, and performing one- click sysprep from within the application. If launching a Windows instance, the EC2Config service can also execute scripts passed to the instance by means of the user data. The data can be in the form of commands that you run at the cmd.exe prompt or Windows PowerShell prompt. This approach works well for simple use cases. However, as the number of instance roles (web, database, and so on) grows along with the number of environments that you need to manage, your scripts might become large and difficult to maintain. Additionally, user data is limited to 16 KB, so if you have a large number of configuration tasks and associated logic, we recommend that you use the user data to download additional scripts from Amazon S3 that can then be executed. Leveraging EC2 Metadata When you configure a new instance, you typically need to understand the context in which the instance is being launched. For example, you might need to know the hostname of the instance or which region or Availability Zone the instance has been launched into. The EC2 metadata service can be queried to provide such contextual information about an instance, as well as retrieving the user data. To access the instance metadata from within a running instance, you can make a standard HTTP GET using tools such as cURL or the GET command. For example, to retrieve the host name of the instance, you can make an HTTP GET request to the following URL: http://169.254.169.254/latest/meta-data/hostname Archived Page 13 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Resource Tagging To help you manage your EC2 resources, you can assign your own metadata to each instance in addition to the EC2 metadata that is used to define hostnames, Availability Zones, and other resources. You do this with tags. Each tag consists of a key and a value, both of which you define when the instance is launched. You can use EC2 tags to define further context to the instance being launched. For example, you can tag your instances for different environments and roles, as shown in the following figure. Key i-1bbb2637 Value environment = production role = web i-f2871ade environment = dev role = app Figure 4: Example of EC2 Tag Usage As long as your EC2 instance has access to the Internet, these tags can be retrieved by using the AWS Command Line Interface (CLI) within your bootstrapping scripts to configure your instances based on their role and the environment in which they are being launched. Putting it all Together The following figure shows a typical bootstrapping process using user data and a set of configuration scripts hosted on Amazon S3. Archived Page 14 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Instance Launch Request User Data Receive user data and expose via metadata service EC2 Metadata Service describe-tags describe-tags Retrieve and process User Data Download base config and execute Base Configuration Retrieve server role from EC2 API, download and execute appropriate script Retrieve server environment from EC2 API, download and execute appropriate script Server Role Overlay Scripts Bootstrap Complete EC2 API Amazon EC2 Instance Environment Overlay Scripts Amazon S3 Bucket Figure 5: Example of an End-to-End Workflow This example uses the user data as a lightweight mechanism to download a base configuration script from Amazon S3. The script is responsible for configuring the system to a baseline across all instances regardless of role and environment (for example, the script might install monitoring agents and ensure that the OS is patched). Archived This base configuration script uses the CLI to retrieve the instances tags. Based on the value of the “role” tag, the script downloads an additional overlay script responsible for the additional configuration required for the instance to perform its specific role (for example, installing Apache on a web server). Finally, the script uses the instances “environment” tag to download an appropriate environment overlay script to carry out the Page 15 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 final configuration for the environment the instance resides in (for example, setting log levels to DEBUG in the development environment). To protect sensitive information that might be contained in your scripts, you should restrict access to these assets by using IAM Roles.4 Using Configuration Management Tools Although scripting your own solution works, it can quickly become complex when managing large environments. It also can become difficult to govern and audit your environment, such as identifying changes or troubleshooting configuration issues. You can address some of these issues by using a configuration management tool to manage instance configurations. Configuration management tools allow you to define your environment’s configuration in code, typically by using a domain-specific language. These domain-specific languages use a declarative approach to code, where the code describes the end state and is not a script that can be executed. Because the environment is defined using code, you can track changes to the configuration and apply version control. Many configuration management tools also offer additional features such as compliance, auditing, and search. Push vs. Pull Models Configuration management tools typically leverage one of two models, push or pull. The model used by a tool is defined by how a node (a target EC2 instance in AWS) interacts with the master configuration management server. In a push model, a master configuration management server is aware of the nodes that it needs to manage and pushes the configuration to them remotely. These nodes need to be pre-registered on the master server. Some push tools are agentless and execute configuration remotely using existing protocols such as SSH. Others push a package, which is then executed locally using an agent. The push model typically has some constraints when working with dynamic and scalable AWS resources: • The master server needs to have information about the nodes that it needs to manage. When you use tools such as Auto Scaling, where nodes might come and go, this can be a challenge. • Push systems that do remote execution do not scale as well as systems where configuration changes are offloaded and executed locally on a node. In large Archived 4http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html Page 16 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 environments, the master server might get overloaded when configuring multiple systems in parallel. • Connecting to nodes remotely requires you to allow specific ports to be allowed inbound to your nodes. For some remote execution tools, this includes remote SSH. The second model is the pull model. Configuration management tools that use a pull system use an agent that is installed on a node. The agent asks the master server for configuration. A node can pull its configuration at boot time, or agents can be daemonized to poll the master periodically for configuration changes. Pull systems are especially useful for managing dynamic and scalable AWS environments. Following are the main benefits of the pull model: • Nodes can scale up and down easily, as the master does not need to know they exist before they can be configured. Nodes can simply register themselves with the server. • Configuration management masters require less scaling when using a pull system because all processing is offloaded and executed locally on the remote node. • No specific ports need to be opened inbound to the nodes. Most tools allow the agent to communicate with the master server by using typical outbound ports such as HTTPS. Chef Example Many configuration management tools work with AWS. Some of the most popular are Chef, Puppet, Ansible, and SaltStack. For our example in this section, we use Chef to demonstrate bootstrapping with a configuration management tool. You can use other tools and apply the same principles. Chef is an open source configuration management tool that uses an agent (chef-client) to pull configuration from a master server (Chef server). Our example shows how to bootstrap nodes by pulling configuration from a Chef server at boot time. The example is based on the following assumptions: • You have configured a Chef server • You have an AMI that has the AWS command line tools installed and configured • You have the chef-client installed and included into your AMI Archived First, let’s look at what we are going to configure within Chef. We’ll create a simple Chef cookbook that installs an Apache web server and deploys a ‘Hello World’ site. A Chef cookbook is a collection of recipes; a recipe is a definition of resources that should be configured on a node. This can include files, packages, permissions, and more. The default recipe for this Apache cookbook might look something like this: Page 17 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 # # Cookbook Name:: apache # Recipe:: default # # Copyright 2014, YOUR_COMPANY_NAME # # All rights reserved - Do Not Redistribute # package "httpd" #Allow Apache to start on boot service "httpd" do action [:enable, :start] end #Add HTML Template into Web Root template "/var/www/html/index.html" do source "index.html.erb" mode "0644" end In this recipe, we install, enable, and start the HTTPD (HTTP daemon) service. Next, we render a template for index.html and place it into the /var/www/html directory. The index.html.erb template in this case is a very simple HTML page:
Hello World
Next, the cookbook is uploaded to the Chef server. Chef offers the ability to group cookbooks into roles. Roles are useful in large-scale environments where servers within your environment might have many different roles, and cookbooks might have overlapping roles. In our example, we add this cookbook to a role called ‘webserver’. Now when we launch EC2 instances (nodes), we can provide EC2 user data to bootstrap them by using Chef. To make this as dynamic as possible, we can use an EC2 tag to define which Chef role to apply to our node. This allows us to use the same user data script for all nodes, whichever role is intended for them. For example, a web server and a database server can use the same user data if you assign different values to the ‘role’ tag in EC2. Archived We also need to consider how our new instance will authenticate with the Chef server. We can store our private key in an encrypted Amazon S3 bucket by using Amazon S3 Page 18 of 32 Archived Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Page 19 of 32 server side encryption,5 and we can restrict access to this bucket by using IAM roles. The key can then be used to authenticate with the Chef server. The chef-client uses a validator.pem file to authenticate to the Chef server when registering new nodes. We also need to know which Chef server to pull our configuration from. We can store a pre-populated client.rb file in Amazon S3 and copy this within our user data script. You might want to dynamically populate this client.rb file depending on environment, but for our example we assume that we have only one Chef server and that a pre-populated client.rb file is sufficient. You could also include these two files into your custom AMI build. The user data would look like this: #!/bin/bash cd /etc/chef #Copy Chef Server Private Key from S3 Bucket aws s3 cp s3://s3-bucket/orgname-validator.pem orgname validator.pem #Copy Chef Client Configuration File from S3 Bucket aws s3 cp s3://s3-bucket/client.rb client.rb #Change permissions on Chef Server private key. chmod 400 /etc/chef/orgname-validator.pem #Get EC2 Instance ID from the Meta-Data Service INSTANCE_ID=`curl -s http://169.254.169.254/latest/meta data/instance-id` #Get Tag with Key of ‘role’ for this EC2 instance ROLE_TAG=$(aws ec2 describe-tags --filters "Name=resource id,Values=$INSTANCE_ID" "Name=key,Values=role" --output text) #Get value of Tag with Key of ‘role’ as string ROLE_TAG_VALUE=$(echo $ROLE_TAG | awk 'NF>1{print $NF}') #Create first_boot.json file dynamically adding the tag value as the chef role in the run-list echo "{\"run_list\":[\"role[$ROLE_TAG_VALUE]\"]}" > first_boot.json 5 http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 #execute the chef-client using first_boot.json config chef-client -j first_boot.json #daemonize the chef-client to run every 5 minutes chef-client -d -i 300 -s 30 As shown in the preceding user data example, we copy our client configuration files from a private S3 bucket. We then use the EC2 metadata service to get some information about the instance (in this example, Instance ID). Next, we query the Amazon EC2 API for any tags with the key of ‘role,’ and dynamically configure a Chef run-list with a Chef role of this value. Finally, we execute the first chef-client run by providing the first_boot.json options, which include our new run-list. We then execute chef-client once more; however, this time we execute it in a daemonized setup to pull configuration every 5 minutes. We now have some re-usable EC2 user data that we can apply to any new EC2 instances. As long as a ‘role’ tag is provided with a value that matches a role on the target Chef server, the instance will be configured using the corresponding Chef cookbooks. Putting it all Together The following figure shows a typical workflow, from instance launch to a fully configured instance that is ready to serve traffic. Archived Page 20 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Instance Launch Request User Data Receive user data and expose via metadata service EC2 Metadata Service describe-tags EC2 API describe-tags Retrieve and process User Data Download private key and client.rb from S3 bucket Retrieve server role from EC2 API Chef config files Configure first_boot.son to use chef role with tag value EC2 API Pull Config from Chef Server and configure instance Bootstrap Complete Amazon EC2 Instance Amazon S3 Bucket Figure 6: Example of an End-to-End Workflow Archived Page 21 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Using AWS Services to Help Manage Your Environments In the preceding sections, we discussed tools and techniques that systems administrators and developers can use to provision EC2 instances in an automated, predictable, and repeatable manner. AWS also provides a range of application management services that help make this process simpler and more productive. The following figure shows how to select the right service for your application based on the level of control that you require. Figure 7: AWS Deployment and Management Services In addition to provisioning EC2 instances, these services can also help you to provision any other associated AWS components that you need in your systems, such as Auto Scaling groups, load balancers, and networking components. We provide more information about how to use these services in the following sections. AWS Elastic Beanstalk AWS Elastic Beanstalk allows web developers to easily upload code without worrying about managing or implementing any underlying infrastructure components. Elastic Beanstalk takes care of deployment, capacity provisioning, load balancing, auto scaling, and application health monitoring. It is worth noting that Elastic Beanstalk is not a black box service: You have full visibility and control of the underlying AWS resources that are deployed, such as EC2 instances and load balancers. Archived Elastic Beanstalk supports deployment of Java, .NET, Ruby, PHP, Python, Node.js, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk provides a default configuration, but you can extend the configuration as needed. For example, you might want to install additional packages from a yum repository or copy configuration files that your application depends on, such as a replacement for httpd.conf to override specific settings. Page 22 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 You can write the configuration files in YAML or JSON format and create the files with a .config file extension. You then place the files in a folder in the application root named .ebextensions. You can use configuration files to manage packages and services, work with files, and execute commands. For more information about using and extending Elastic Beanstalk, see AWS Elastic Beanstalk Documentation.6 AWS OpsWorks AWS OpsWorks is an application management service that makes it easy to deploy and manage any application and its required AWS resources. With AWS OpsWorks, you build application stacks that consist of one or many layers. You configure a layer by using an AWS OpsWorks configuration, a custom configuration, or a mix of both. AWS OpsWorks uses Chef, the open source configuration management tool, to configure AWS resources. This gives you the ability to provide your own custom or community Chef recipes. AWS OpsWorks features a set of lifecycle events—Setup, Configure, Deploy, Undeploy, and Shutdown—that automatically run the appropriate recipes at the appropriate time on each instance. AWS OpsWorks provides some AWS-managed layers for typical application stacks. These layers are open and customizable, which means that you can add additional custom recipes to the layers provided by AWS OpsWorks or create custom layers from scratch using your existing recipes. It is important to ensure that the correct recipes are associated with the correct lifecycle events. Lifecycle events run during the following times: • Setup – Occurs on a new instance after it successfully boots • Configure – Occurs on all of the stack’s instances when an instance enters or leaves the online state • Deploy – Occurs when you deploy an app • Undeploy – Occurs when you delete an app • Shutdown – Occurs when you stop an instance For example, the configure event is useful when building distributed systems or for any system that needs to be aware of when new instances are added or removed from the stack. You could use this event to update a load balancer when new web servers are added to the stack. Archived 6http://aws.amazon.com/documentation/elastic-beanstalk/ Page 23 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 In addition to typical server configuration, AWS OpsWorks manages application deployment and integrates with your application’s code repository. This allows you to track application versions and rollback deployments if needed. For more information about AWS OpsWorks, see AWS OpsWorks Documentation.7 AWS CloudFormation AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. Compared to Elastic Beanstalk and AWS OpsWorks, AWS CloudFormation gives you the most control and flexibility when provisioning resources. AWS CloudFormation allows you to manage a broad set of AWS resources. For the purposes of this whitepaper, we focus on the features that you can use to bootstrap your EC2 instances. User Data Earlier in this whitepaper, we described the process of using user data to configure and customize your EC2 instances (see Scripting Your Own Solution). You also can include user data in an AWS CloudFormation template, which is executed on the instance once it is created. You can include user data when specifying a single EC2 instance as well as when specifying a launch configuration. The following example shows a launch configuration that provisions instances configured to be PHP web servers: "MyLaunchConfig" : { "Type" : "AWS::AutoScaling::LaunchConfiguration", "Properties" : { "ImageId" : "i-123456", "SecurityGroups" : "MySecurityGroup", "InstanceType" : "m3.medium", "KeyName" : "MyKey", "UserData": {"Fn::Base64": {"Fn::Join":["",[ "#!/bin/bash\n", "yum update -y\n", "yum -y install httpd php php-mysql\n", "chkconfig httpd on\n", "/etc/init.d/httpd start\n" ]]}} Archived 7http://aws.amazon.com/documentation/opsworks/ Page 24 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 } } cfn-init The cfn-init script is an AWS CloudFormation helper script that you can use to specify the end state of an EC2 instance in a more declarative manner. The cfn-init script is installed by default on Amazon Linux and AWS-supplied Windows AMIs. Administrators can also install cfn-init on other Linux distributions, and then include this into their own AMI if needed. The cfn-init script parses metadata from the AWS CloudFormation template and uses the metadata to customize the instance accordingly. The cfn-init script can do the following: • Install packages from package repositories (such as yum and apt-get) • Download and unpack archives, such as .zip and .tar files • Write files to disk • Execute arbitrary commands • Create users and groups • Enable/disable and start/stop services In an AWS CloudFormation template, the cfn-init helper script is called from the user data. Once it is called, it will inspect the metadata associated with the resource passed into the request and then act accordingly. For example, you can use the following launch configuration metadata to instruct cfn-init to configure an EC2 instance to become a PHP web server (similar to the preceding user data example): "MyLaunchConfig" : { "Type" : "AWS::AutoScaling::LaunchConfiguration", "Metadata" : { "AWS::CloudFormation::Init" : { "config" : { "packages" : { "yum" : { "httpd" : [], "php" : [], "php-mysql" : [] } }, Archived "services" : { "sysvinit" : { "httpd" : { Page 25 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 "enabled" : "true", "ensureRunning" : "true" } } } } } }, "Properties" : { "ImageId" : "i-123456", "SecurityGroups" : "MySecurityGroup", "InstanceType" : "m3.medium", "KeyName" : "MyKey", "UserData": {"Fn::Base64": {"Fn::Join":["",[ "#!/bin/bash\n", "yum update -y aws-cfn-bootstrap\n", "/opt/aws/bin/cfn-init --stack ", { "Ref" : "AWS::StackId" }, " --resource MyLaunchConfig ", " --region ", { "Ref" : "AWS::Region" }, "\n", ]]}} } } For a detailed walkthrough of bootstrapping EC2 instances by using AWS CloudFormation and its related helper scripts, see the Bootstrapping Applications via AWS CloudFormation whitepaper.8 Using the Services Together You can use the services separately to help you provision new infrastructure components, but you also can combine them to create a single solution. This approach has clear advantages. For example, you can model an entire architecture, including networking and database configurations, directly into an AWS CloudFormation template, and then deploy and manage your application by using AWS Elastic Beanstalk or AWS OpsWorks. This approach unifies resource and application management, making it easier to apply version control to your entire architecture. Archived 8https://s3.amazonaws.com/cloudformation-examples/BoostrappingApplicationsWithAWSCloudFormation.pdf Page 26 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 Managing Application and Instance State After you implement a suitable process to automatically provision new infrastructure components, your system will have the capability to create new EC2 instances and even entire new environments in a quick, repeatable, and predictable manner. However, in a dynamic cloud environment you will also need to consider how to remove EC2 instances from your environments, and what impact this might have on the service that you provide to your users. There are a number of reasons why an instance might be removed from your system: • The instance is terminated as a result of a hardware or software failure • The instance is terminated as a response to a “scale down” event to remove instances from an Auto Scaling group • The instance is terminated because you’ve deployed a new version of your software stack by using blue-green deployments (instances running the older version of the application are terminated after the deployment) To handle the removal of instances without impacting your service, you need to ensure that your application instances are stateless. This means that all system and application state is stored and managed outside of the instances themselves. There are many forms of system and application state that you need to consider when designing your system, as shown in the following table. State Examples Structured application data Customer orders Unstructured application data User session data Application and system logs Application and system metrics Images and documents Position in the app; contents of a shopping cart Access logs; security audit logs CPU load; network utilization Running stateless application instances means that no instance in a fleet is any different from its counterparts. This offers a number of advantages: • Providing a robust service – Instances can serve any request from any user at any time. If an instance fails, subsequent requests can be routed to alternative instances while the failed instance is replaced. This can be achieved with no interruption to service for any of your users. Archived • Quicker, less complicated bootstrapping – Because your instances don’t contain any dynamic state, your bootstrapping process needs to concern itself only with provisioning your system up to the application layer. There is no need to try to Page 27 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 recover state and data, which is often large and therefore can significantly increase bootstrapping times. • EC2 instances as a unit of deployment – Because all state is maintained off of the EC2 instances themselves, you can replace the instances while orchestrating application deployments. This can simplify your deployment processes and allow new deployment techniques, such as blue-green deployments. The following section describes each form of application and instance state, and outlines some of the tools and techniques that you can use to ensure it is stored separately and independently from the application instances themselves. Structured Application Data Most applications produce structured, textual data, such as customer orders in an order management system or a list of web pages in a CMS. In most cases, this kind of content is best stored in a database. Depending on the structure of the data and the requirements for access speed and concurrency, you might decide to use a relational database management system or a NoSQL data store. In either case, it is important to store this content in a durable, highly available system away from the instances running your application. This will ensure that the service you provide your users will not be interrupted or their data lost, even in the event of an instance failure. AWS offers both relational and NoSQL managed databases that you can use as a persistence layer for your applications. We discuss these database options in the following sections. Amazon RDS Amazon Relational Database Service (Amazon RDS) is a web service that makes it easy to set up, operate, and scale a relational database in the cloud. It allows you to continue to work with the relational database engines you’re familiar with, including MySQL, Oracle, Microsoft SQL Server, or PostgreSQL. This means that the code, applications, and operational tools that you are already using can be used with Amazon RDS. Amazon RDS also handles time-consuming database management tasks, such as data backups, recovery, and patch management, which frees your database administrators to pursue higher value application development or database refinements. In addition, Amazon RDS Multi-AZ deployments increase your database availability and protect your database against unplanned outages. This gives your service an additional level of resiliency. Amazon DynamoDB Archived Amazon DynamoDB is a fully managed NoSQL database service offering both document (JSON) and key-value data models. DynamoDB has been designed to provide consistent, single-digit millisecond latency at any scale, making it ideal for high Page 28 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 traffic applications with a requirement for low latency data access. DynamoDB manages the scaling and partitioning of infrastructure on your behalf. When you create a table, you specify how much request capacity you require. If your throughput requirements change, you can update this capacity as needed with no impact on service. Unstructured Application Data In addition to the structured data created by most applications, some systems also have a requirement to receive and store unstructured resources such as documents, images, and other binary data. For example, this might be the case in a CMS where an editor uploads images and PDFs to be hosted on a website. In most cases, a database is not a suitable storage mechanism for this type of content. Instead, you can use Amazon Simple Storage Service (Amazon S3). Amazon S3 provides a highly available and durable object store that is well suited to storing this kind of data. Once your data is stored in Amazon S3, you have the option of serving these files directly from Amazon S3 to your end users over HTTP(S), bypassing the need for these requests to go to your application instances. User Session Data Many applications produce information associated with a user’s current position within an application. For example, as users browse an e-commerce site, they might start to add various items into their shopping basket. This information is known as session state. It would be frustrating to users if the items in their baskets disappeared without notice, so it’s important to store the session state away from the application instances themselves. This ensures that baskets remain populated, even if users’ requests are directed to an alternative instance behind your load balancer, or if the current instance is removed from service for any reason. The AWS platform offers a number of services that you can use to provide a highly available session store. Amazon ElastiCache Amazon ElastiCache makes it easy to deploy, operate, and scale an in-memory data store in AWS. In-memory data stores are ideal for storing transient session data due to the low latency these technologies offer. ElastiCache supports two open source, in memory caching engines: • Memcached – A widely adopted memory object caching system. ElastiCache is protocol compliant with Memcached, which is already supported by many open source applications as an in-memory session storage platform. Archived Page 29 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 • Redis – A popular open source, in-memory key-value store that supports data structures such as sorted sets and lists. ElastiCache supports master/slave replication and Multi-AZ, which you can use to achieve cross-AZ redundancy. In addition to the in-memory data stores offered by Memcached and Redis on ElastiCache, some applications require a more durable storage platform for their session data. For these applications, Amazon DynamoDB offers a low latency, highly scalable, and durable solution. DynamoDB replicates data across three facilities in an AWS region to provide fault tolerance in the event of a server failure or Availability Zone outage. To help customers easily integrate DynamoDB as a session store within their applications, AWS provides pre-built DynamoDB session handlers for both Tomcat based Java applications9 and PHP applications.10 System Metrics To properly support a production system, operational teams need access to system metrics that indicate the overall health of the system and the relative load under which it’s currently operating. In a traditional environment, this information is often obtained by logging into one of the instances and looking at OS-level metrics such as system load or CPU utilization. However, in an environment where you have multiple instances running, and these instances can appear and disappear at any moment, this approach soon becomes ineffective and difficult to manage. Instead, you should push this data to an external monitoring system for collection and analysis. Amazon CloudWatch Amazon CloudWatch is a fully managed monitoring service for AWS resources and the applications that you run on top of them. You can use Amazon CloudWatch to collect and store metrics on a durable platform that is separate and independent from your own infrastructure. This means that the metrics will be available to your operational teams even when the instances themselves have been terminated. In addition to tracking metrics, you can use Amazon CloudWatch to trigger alarms on the metrics when they pass certain thresholds. You can use the alarms to notify your teams and to initiate further automated actions to deal with issues and bring your system back within its normal operating tolerances. For example, an automated action could initiate an Auto Scaling policy to increase or decrease the number of instances in an Auto Scaling group. Archived 9http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-tomcat-session-manager.html 10 http://docs.aws.amazon.com/aws-sdk-php/guide/latest/feature-dynamodb-session-handler.html Page 30 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 By default, Amazon CloudWatch can monitor a broad range of metrics across your AWS resources. That said, it is also important to remember that AWS doesn’t have access to the OS or applications running on your EC2 instances. Because of this, Amazon CloudWatch cannot automatically monitor metrics that are accessible only within the OS, such as memory and disk volume utilization. If you want to monitor OS and application metrics by using Amazon CloudWatch, you can publish your own metrics to CloudWatch through a simple API request. With this approach, you can manage these metrics in the same way that you manage other, native metrics, including configuring alarms and associated actions. You can use the EC2Config service11 to push additional OS-level operating metrics into CloudWatch without the need to manually code against the CloudWatch APIs. If you are running Linux AMIs, you can use the set of sample Perl scripts12 provided by AWS that demonstrate how to produce and consume Amazon CloudWatch custom metrics. In addition to CloudWatch, you can use third-party monitoring solutions in AWS to extend your monitoring capabilities. Log Management Log data is used by your operational team to better understand how the system is performing and to diagnose any issues that might arise. Log data can be produced by the application itself, but also by system components lower down in your stack. This might include anything from access logs produced by your web server to security audit logs produced by the operating system itself. Your operations team needs reliable and timely access to these logs at all times, regardless of whether the instance that originally produced the log is still in existence. For this reason, it’s important to move log data from the instance to a more durable storage platform as close to real time as possible. Amazon CloudWatch Logs Amazon CloudWatch Logs is a service that allows you to quickly and easily move your system and application logs from the EC2 instances themselves to a centralized, durable storage platform (Amazon S3). This ensures that this data is available even when the instance itself has been terminated. You also have control over the log retention policy to ensure that all logs are retained for a specified period of time. The CloudWatch Logs service provides a log management agent that you can install onto your EC2 instances to manage the ingestion of your logs into the log management service. Archived 11 http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/UsingConfig_WinAMI.html 12 http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/mon-scripts-perl.html Page 31 of 32 Amazon Web Services – Managing Your AWS Infrastructure at Scale February 2015 In addition to moving your logs to durable storage, the CloudWatch Logs service also allows you to monitor your logs in near real-time for specific phrases, values, or patterns (metrics). You can use these metrics in the same way as any other CloudWatch metrics. For example, you can create a CloudWatch alarm on the number of errors being thrown by your application or when certain, suspect actions are detected in your security audit logs. Conclusion This whitepaper showed you how to accomplish the following: • Quickly provision new infrastructure components in an automated, repeatable, and predictable manner • Ensure that no EC2 instance in your environment is unique, and that all instances are stateless and therefore easily replaced Having these capabilities in place allows you to think differently about how you provision and manage infrastructure components when compared to traditional environments. Instead of manually building each instance and maintaining consistency through a set of operational checks and balances, you can treat your infrastructure as if it were software. By specifying the desired end state of your infrastructure through the software-based tools and processes described in this whitepaper, you can fundamentally change the way your infrastructure is managed, and you can take full advantage of the dynamic, elastic, and automated nature of the AWS cloud. Further Reading • AWS Elastic Beanstalk Documentation • AWS OpsWorks Documentation • Bootstrapping Applications via AWS CloudFormation whitepaper • Using Chef with AWS CloudFormation • Integrating AWS CloudFormation with Puppet Archived Page 32 of 32  
